"""
ë£¨ë¦¬ì›¹ ê°œë³„ ê²Œì‹œê¸€ ìƒì„¸ ìŠ¤í¬ë˜í•‘ ìŠ¤í¬ë¦½íŠ¸

ëª©ì°¨:
- MongoDB ì—°ê²° ë° ê¸°ì¡´ ë°ì´í„° ì¡°íšŒ (ë¼ì¸ 1-50)
- ê°œë³„ ê²Œì‹œê¸€ ìƒì„¸ ìŠ¤í¬ë˜í•‘ (ë¼ì¸ 51-100)
- ë°ì´í„° ì—…ë°ì´íŠ¸ (ë¼ì¸ 101-150)

ì‘ì„±ì: AI Assistant
ì‘ì„±ì¼: 2025-01-28
ëª©ì : ë£¨ë¦¬ì›¹ ê²Œì‹œê¸€ì˜ ë³¸ë¬¸ê³¼ ëŒ“ê¸€ì„ ìƒì„¸ ìŠ¤í¬ë˜í•‘í•˜ì—¬ ì—…ë°ì´íŠ¸
"""

import asyncio
import sys
import os
from datetime import datetime
from typing import List, Dict
import pytz

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from scraping.scrapers.ruliweb_scraper import ImprovedRuliwebScraper
import pymongo
from loguru import logger

# MongoDB ì—°ê²° ì„¤ì •
MONGODB_URI = 'mongodb+srv://nadle:miQXsXpzayvMQAzE@cluster0.bh7mhfi.mongodb.net/political_public_sphere'
DATABASE_NAME = 'political_public_sphere'
COLLECTION_NAME = 'community_posts'

# í•œêµ­ ì‹œê°„ëŒ€ ì„¤ì •
KST = pytz.timezone('Asia/Seoul')

async def connect_mongodb():
    """MongoDB ì—°ê²°"""
    try:
        client = pymongo.MongoClient(MONGODB_URI)
        db = client[DATABASE_NAME]
        collection = db[COLLECTION_NAME]
        
        # ì—°ê²° í…ŒìŠ¤íŠ¸
        client.admin.command('ping')
        logger.info("âœ… MongoDB ì—°ê²° ì„±ê³µ")
        
        return client, db, collection
    except Exception as e:
        logger.error(f"ğŸ’¥ MongoDB ì—°ê²° ì‹¤íŒ¨: {e}")
        raise

async def scrape_ruliweb_post_details():
    """ë£¨ë¦¬ì›¹ ê²Œì‹œê¸€ ìƒì„¸ ë‚´ìš© ìŠ¤í¬ë˜í•‘"""
    try:
        logger.info("ğŸš€ ë£¨ë¦¬ì›¹ ê²Œì‹œê¸€ ìƒì„¸ ìŠ¤í¬ë˜í•‘ ì‹œì‘")
        
        # MongoDB ì—°ê²°
        client, db, collection = await connect_mongodb()
        
        # ë£¨ë¦¬ì›¹ ê²Œì‹œê¸€ ì¤‘ ë³¸ë¬¸ì´ ë¶€ì¡±í•œ ê²ƒë“¤ ì¡°íšŒ (contentê°€ 1ê°œ ì´í•˜ì¸ ê²ƒë“¤)
        ruliweb_posts = list(collection.find({
            'site': 'ruliweb',
            '$or': [
                {'content': {'$size': 0}},
                {'content': {'$size': 1}},
                {'comments': {'$size': 0}}
            ]
        }).limit(5))  # ì²˜ìŒ 5ê°œë§Œ í…ŒìŠ¤íŠ¸
        
        logger.info(f"ğŸ“‹ ìƒì„¸ ìŠ¤í¬ë˜í•‘ ëŒ€ìƒ: {len(ruliweb_posts)}ê°œ ê²Œì‹œê¸€")
        
        if not ruliweb_posts:
            logger.info("âœ… ëª¨ë“  ë£¨ë¦¬ì›¹ ê²Œì‹œê¸€ì´ ì´ë¯¸ ìƒì„¸ ìŠ¤í¬ë˜í•‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return
        
        # ë£¨ë¦¬ì›¹ ìŠ¤í¬ë˜í¼ ì´ˆê¸°í™”
        async with ImprovedRuliwebScraper() as scraper:
            updated_count = 0
            
            for post in ruliweb_posts:
                try:
                    post_url = post.get('post_url')
                    post_id = post.get('post_id')
                    
                    if not post_url:
                        logger.warning(f"âš ï¸ ê²Œì‹œê¸€ URLì´ ì—†ìŠµë‹ˆë‹¤: {post_id}")
                        continue
                    
                    logger.info(f"ğŸ“„ ìƒì„¸ ìŠ¤í¬ë˜í•‘ ì‹œì‘: {post_id}")
                    
                    # ê°œë³„ ê²Œì‹œê¸€ ìƒì„¸ ìŠ¤í¬ë˜í•‘
                    detailed_data = await scraper.scrape_post(post_url)
                    
                    if detailed_data and detailed_data.get('content'):
                        # ê¸°ì¡´ ë°ì´í„° ì—…ë°ì´íŠ¸
                        update_data = {
                            'content': detailed_data.get('content', []),
                            'comments': detailed_data.get('comments', []),
                            'updated_at': datetime.now(KST).isoformat()
                        }
                        
                        # ë©”íƒ€ë°ì´í„°ë„ ë” ìƒì„¸í•œ ì •ë³´ë¡œ ì—…ë°ì´íŠ¸
                        if detailed_data.get('metadata'):
                            detailed_metadata = detailed_data['metadata']
                            current_metadata = post.get('metadata', {})
                            
                            # ê¸°ì¡´ ë©”íƒ€ë°ì´í„°ì™€ ìƒˆ ë©”íƒ€ë°ì´í„° ë³‘í•©
                            merged_metadata = {**current_metadata}
                            
                            # ë” ìƒì„¸í•œ ì •ë³´ê°€ ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸
                            if detailed_metadata.get('view_count'):
                                merged_metadata['view_count'] = detailed_metadata['view_count']
                            if detailed_metadata.get('like_count'):
                                merged_metadata['like_count'] = detailed_metadata['like_count']
                            if detailed_metadata.get('comment_count'):
                                merged_metadata['comment_count'] = detailed_metadata['comment_count']
                            
                            update_data['metadata'] = merged_metadata
                        
                        # MongoDB ì—…ë°ì´íŠ¸
                        result = collection.update_one(
                            {'_id': post['_id']},
                            {'$set': update_data}
                        )
                        
                        if result.modified_count > 0:
                            updated_count += 1
                            content_count = len(detailed_data.get('content', []))
                            comment_count = len(detailed_data.get('comments', []))
                            logger.info(f"âœ… ê²Œì‹œê¸€ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {post_id} (ë³¸ë¬¸: {content_count}ê°œ, ëŒ“ê¸€: {comment_count}ê°œ)")
                        else:
                            logger.warning(f"âš ï¸ ê²Œì‹œê¸€ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {post_id}")
                    else:
                        logger.warning(f"âš ï¸ ìƒì„¸ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {post_id}")
                    
                    # ìš”ì²­ ê°„ ëŒ€ê¸° (ì„œë²„ ë¶€í•˜ ë°©ì§€)
                    await asyncio.sleep(3)
                    
                except Exception as e:
                    logger.error(f"ğŸ’¥ ê²Œì‹œê¸€ ìƒì„¸ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨ ({post_id}): {e}")
                    continue
            
            logger.info(f"âœ… ë£¨ë¦¬ì›¹ ìƒì„¸ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: {updated_count}ê°œ ê²Œì‹œê¸€ ì—…ë°ì´íŠ¸")
            
    except Exception as e:
        logger.error(f"ğŸ’¥ ë£¨ë¦¬ì›¹ ìƒì„¸ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
        raise
    finally:
        # MongoDB ì—°ê²° ì¢…ë£Œ
        if 'client' in locals():
            client.close()

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        await scrape_ruliweb_post_details()
        logger.info("ğŸ‰ ë£¨ë¦¬ì›¹ ìƒì„¸ ìŠ¤í¬ë˜í•‘ ì‘ì—… ì™„ë£Œ")
    except Exception as e:
        logger.error(f"ğŸ’¥ ìƒì„¸ ìŠ¤í¬ë˜í•‘ ì‘ì—… ì‹¤íŒ¨: {e}")
        sys.exit(1)

if __name__ == "__main__":
    # ë¡œê·¸ ì„¤ì •
    logger.add("logs/ruliweb_detail_scraping.log", rotation="1 day", retention="7 days")
    
    # ë¹„ë™ê¸° ì‹¤í–‰
    asyncio.run(main()) 