 """
í†µí•© ìŠ¤í¬ë˜í¼ ì‚¬ìš© ì˜ˆì œ

ì£¼ìš” ê¸°ëŠ¥:
- ì¶”ìƒ ê¸°ë³¸ í´ë˜ìŠ¤ ê¸°ë°˜ í†µí•© ìŠ¤í¬ë˜í•‘ (line 20-60)
- ì§€í‘œë³„ ì„ ë³„ ë¡œì§ ì ìš© (line 62-120)
- CommunityPost ëª¨ë¸ ì¶œë ¥ (line 122-180)
- MongoDB ì €ì¥ í†µí•© (line 182-220)

ì‘ì„±ì: AI Assistant
ì‘ì„±ì¼: 2025-01-28
ëª©ì : TDDë¡œ ê°œë°œëœ í†µí•© ìŠ¤í¬ë˜í¼ êµ¬ì¡° ì‹¤ì œ í™œìš© ì˜ˆì œ
"""

import asyncio
import pymongo
from datetime import datetime
from typing import Dict, List, Any
from loguru import logger

# í†µí•© ìŠ¤í¬ë˜í¼ import
from scraping.scrapers.fmkorea_scraper_v2 import FMKoreaScraper
from scraping.scrapers.ruliweb_scraper_v2 import RuliwebScraper
from scripts.community_post_utils import CommunityPostManager, SelectionCriteria


async def scrape_fmkorea_with_selection():
    """FMì½”ë¦¬ì•„ ì§€í‘œë³„ ì„ ë³„ ìŠ¤í¬ë˜í•‘ ì˜ˆì œ"""
    logger.info("ğŸš€ FMì½”ë¦¬ì•„ ì§€í‘œë³„ ì„ ë³„ ìŠ¤í¬ë˜í•‘ ì‹œì‘")
    
    # FMì½”ë¦¬ì•„ ì •ì¹˜ ê²Œì‹œíŒ
    board_url = "https://www.fmkorea.com/index.php?mid=politics"
    
    async with FMKoreaScraper() as scraper:
        # ì§€í‘œë³„ ì„ ë³„ ìŠ¤í¬ë˜í•‘ (ê° ê¸°ì¤€ë‹¹ 3ê°œì”©)
        selected_posts = await scraper.scrape_board_with_selection(board_url, criteria_count=3)
        
        logger.info(f"âœ… FMì½”ë¦¬ì•„ ì„ ë³„ ì™„ë£Œ:")
        for criteria, posts in selected_posts.items():
            logger.info(f"  - {criteria}: {len(posts)}ê°œ")
            for post in posts:
                logger.info(f"    ğŸ“ {post['title'][:40]}... (ì¶”ì²œ:{post['like_count']}, ëŒ“ê¸€:{post['comment_count']}, ì¡°íšŒ:{post['view_count']})")
        
        return selected_posts


async def scrape_ruliweb_with_selection():
    """ë£¨ë¦¬ì›¹ ì§€í‘œë³„ ì„ ë³„ ìŠ¤í¬ë˜í•‘ ì˜ˆì œ"""
    logger.info("ğŸš€ ë£¨ë¦¬ì›¹ ì§€í‘œë³„ ì„ ë³„ ìŠ¤í¬ë˜í•‘ ì‹œì‘")
    
    # ë£¨ë¦¬ì›¹ ììœ ê²Œì‹œíŒ
    board_url = "https://bbs.ruliweb.com/community/board/300143"
    
    async with RuliwebScraper() as scraper:
        # ì§€í‘œë³„ ì„ ë³„ ìŠ¤í¬ë˜í•‘ (ê° ê¸°ì¤€ë‹¹ 3ê°œì”©)
        selected_posts = await scraper.scrape_board_with_selection(board_url, criteria_count=3)
        
        logger.info(f"âœ… ë£¨ë¦¬ì›¹ ì„ ë³„ ì™„ë£Œ:")
        for criteria, posts in selected_posts.items():
            logger.info(f"  - {criteria}: {len(posts)}ê°œ")
            for post in posts:
                logger.info(f"    ğŸ“ {post['title'][:40]}... (ì¶”ì²œ:{post['like_count']}, ëŒ“ê¸€:{post['comment_count']}, ì¡°íšŒ:{post['view_count']})")
        
        return selected_posts


async def scrape_post_details_with_criteria(scraper, selected_posts: Dict[str, List[Dict]], site: str):
    """ì„ ë³„ëœ ê²Œì‹œê¸€ë“¤ì˜ ìƒì„¸ ì •ë³´ ìŠ¤í¬ë˜í•‘"""
    logger.info(f"ğŸ” {site} ì„ ë³„ëœ ê²Œì‹œê¸€ ìƒì„¸ ìŠ¤í¬ë˜í•‘ ì‹œì‘")
    
    detailed_posts = {}
    
    for criteria_name, posts in selected_posts.items():
        detailed_posts[criteria_name] = []
        
        # SelectionCriteria enumìœ¼ë¡œ ë³€í™˜
        try:
            criteria_enum = SelectionCriteria(criteria_name)
        except ValueError:
            logger.warning(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ì„ ë³„ ê¸°ì¤€: {criteria_name}")
            continue
        
        for post in posts:
            try:
                logger.info(f"ğŸ“– ìƒì„¸ ìŠ¤í¬ë˜í•‘: {post['title'][:30]}...")
                
                # ê²Œì‹œê¸€ ìƒì„¸ ìŠ¤í¬ë˜í•‘
                post_detail = await scraper.scrape_post_detail(post['post_url'])
                
                # ì„ ë³„ ê¸°ì¤€ ì •ë³´ ì¶”ê°€
                post_detail['selection_criteria'] = criteria_enum.value
                post_detail['site'] = site
                
                detailed_posts[criteria_name].append(post_detail)
                
                logger.info(f"âœ… ìƒì„¸ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: ì½˜í…ì¸  {len(post_detail.get('content', []))}ê°œ, ëŒ“ê¸€ {len(post_detail.get('comments', []))}ê°œ")
                
            except Exception as e:
                logger.error(f"ğŸ’¥ ìƒì„¸ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {post['title'][:30]}... - {e}")
                continue
    
    return detailed_posts


async def save_to_mongodb(detailed_posts: Dict[str, List[Dict]], site: str):
    """MongoDBì— CommunityPost ëª¨ë¸ë¡œ ì €ì¥"""
    logger.info(f"ğŸ’¾ {site} ê²Œì‹œê¸€ MongoDB ì €ì¥ ì‹œì‘")
    
    # MongoDB ì—°ê²° (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©)
    mongodb_uri = "mongodb://localhost:27017/"
    
    try:
        manager = CommunityPostManager(mongodb_uri)
        
        save_stats = {}
        total_saved = 0
        
        for criteria_name, posts in detailed_posts.items():
            saved_count = 0
            
            # SelectionCriteria enumìœ¼ë¡œ ë³€í™˜
            try:
                criteria_enum = SelectionCriteria(criteria_name)
            except ValueError:
                logger.warning(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ì„ ë³„ ê¸°ì¤€: {criteria_name}")
                continue
            
            for post_detail in posts:
                try:
                    # CommunityPost ëª¨ë¸ë¡œ ë³€í™˜ ë° ì €ì¥
                    success = manager.convert_and_save(post_detail, site, criteria_enum)
                    if success:
                        saved_count += 1
                        total_saved += 1
                except Exception as e:
                    logger.error(f"ğŸ’¥ ì €ì¥ ì‹¤íŒ¨: {e}")
                    continue
            
            save_stats[criteria_name] = saved_count
            logger.info(f"âœ… {criteria_name} ê¸°ì¤€ ì €ì¥: {saved_count}/{len(posts)}ê°œ")
        
        # í†µê³„ ì¡°íšŒ
        stats = manager.get_stats(site)
        logger.info(f"ğŸ“Š {site} ì €ì¥ í†µê³„:")
        logger.info(f"  - ì´ ê²Œì‹œê¸€: {stats['total_posts']}ê°œ")
        logger.info(f"  - ê¸°ì¤€ë³„ í†µê³„: {stats['criteria_stats']}")
        
        manager.close()
        
        return save_stats, total_saved
        
    except Exception as e:
        logger.error(f"ğŸ’¥ MongoDB ì €ì¥ ì‹¤íŒ¨: {e}")
        return {}, 0


async def unified_scraping_workflow():
    """í†µí•© ìŠ¤í¬ë˜í•‘ ì›Œí¬í”Œë¡œìš° ì˜ˆì œ"""
    logger.info("ğŸŒŸ í†µí•© ìŠ¤í¬ë˜í•‘ ì›Œí¬í”Œë¡œìš° ì‹œì‘")
    
    start_time = datetime.now()
    
    try:
        # 1ë‹¨ê³„: ê²Œì‹œíŒ ëª©ë¡ ìŠ¤í¬ë˜í•‘ ë° ì„ ë³„
        logger.info("ğŸ“‹ 1ë‹¨ê³„: ê²Œì‹œíŒ ëª©ë¡ ìŠ¤í¬ë˜í•‘ ë° ì„ ë³„")
        
        # FMì½”ë¦¬ì•„ ì„ ë³„
        fmkorea_selected = await scrape_fmkorea_with_selection()
        
        # ë£¨ë¦¬ì›¹ ì„ ë³„
        ruliweb_selected = await scrape_ruliweb_with_selection()
        
        # 2ë‹¨ê³„: ì„ ë³„ëœ ê²Œì‹œê¸€ ìƒì„¸ ìŠ¤í¬ë˜í•‘
        logger.info("ğŸ” 2ë‹¨ê³„: ì„ ë³„ëœ ê²Œì‹œê¸€ ìƒì„¸ ìŠ¤í¬ë˜í•‘")
        
        # FMì½”ë¦¬ì•„ ìƒì„¸ ìŠ¤í¬ë˜í•‘
        async with FMKoreaScraper() as fmkorea_scraper:
            fmkorea_detailed = await scrape_post_details_with_criteria(
                fmkorea_scraper, fmkorea_selected, 'fmkorea'
            )
        
        # ë£¨ë¦¬ì›¹ ìƒì„¸ ìŠ¤í¬ë˜í•‘
        async with RuliwebScraper() as ruliweb_scraper:
            ruliweb_detailed = await scrape_post_details_with_criteria(
                ruliweb_scraper, ruliweb_selected, 'ruliweb'
            )
        
        # 3ë‹¨ê³„: MongoDB ì €ì¥
        logger.info("ğŸ’¾ 3ë‹¨ê³„: CommunityPost ëª¨ë¸ë¡œ MongoDB ì €ì¥")
        
        # FMì½”ë¦¬ì•„ ì €ì¥
        fmkorea_save_stats, fmkorea_saved = await save_to_mongodb(fmkorea_detailed, 'fmkorea')
        
        # ë£¨ë¦¬ì›¹ ì €ì¥
        ruliweb_save_stats, ruliweb_saved = await save_to_mongodb(ruliweb_detailed, 'ruliweb')
        
        # 4ë‹¨ê³„: ê²°ê³¼ ìš”ì•½
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        logger.info("ğŸ‰ í†µí•© ìŠ¤í¬ë˜í•‘ ì›Œí¬í”Œë¡œìš° ì™„ë£Œ!")
        logger.info(f"â±ï¸ ì´ ì†Œìš”ì‹œê°„: {duration:.2f}ì´ˆ")
        logger.info(f"ğŸ“Š ìµœì¢… ê²°ê³¼:")
        logger.info(f"  - FMì½”ë¦¬ì•„: {fmkorea_saved}ê°œ ì €ì¥")
        logger.info(f"  - ë£¨ë¦¬ì›¹: {ruliweb_saved}ê°œ ì €ì¥")
        logger.info(f"  - ì´ ì €ì¥: {fmkorea_saved + ruliweb_saved}ê°œ")
        
        return {
            'fmkorea': {
                'selected': fmkorea_selected,
                'detailed': fmkorea_detailed,
                'saved': fmkorea_saved,
                'save_stats': fmkorea_save_stats
            },
            'ruliweb': {
                'selected': ruliweb_selected,
                'detailed': ruliweb_detailed,
                'saved': ruliweb_saved,
                'save_stats': ruliweb_save_stats
            },
            'duration': duration
        }
        
    except Exception as e:
        logger.error(f"ğŸ’¥ í†µí•© ìŠ¤í¬ë˜í•‘ ì›Œí¬í”Œë¡œìš° ì‹¤íŒ¨: {e}")
        return None


async def quick_test_example():
    """ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ì˜ˆì œ (ì €ì¥ ì—†ì´ ìŠ¤í¬ë˜í•‘ë§Œ)"""
    logger.info("âš¡ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ì‹œì‘ (ì €ì¥ ì—†ì´ ìŠ¤í¬ë˜í•‘ë§Œ)")
    
    try:
        # FMì½”ë¦¬ì•„ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
        async with FMKoreaScraper() as scraper:
            board_url = "https://www.fmkorea.com/index.php?mid=politics"
            posts = await scraper.scrape_board_list(board_url)
            
            logger.info(f"âœ… FMì½”ë¦¬ì•„: {len(posts)}ê°œ ê²Œì‹œê¸€ ì¶”ì¶œ")
            if posts:
                logger.info(f"ğŸ“ ì²« ë²ˆì§¸ ê²Œì‹œê¸€: {posts[0]['title'][:50]}...")
        
        # ë£¨ë¦¬ì›¹ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
        async with RuliwebScraper() as scraper:
            board_url = "https://bbs.ruliweb.com/community/board/300143"
            posts = await scraper.scrape_board_list(board_url)
            
            logger.info(f"âœ… ë£¨ë¦¬ì›¹: {len(posts)}ê°œ ê²Œì‹œê¸€ ì¶”ì¶œ")
            if posts:
                logger.info(f"ğŸ“ ì²« ë²ˆì§¸ ê²Œì‹œê¸€: {posts[0]['title'][:50]}...")
        
        logger.info("ğŸ‰ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        
    except Exception as e:
        logger.error(f"ğŸ’¥ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")


if __name__ == "__main__":
    # ì‚¬ìš© ì˜ˆì œ ì„ íƒ
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "quick":
        # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
        asyncio.run(quick_test_example())
    elif len(sys.argv) > 1 and sys.argv[1] == "fmkorea":
        # FMì½”ë¦¬ì•„ë§Œ í…ŒìŠ¤íŠ¸
        asyncio.run(scrape_fmkorea_with_selection())
    elif len(sys.argv) > 1 and sys.argv[1] == "ruliweb":
        # ë£¨ë¦¬ì›¹ë§Œ í…ŒìŠ¤íŠ¸
        asyncio.run(scrape_ruliweb_with_selection())
    else:
        # ì „ì²´ ì›Œí¬í”Œë¡œìš°
        result = asyncio.run(unified_scraping_workflow())
        if result:
            print("\nğŸ¯ ì›Œí¬í”Œë¡œìš° ì„±ê³µ!")
            print(f"ğŸ“Š FMì½”ë¦¬ì•„: {result['fmkorea']['saved']}ê°œ ì €ì¥")
            print(f"ğŸ“Š ë£¨ë¦¬ì›¹: {result['ruliweb']['saved']}ê°œ ì €ì¥")
            print(f"â±ï¸ ì†Œìš”ì‹œê°„: {result['duration']:.2f}ì´ˆ")
        else:
            print("\nâŒ ì›Œí¬í”Œë¡œìš° ì‹¤íŒ¨!")