"""
FMì½”ë¦¬ì•„ ë°ì´í„°ë¥¼ ì¼ë°˜ì ì¸ ì»¤ë®¤ë‹ˆí‹° í¬ìŠ¤íŠ¸ êµ¬ì¡°ë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜

ì£¼ìš” ê¸°ëŠ¥:
- ê¸°ì¡´ fmkorea_posts ì»¬ë ‰ì…˜ ë°ì´í„° ì½ê¸° (line 20-60)
- ìƒˆë¡œìš´ community_posts êµ¬ì¡°ë¡œ ë³€í™˜ (line 61-120)
- ì•ˆì „í•œ ë§ˆì´ê·¸ë ˆì´ì…˜ ë° ë¡¤ë°± ì§€ì› (line 121-180)
- ìƒˆë¡œìš´ ë©”íŠ¸ë¦­ êµ¬ì¡° ì§€ì› (line 181-220)

ì‘ì„±ì: AI Assistant
ì‘ì„±ì¼: 2025-01-28
ëª©ì : ì ì§„ì  ëª¨ë¸ ë³€í™˜ì„ ìœ„í•œ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜
"""

import asyncio
import pymongo
from datetime import datetime
from typing import Dict, List, Any, Optional
from loguru import logger


class FmkoreaToCommunitMigrator:
    """FMì½”ë¦¬ì•„ ë°ì´í„°ë¥¼ ì¼ë°˜ì ì¸ ì»¤ë®¤ë‹ˆí‹° êµ¬ì¡°ë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜ (ìƒˆë¡œìš´ ë©”íŠ¸ë¦­ êµ¬ì¡° ì§€ì›)"""
    
    def __init__(self, mongodb_uri: str):
        self.client = pymongo.MongoClient(mongodb_uri)
        self.db = self.client.political_public_sphere
        self.source_collection = self.db.fmkorea_posts
        self.target_collection = self.db.community_posts
        
    def transform_post_data(self, fmkorea_post: Dict[str, Any]) -> Dict[str, Any]:
        """FMì½”ë¦¬ì•„ ê²Œì‹œê¸€ ë°ì´í„°ë¥¼ ì¼ë°˜ì ì¸ ì»¤ë®¤ë‹ˆí‹° êµ¬ì¡°ë¡œ ë³€í™˜ (ìƒˆë¡œìš´ ë©”íŠ¸ë¦­ êµ¬ì¡°)"""
        try:
            # ê¸°ë³¸ í•„ë“œ ë§¤í•‘
            community_post = {
                'post_id': fmkorea_post.get('post_id'),
                'post_url': fmkorea_post.get('post_url'),
                'site': 'fmkorea',  # í•˜ë“œì½”ë”© ì œê±°
                'scraped_at': fmkorea_post.get('scraped_at'),
                'created_at': fmkorea_post.get('scraped_at'),  # ê¸°ë³¸ê°’ìœ¼ë¡œ scraped_at ì‚¬ìš©
                'updated_at': datetime.now()
            }
            
            # ë©”íƒ€ë°ì´í„° ë³€í™˜
            old_metadata = fmkorea_post.get('metadata', {})
            community_post['metadata'] = {
                'title': old_metadata.get('title'),
                'author': old_metadata.get('author'),
                'date': old_metadata.get('date'),
                'view_count': old_metadata.get('view_count'),
                'like_count': old_metadata.get('like_count'),
                'dislike_count': old_metadata.get('dislike_count'),
                'comment_count': old_metadata.get('comment_count')
            }
            
            # ì½˜í…ì¸  ë³€í™˜ (ìƒˆë¡œìš´ êµ¬ì¡° ì§€ì›)
            old_content = fmkorea_post.get('content', [])
            community_post['content'] = []
            
            for item in old_content:
                if isinstance(item, dict):
                    transformed_item = {
                        'type': item.get('type', 'text'),
                        'order': item.get('order', 0),
                        'data': item.get('data', {})
                    }
                    
                    # ê¸°ì¡´ êµ¬ì¡° í˜¸í™˜ì„± ìœ ì§€
                    if 'content' in item and 'data' not in item:
                        transformed_item['data'] = {'text': item['content']}
                    elif 'src' in item and 'data' not in item:
                        transformed_item['data'] = {
                            'src': item['src'],
                            'alt': item.get('alt', ''),
                            'width': item.get('width', ''),
                            'height': item.get('height', '')
                        }
                    
                    community_post['content'].append(transformed_item)
            
            # ëŒ“ê¸€ ë³€í™˜
            old_comments = fmkorea_post.get('comments', [])
            community_post['comments'] = []
            
            for comment in old_comments:
                if isinstance(comment, dict):
                    transformed_comment = {
                        'id': comment.get('id'),
                        'author': comment.get('author'),
                        'content': comment.get('content'),
                        'date': comment.get('date'),
                        'depth': comment.get('depth', 0),
                        'is_reply': comment.get('is_reply', False),
                        'parent_id': comment.get('parent_id'),
                        'like_count': comment.get('like_count'),
                        'dislike_count': comment.get('dislike_count'),
                        'is_best': comment.get('is_best', False),
                        'is_author': comment.get('is_author', False)
                    }
                    community_post['comments'].append(transformed_comment)
            
            return community_post
            
        except Exception as e:
            logger.error(f"ë°ì´í„° ë³€í™˜ ì‹¤íŒ¨: {e}")
            logger.error(f"ì›ë³¸ ë°ì´í„°: {fmkorea_post}")
            raise
    
    def validate_transformed_data(self, original: Dict[str, Any], transformed: Dict[str, Any]) -> bool:
        """ë³€í™˜ëœ ë°ì´í„°ì˜ ìœ íš¨ì„± ê²€ì¦ (ìƒˆë¡œìš´ ë©”íŠ¸ë¦­ êµ¬ì¡° ê³ ë ¤)"""
        try:
            # í•„ìˆ˜ í•„ë“œ í™•ì¸
            required_fields = ['post_id', 'post_url', 'site', 'scraped_at']
            for field in required_fields:
                if not transformed.get(field):
                    logger.error(f"í•„ìˆ˜ í•„ë“œ ëˆ„ë½: {field}")
                    return False
            
            # ë°ì´í„° ì¼ê´€ì„± í™•ì¸
            if transformed['post_id'] != original.get('post_id'):
                logger.error("post_id ë¶ˆì¼ì¹˜")
                return False
            
            if transformed['site'] != 'fmkorea':
                logger.error("site í•„ë“œ ì˜¤ë¥˜")
                return False
            
            # ì½˜í…ì¸  ê°œìˆ˜ í™•ì¸
            original_content_count = len(original.get('content', []))
            transformed_content_count = len(transformed.get('content', []))
            
            if original_content_count != transformed_content_count:
                logger.warning(f"ì½˜í…ì¸  ê°œìˆ˜ ë¶ˆì¼ì¹˜: {original_content_count} -> {transformed_content_count}")
            
            # ëŒ“ê¸€ ê°œìˆ˜ í™•ì¸
            original_comment_count = len(original.get('comments', []))
            transformed_comment_count = len(transformed.get('comments', []))
            
            if original_comment_count != transformed_comment_count:
                logger.warning(f"ëŒ“ê¸€ ê°œìˆ˜ ë¶ˆì¼ì¹˜: {original_comment_count} -> {transformed_comment_count}")
            
            # ë©”íŠ¸ë¦­ ë°ì´í„° ê²€ì¦ (ìƒˆë¡œìš´ êµ¬ì¡°)
            metadata = transformed.get('metadata', {})
            if metadata.get('view_count') is not None and metadata.get('view_count') < 0:
                logger.warning("ì¡°íšŒìˆ˜ê°€ ìŒìˆ˜ì…ë‹ˆë‹¤")
            
            if metadata.get('like_count') is not None and metadata.get('like_count') < 0:
                logger.warning("ì¶”ì²œìˆ˜ê°€ ìŒìˆ˜ì…ë‹ˆë‹¤")
            
            if metadata.get('comment_count') is not None and metadata.get('comment_count') < 0:
                logger.warning("ëŒ“ê¸€ìˆ˜ê°€ ìŒìˆ˜ì…ë‹ˆë‹¤")
            
            return True
            
        except Exception as e:
            logger.error(f"ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨: {e}")
            return False
    
    def calculate_post_metrics(self, post: Dict[str, Any]) -> Dict[str, float]:
        """ê²Œì‹œê¸€ì˜ ë©”íŠ¸ë¦­ ê³„ì‚° (ìƒˆë¡œìš´ êµ¬ì¡°)"""
        try:
            metadata = post.get('metadata', {})
            views = metadata.get('view_count', 0)
            likes = metadata.get('like_count', 0)
            comments_count = len(post.get('comments', []))
            
            # ë©”íŠ¸ë¦­ ê³„ì‚°
            likes_per_view = likes / views if views > 0 else 0
            comments_per_view = comments_count / views if views > 0 else 0
            
            # ì‹œê°„ë‹¹ ì¡°íšŒìˆ˜ ê³„ì‚°
            created_at = post.get('created_at')
            if created_at:
                try:
                    if isinstance(created_at, str):
                        created_date = datetime.fromisoformat(created_at.replace('Z', '+00:00'))
                    else:
                        created_date = created_at
                    
                    hours_from_creation = (datetime.now() - created_date).total_seconds() / 3600
                    views_per_exposure_hour = views / hours_from_creation if hours_from_creation > 0 else 0
                except:
                    views_per_exposure_hour = 0
            else:
                views_per_exposure_hour = 0
            
            return {
                'likes_per_view': likes_per_view,
                'comments_per_view': comments_per_view,
                'views_per_exposure_hour': views_per_exposure_hour
            }
            
        except Exception as e:
            logger.error(f"ë©”íŠ¸ë¦­ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {
                'likes_per_view': 0,
                'comments_per_view': 0,
                'views_per_exposure_hour': 0
            }
    
    def migrate_posts(self, batch_size: int = 100, dry_run: bool = True) -> Dict[str, int]:
        """ê²Œì‹œê¸€ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰ (ìƒˆë¡œìš´ ë©”íŠ¸ë¦­ êµ¬ì¡° ì§€ì›)"""
        stats = {
            'total': 0,
            'success': 0,
            'failed': 0,
            'skipped': 0
        }
        
        try:
            # ì „ì²´ ê²Œì‹œê¸€ ìˆ˜ í™•ì¸
            total_posts = self.source_collection.count_documents({})
            stats['total'] = total_posts
            
            logger.info(f"ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘: {total_posts}ê°œ ê²Œì‹œê¸€")
            logger.info(f"Dry run ëª¨ë“œ: {dry_run}")
            logger.info(f"ìƒˆë¡œìš´ ë©”íŠ¸ë¦­ êµ¬ì¡° ì§€ì›: top_likes, top_comments, top_views")
            
            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
            skip = 0
            while skip < total_posts:
                posts = list(self.source_collection.find({}).skip(skip).limit(batch_size))
                
                for post in posts:
                    try:
                        # ì´ë¯¸ ë§ˆì´ê·¸ë ˆì´ì…˜ëœ ê²Œì‹œê¸€ì¸ì§€ í™•ì¸
                        existing = self.target_collection.find_one({
                            'site': 'fmkorea',
                            'post_id': post.get('post_id')
                        })
                        
                        if existing:
                            logger.debug(f"ì´ë¯¸ ë§ˆì´ê·¸ë ˆì´ì…˜ë¨: {post.get('post_id')}")
                            stats['skipped'] += 1
                            continue
                        
                        # ë°ì´í„° ë³€í™˜
                        transformed = self.transform_post_data(post)
                        
                        # ë°ì´í„° ê²€ì¦
                        if not self.validate_transformed_data(post, transformed):
                            logger.error(f"ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨: {post.get('post_id')}")
                            stats['failed'] += 1
                            continue
                        
                        # ë©”íŠ¸ë¦­ ê³„ì‚° ë° ë¡œê¹…
                        metrics = self.calculate_post_metrics(transformed)
                        logger.debug(f"ê²Œì‹œê¸€ {post.get('post_id')} ë©”íŠ¸ë¦­: {metrics}")
                        
                        # ì‹¤ì œ ë§ˆì´ê·¸ë ˆì´ì…˜ (dry_runì´ ì•„ë‹Œ ê²½ìš°)
                        if not dry_run:
                            self.target_collection.insert_one(transformed)
                        
                        stats['success'] += 1
                        logger.debug(f"ë§ˆì´ê·¸ë ˆì´ì…˜ ì„±ê³µ: {post.get('post_id')}")
                        
                    except Exception as e:
                        logger.error(f"ê²Œì‹œê¸€ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨: {post.get('post_id', 'unknown')} - {e}")
                        stats['failed'] += 1
                
                skip += batch_size
                logger.info(f"ì§„í–‰ë¥ : {min(skip, total_posts)}/{total_posts} ({(min(skip, total_posts)/total_posts)*100:.1f}%)")
            
            logger.info("ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ")
            logger.info(f"í†µê³„: {stats}")
            
            return stats
            
        except Exception as e:
            logger.error(f"ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨: {e}")
            raise
    
    def rollback_migration(self) -> int:
        """ë§ˆì´ê·¸ë ˆì´ì…˜ ë¡¤ë°± (FMì½”ë¦¬ì•„ ë°ì´í„°ë§Œ ì‚­ì œ)"""
        try:
            result = self.target_collection.delete_many({'site': 'fmkorea'})
            deleted_count = result.deleted_count
            
            logger.info(f"ë¡¤ë°± ì™„ë£Œ: {deleted_count}ê°œ ê²Œì‹œê¸€ ì‚­ì œ")
            return deleted_count
            
        except Exception as e:
            logger.error(f"ë¡¤ë°± ì‹¤íŒ¨: {e}")
            raise
    
    def verify_migration(self) -> Dict[str, Any]:
        """ë§ˆì´ê·¸ë ˆì´ì…˜ ê²°ê³¼ ê²€ì¦"""
        try:
            source_count = self.source_collection.count_documents({})
            target_count = self.target_collection.count_documents({'site': 'fmkorea'})
            
            # ìƒ˜í”Œ ë°ì´í„° ë¹„êµ
            sample_source = self.source_collection.find_one({})
            sample_target = self.target_collection.find_one({'site': 'fmkorea'})
            
            verification = {
                'source_count': source_count,
                'target_count': target_count,
                'count_match': source_count == target_count,
                'sample_source_exists': sample_source is not None,
                'sample_target_exists': sample_target is not None,
                'migration_complete': source_count > 0 and target_count > 0 and source_count == target_count
            }
            
            logger.info(f"ë§ˆì´ê·¸ë ˆì´ì…˜ ê²€ì¦ ê²°ê³¼: {verification}")
            return verification
            
        except Exception as e:
            logger.error(f"ë§ˆì´ê·¸ë ˆì´ì…˜ ê²€ì¦ ì‹¤íŒ¨: {e}")
            raise
    
    def close(self):
        """ì—°ê²° ì¢…ë£Œ"""
        self.client.close()


async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    mongodb_uri = "mongodb+srv://nadle:miQXsXpzayvMQAzE@cluster0.bh7mhfi.mongodb.net/"
    
    migrator = FmkoreaToCommunitMigrator(mongodb_uri)
    
    try:
        logger.info("ğŸš€ FMì½”ë¦¬ì•„ -> ì»¤ë®¤ë‹ˆí‹° í¬ìŠ¤íŠ¸ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘")
        
        # 1. Dry runìœ¼ë¡œ í…ŒìŠ¤íŠ¸
        logger.info("1ï¸âƒ£ Dry run í…ŒìŠ¤íŠ¸")
        dry_run_stats = migrator.migrate_posts(batch_size=10, dry_run=True)
        
        if dry_run_stats['failed'] > 0:
            logger.error(f"Dry runì—ì„œ {dry_run_stats['failed']}ê°œ ì‹¤íŒ¨. ë§ˆì´ê·¸ë ˆì´ì…˜ ì¤‘ë‹¨.")
            return
        
        # 2. ì‹¤ì œ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰ ì—¬ë¶€ í™•ì¸
        user_input = input("ì‹¤ì œ ë§ˆì´ê·¸ë ˆì´ì…˜ì„ ì‹¤í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ")
        if user_input.lower() != 'y':
            logger.info("ë§ˆì´ê·¸ë ˆì´ì…˜ ì·¨ì†Œë¨")
            return
        
        # 3. ì‹¤ì œ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰
        logger.info("2ï¸âƒ£ ì‹¤ì œ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰")
        migration_stats = migrator.migrate_posts(batch_size=100, dry_run=False)
        
        # 4. ë§ˆì´ê·¸ë ˆì´ì…˜ ê²€ì¦
        logger.info("3ï¸âƒ£ ë§ˆì´ê·¸ë ˆì´ì…˜ ê²€ì¦")
        verification = migrator.verify_migration()
        
        if verification['migration_complete']:
            logger.info("âœ… ë§ˆì´ê·¸ë ˆì´ì…˜ ì„±ê³µ!")
        else:
            logger.error("âŒ ë§ˆì´ê·¸ë ˆì´ì…˜ ê²€ì¦ ì‹¤íŒ¨")
            
            # ë¡¤ë°± ì—¬ë¶€ í™•ì¸
            rollback_input = input("ë¡¤ë°±ì„ ì‹¤í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ")
            if rollback_input.lower() == 'y':
                migrator.rollback_migration()
        
    except Exception as e:
        logger.error(f"ë§ˆì´ê·¸ë ˆì´ì…˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise
    finally:
        migrator.close()


if __name__ == "__main__":
    asyncio.run(main()) 