"""
ìƒˆë¡œìš´ v2 ìŠ¤í¬ë˜í¼ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

ì£¼ìš” ê¸°ëŠ¥:
- FMKoreaScraperì™€ RuliwebScraper v2 í…ŒìŠ¤íŠ¸ (line 20-50)
- ê° ì»¤ë®¤ë‹ˆí‹° ì²« í˜ì´ì§€ ê²Œì‹œíŒ ëª©ë¡ ìŠ¤í¬ë˜í•‘ (line 52-100)
- ìƒìœ„ ê²Œì‹œê¸€ ìƒì„¸ ìŠ¤í¬ë˜í•‘ (line 102-150)
- JSON ê²°ê³¼ ì €ì¥ ë° ë¶„ì„ (line 152-200)

ì‘ì„±ì: AI Assistant
ì‘ì„±ì¼: 2025-01-28
ëª©ì : v2 ìŠ¤í¬ë˜í¼ ê²€ì¦ ë° ë°ì´í„° êµ¬ì¡° ë¶„ì„
"""

import asyncio
import json
import os
from datetime import datetime
from pathlib import Path
import sys
import pytz
import requests

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from scraping.scrapers.fmkorea_scraper_v2 import FMKoreaScraper
from scraping.scrapers.ruliweb_scraper_v2 import RuliwebScraper
from loguru import logger

# í•œêµ­ ì‹œê°„ëŒ€ ì„¤ì •
KST = pytz.timezone('Asia/Seoul')

# API ì„œë²„ URL
API_BASE_URL = "http://localhost:8000"

async def save_to_mongodb():
    """ìŠ¤í¬ë˜í•‘ ë°ì´í„°ë¥¼ MongoDBì— ì €ì¥"""
    try:
        logger.info("ğŸ—„ï¸ MongoDBì— ë°ì´í„° ì €ì¥ ì‹œì‘")
        
        response = requests.post(f"{API_BASE_URL}/community-posts/save")
        
        if response.status_code == 200:
            result = response.json()
            logger.info("âœ… MongoDB ì €ì¥ ì„±ê³µ")
            logger.info(f"   - ê²Œì‹œê¸€: ì €ì¥ {result['posts']['saved']}ê°œ, ì—…ë°ì´íŠ¸ {result['posts']['updated']}ê°œ")
            logger.info(f"   - ìƒì„¸ ê²Œì‹œê¸€: ì €ì¥ {result['detailed_posts']['saved']}ê°œ, ì—…ë°ì´íŠ¸ {result['detailed_posts']['updated']}ê°œ")
            return True
        else:
            logger.error(f"âŒ MongoDB ì €ì¥ ì‹¤íŒ¨: {response.status_code} - {response.text}")
            return False
            
    except Exception as e:
        logger.error(f"âŒ MongoDB ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
        return False

async def test_api_connection():
    """API ì„œë²„ ì—°ê²° í…ŒìŠ¤íŠ¸"""
    try:
        response = requests.get(f"{API_BASE_URL}/health")
        if response.status_code == 200:
            logger.info("âœ… API ì„œë²„ ì—°ê²° ì„±ê³µ")
            return True
        else:
            logger.warning(f"âš ï¸ API ì„œë²„ ì‘ë‹µ ì´ìƒ: {response.status_code}")
            return False
    except Exception as e:
        logger.warning(f"âš ï¸ API ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {e}")
        return False

async def test_fmkorea_scraper():
    """FMì½”ë¦¬ì•„ ìŠ¤í¬ë˜í¼ v2 í…ŒìŠ¤íŠ¸"""
    logger.info("ğŸ§ª FMì½”ë¦¬ì•„ ìŠ¤í¬ë˜í¼ v2 í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    # í…ŒìŠ¤íŠ¸í•  ê²Œì‹œíŒ URL (ì •ì¹˜/ì‹œì‚¬ ê²Œì‹œíŒ)
    board_url = "https://www.fmkorea.com/politics"
    
    try:
        async with FMKoreaScraper() as scraper:
            # 1. ê²Œì‹œíŒ ëª©ë¡ ìŠ¤í¬ë˜í•‘
            logger.info("ğŸ“‹ FMì½”ë¦¬ì•„ ê²Œì‹œíŒ ëª©ë¡ ìŠ¤í¬ë˜í•‘ ì¤‘...")
            board_posts = await scraper.scrape_board_list(board_url)
            
            if not board_posts:
                logger.warning("âš ï¸ FMì½”ë¦¬ì•„ ê²Œì‹œíŒ ëª©ë¡ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                # ë””ë²„ê¹…ì„ ìœ„í•´ í˜ì´ì§€ ì œëª© í™•ì¸
                try:
                    page_title = await scraper.page.title()
                    logger.info(f"ğŸ” í˜„ì¬ í˜ì´ì§€ ì œëª©: {page_title}")
                    
                    # í˜ì´ì§€ HTML ì¼ë¶€ í™•ì¸
                    body_text = await scraper.page.evaluate("() => document.body.innerText.substring(0, 200)")
                    logger.info(f"ğŸ” í˜ì´ì§€ ë‚´ìš© ì¼ë¶€: {body_text}")
                except:
                    pass
                
                return {"board_posts": [], "detailed_posts": []}
            
            logger.info(f"âœ… FMì½”ë¦¬ì•„ ê²Œì‹œíŒ ëª©ë¡ {len(board_posts)}ê°œ ì¶”ì¶œ ì™„ë£Œ")
            
            # 2. ìƒìœ„ 3ê°œ ê²Œì‹œê¸€ ìƒì„¸ ìŠ¤í¬ë˜í•‘
            detailed_posts = []
            for i, post in enumerate(board_posts[:3]):
                try:
                    logger.info(f"ğŸ“„ FMì½”ë¦¬ì•„ ê²Œì‹œê¸€ {i+1}/3 ìƒì„¸ ìŠ¤í¬ë˜í•‘: {post.get('title', 'Unknown')[:30]}...")
                    
                    post_detail = await scraper.scrape_post_detail(post['post_url'])
                    if post_detail:
                        detailed_posts.append(post_detail)
                        logger.info(f"âœ… FMì½”ë¦¬ì•„ ê²Œì‹œê¸€ {i+1} ìƒì„¸ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ")
                    
                    # ìš”ì²­ ê°„ê²© (ì„œë²„ ë¶€í•˜ ë°©ì§€)
                    await asyncio.sleep(2)
                    
                except Exception as e:
                    logger.error(f"ğŸ’¥ FMì½”ë¦¬ì•„ ê²Œì‹œê¸€ {i+1} ìƒì„¸ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
                    continue
            
            return {
                "board_posts": board_posts,
                "detailed_posts": detailed_posts
            }
            
    except Exception as e:
        logger.error(f"ğŸ’¥ FMì½”ë¦¬ì•„ ìŠ¤í¬ë˜í¼ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return {"board_posts": [], "detailed_posts": []}


async def test_ruliweb_scraper():
    """ë£¨ë¦¬ì›¹ ìŠ¤í¬ë˜í¼ v2 í…ŒìŠ¤íŠ¸"""
    logger.info("ğŸ§ª ë£¨ë¦¬ì›¹ ìŠ¤í¬ë˜í¼ v2 í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    # í…ŒìŠ¤íŠ¸í•  ê²Œì‹œíŒ URL (ìƒˆë¡œìš´ ê²Œì‹œíŒ)
    board_url = "https://bbs.ruliweb.com/community/board/300148"
    
    try:
        async with RuliwebScraper() as scraper:
            # 1. ê²Œì‹œíŒ ëª©ë¡ ìŠ¤í¬ë˜í•‘
            logger.info("ğŸ“‹ ë£¨ë¦¬ì›¹ ê²Œì‹œíŒ ëª©ë¡ ìŠ¤í¬ë˜í•‘ ì¤‘...")
            board_posts = await scraper.scrape_board_list(board_url)
            
            if not board_posts:
                logger.warning("âš ï¸ ë£¨ë¦¬ì›¹ ê²Œì‹œíŒ ëª©ë¡ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                return {"board_posts": [], "detailed_posts": []}
            
            logger.info(f"âœ… ë£¨ë¦¬ì›¹ ê²Œì‹œíŒ ëª©ë¡ {len(board_posts)}ê°œ ì¶”ì¶œ ì™„ë£Œ")
            
            # 2. ìƒìœ„ 3ê°œ ê²Œì‹œê¸€ ìƒì„¸ ìŠ¤í¬ë˜í•‘
            detailed_posts = []
            for i, post in enumerate(board_posts[:3]):
                try:
                    logger.info(f"ğŸ“„ ë£¨ë¦¬ì›¹ ê²Œì‹œê¸€ {i+1}/3 ìƒì„¸ ìŠ¤í¬ë˜í•‘: {post.get('title', 'Unknown')[:30]}...")
                    
                    post_detail = await scraper.scrape_post_detail(post['post_url'])
                    if post_detail:
                        detailed_posts.append(post_detail)
                        logger.info(f"âœ… ë£¨ë¦¬ì›¹ ê²Œì‹œê¸€ {i+1} ìƒì„¸ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ")
                    
                    # ìš”ì²­ ê°„ê²© (ì„œë²„ ë¶€í•˜ ë°©ì§€)
                    await asyncio.sleep(2)
                    
                except Exception as e:
                    logger.error(f"ğŸ’¥ ë£¨ë¦¬ì›¹ ê²Œì‹œê¸€ {i+1} ìƒì„¸ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
                    continue
            
            return {
                "board_posts": board_posts,
                "detailed_posts": detailed_posts
            }
            
    except Exception as e:
        logger.error(f"ğŸ’¥ ë£¨ë¦¬ì›¹ ìŠ¤í¬ë˜í¼ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return {"board_posts": [], "detailed_posts": []}


def save_results_to_json(results: dict, filename: str):
    """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
    try:
        # ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
        results_dir = project_root / "results"
        results_dir.mkdir(exist_ok=True)
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        full_filename = f"{timestamp}_{filename}"
        
        filepath = results_dir / full_filename
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2, default=str)
        
        logger.info(f"âœ… ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {filepath}")
        return str(filepath)
        
    except Exception as e:
        logger.error(f"ğŸ’¥ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
        return None


def analyze_results(results: dict):
    """ìŠ¤í¬ë˜í•‘ ê²°ê³¼ ë¶„ì„ ë° ìš”ì•½"""
    logger.info("ğŸ“Š ìŠ¤í¬ë˜í•‘ ê²°ê³¼ ë¶„ì„ ì‹œì‘")
    
    analysis = {
        "summary": {},
        "data_structure": {},
        "issues": []
    }
    
    for site_name, site_data in results.items():
        if not site_data:
            continue
            
        board_posts = site_data.get("board_posts", [])
        detailed_posts = site_data.get("detailed_posts", [])
        
        # ê¸°ë³¸ í†µê³„
        analysis["summary"][site_name] = {
            "board_posts_count": len(board_posts),
            "detailed_posts_count": len(detailed_posts),
            "success_rate": len(detailed_posts) / max(len(board_posts[:3]), 1) * 100
        }
        
        # ë°ì´í„° êµ¬ì¡° ë¶„ì„
        if board_posts:
            sample_board_post = board_posts[0]
            analysis["data_structure"][f"{site_name}_board_post"] = list(sample_board_post.keys())
        
        if detailed_posts:
            sample_detailed_post = detailed_posts[0]
            analysis["data_structure"][f"{site_name}_detailed_post"] = {
                "top_level_keys": list(sample_detailed_post.keys()),
                "metadata_keys": list(sample_detailed_post.get("metadata", {}).keys()),
                "content_count": len(sample_detailed_post.get("content", [])),
                "comments_count": len(sample_detailed_post.get("comments", []))
            }
            
            # ì½˜í…ì¸  íƒ€ì… ë¶„ì„
            content_types = {}
            for content in sample_detailed_post.get("content", []):
                content_type = content.get("type", "unknown")
                content_types[content_type] = content_types.get(content_type, 0) + 1
            
            analysis["data_structure"][f"{site_name}_content_types"] = content_types
    
    logger.info("âœ… ìŠ¤í¬ë˜í•‘ ê²°ê³¼ ë¶„ì„ ì™„ë£Œ")
    return analysis


async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("ğŸš€ v2 ìŠ¤í¬ë˜í¼ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    # API ì„œë²„ ì—°ê²° í…ŒìŠ¤íŠ¸
    api_connected = await test_api_connection()
    
    results = {}
    
    # FMì½”ë¦¬ì•„ í…ŒìŠ¤íŠ¸
    logger.info("=" * 50)
    fmkorea_results = await test_fmkorea_scraper()
    results["fmkorea"] = fmkorea_results
    
    # ì‚¬ì´íŠ¸ ê°„ ê°„ê²©
    await asyncio.sleep(5)
    
    # ë£¨ë¦¬ì›¹ í…ŒìŠ¤íŠ¸
    logger.info("=" * 50)
    ruliweb_results = await test_ruliweb_scraper()
    results["ruliweb"] = ruliweb_results
    
    # ê²°ê³¼ ì €ì¥
    logger.info("=" * 50)
    json_path = save_results_to_json(results, "scraper_v2_test_results.json")
    
    # ê²°ê³¼ ë¶„ì„
    analysis = analyze_results(results)
    analysis_path = save_results_to_json(analysis, "scraper_v2_analysis.json")
    
    # MongoDB ì €ì¥ (API ì„œë²„ê°€ ì—°ê²°ëœ ê²½ìš°)
    if api_connected:
        logger.info("=" * 50)
        mongodb_saved = await save_to_mongodb()
        if mongodb_saved:
            logger.info("âœ… MongoDB ì €ì¥ ì™„ë£Œ - ë¸Œë¼ìš°ì €ì—ì„œ í™•ì¸ ê°€ëŠ¥")
            logger.info(f"ğŸŒ API ì—”ë“œí¬ì¸íŠ¸: {API_BASE_URL}/community-posts")
        else:
            logger.warning("âš ï¸ MongoDB ì €ì¥ ì‹¤íŒ¨ - íŒŒì¼ ê¸°ë°˜ìœ¼ë¡œë§Œ í™•ì¸ ê°€ëŠ¥")
    else:
        logger.warning("âš ï¸ API ì„œë²„ ë¯¸ì—°ê²° - MongoDB ì €ì¥ ê±´ë„ˆëœ€")
    
    # ìš”ì•½ ì¶œë ¥
    logger.info("=" * 50)
    logger.info("ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½:")
    for site_name, summary in analysis.get("summary", {}).items():
        logger.info(f"  {site_name}:")
        logger.info(f"    - ê²Œì‹œíŒ ëª©ë¡: {summary['board_posts_count']}ê°œ")
        logger.info(f"    - ìƒì„¸ ê²Œì‹œê¸€: {summary['detailed_posts_count']}ê°œ")
        logger.info(f"    - ì„±ê³µë¥ : {summary['success_rate']:.1f}%")
    
    logger.info(f"ğŸ‰ v2 ìŠ¤í¬ë˜í¼ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    logger.info(f"ğŸ“ ê²°ê³¼ íŒŒì¼: {json_path}")
    logger.info(f"ğŸ“ ë¶„ì„ íŒŒì¼: {analysis_path}")
    
    if api_connected:
        logger.info("ğŸŒ ë¸Œë¼ìš°ì €ì—ì„œ í™•ì¸:")
        logger.info(f"   - ê²Œì‹œê¸€ ëª©ë¡: {API_BASE_URL}/community-posts")
        logger.info(f"   - í†µê³„: {API_BASE_URL}/stats")


if __name__ == "__main__":
    asyncio.run(main()) 