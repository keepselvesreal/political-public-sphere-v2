"""
CommunityPost ëª¨ë¸ë¡œ ì§ì ‘ ìŠ¤í¬ë˜í•‘ ë° ì €ì¥ (Playwright ë²„ì „)

ì£¼ìš” ê¸°ëŠ¥:
- Playwright + Stealth ëª¨ë“œë¡œ ë´‡ ì°¨ë‹¨ ìš°íšŒ (line 20-60)
- CommunityPost êµ¬ì¡°ë¡œ ì§ì ‘ ì €ì¥ (line 61-120)
- ì •ì‹ ìŠ¤í¬ë˜í¼ ë°©ì‹ ì ìš© (line 121-160)

ì‘ì„±ì: AI Assistant
ì‘ì„±ì¼: 2025-01-28
ëª©ì : ìƒˆë¡œìš´ ëª¨ë¸ êµ¬ì¡°ë¡œ ì‹¤ì œ ìŠ¤í¬ë˜í•‘
"""

import asyncio
import pymongo
from datetime import datetime
from typing import Dict, List, Any, Optional
from loguru import logger
import sys
import os

# ì •ì‹ ìŠ¤í¬ë˜í¼ import
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'scraping', 'scrapers'))
from fmkorea_scraper import FMKoreaScraper

from playwright.async_api import async_playwright
from playwright_stealth import stealth_async


class CommunityPostPlaywrightScraper:
    """Playwright ê¸°ë°˜ CommunityPost ìŠ¤í¬ë˜í¼"""
    
    def __init__(self, mongodb_uri: str):
        self.client = pymongo.MongoClient(mongodb_uri)
        self.db = self.client.political_public_sphere
        self.collection = self.db.community_posts
        
        # ì •ì‹ ìŠ¤í¬ë˜í¼ ì„¤ì • ì ìš©
        self.base_url = "https://www.fmkorea.com"
        
    async def scrape_main_page_posts(self) -> List[Dict[str, Any]]:
        """ë©”ì¸ í˜ì´ì§€ì—ì„œ ê²Œì‹œê¸€ URL ëª©ë¡ ìˆ˜ì§‘"""
        try:
            logger.info("ğŸš€ Playwrightë¡œ ë©”ì¸ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘ ì‹œì‘")
            
            playwright = await async_playwright().start()
            
            # ì •ì‹ ìŠ¤í¬ë˜í¼ì™€ ë™ì¼í•œ ë¸Œë¼ìš°ì € ì„¤ì •
            browser = await playwright.chromium.launch(
                headless=False,  # headless=Falseë¡œ ë´‡ ì°¨ë‹¨ ìš°íšŒ
                args=[
                    '--no-sandbox',
                    '--disable-dev-shm-usage',
                    '--disable-blink-features=AutomationControlled',
                    '--window-size=1920,1080'
                ]
            )
            
            page = await browser.new_page()
            
            # Stealth ëª¨ë“œ ì ìš©
            await stealth_async(page)
            
            # ì¶”ê°€ ì„¤ì •
            await page.set_viewport_size({"width": 1920, "height": 1080})
            await page.set_extra_http_headers({
                'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8'
            })
            
            # FMì½”ë¦¬ì•„ ë©”ì¸ í˜ì´ì§€ ì ‘ì†
            await page.goto(self.base_url, wait_until="networkidle", timeout=30000)
            
            # í˜ì´ì§€ ë¡œë“œ í™•ì¸
            page_title = await page.title()
            logger.info(f"ğŸ“„ í˜ì´ì§€ ì œëª©: {page_title}")
            
            # ê²Œì‹œê¸€ ë§í¬ ìˆ˜ì§‘
            post_links = []
            
            # ë‹¤ì–‘í•œ ì…€ë ‰í„°ë¡œ ê²Œì‹œê¸€ ë§í¬ ì°¾ê¸°
            link_selectors = [
                'a[href*="document_srl"]',
                'a[href*="/board/"]',
                'a[href*="srl="]',
                'a[href*="fmkorea.com/"]'
            ]
            
            for selector in link_selectors:
                try:
                    elements = await page.query_selector_all(selector)
                    logger.info(f"ì…€ë ‰í„° '{selector}': {len(elements)}ê°œ ë§í¬ ë°œê²¬")
                    
                    for element in elements[:10]:  # ì²˜ìŒ 10ê°œë§Œ
                        try:
                            href = await element.get_attribute('href')
                            if href and 'document_srl=' in href:
                                # ì ˆëŒ€ URLë¡œ ë³€í™˜
                                if href.startswith('/'):
                                    href = self.base_url + href
                                
                                # ì œëª© ì¶”ì¶œ
                                title = await element.inner_text()
                                title = title.strip() if title else 'ì œëª© ì—†ìŒ'
                                
                                # post_id ì¶”ì¶œ
                                import re
                                post_id_match = re.search(r'document_srl=(\d+)', href)
                                if post_id_match:
                                    post_id = post_id_match.group(1)
                                    
                                    post_links.append({
                                        'post_id': post_id,
                                        'post_url': href,
                                        'title': title
                                    })
                                    
                                    logger.info(f"ê²Œì‹œê¸€ ë°œê²¬: {post_id} - {title[:30]}...")
                        except Exception as e:
                            logger.warning(f"ë§í¬ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                            continue
                    
                    if post_links:
                        break  # ë§í¬ë¥¼ ì°¾ì•˜ìœ¼ë©´ ë‹¤ë¥¸ ì…€ë ‰í„°ëŠ” ì‹œë„í•˜ì§€ ì•ŠìŒ
                        
                except Exception as e:
                    logger.warning(f"ì…€ë ‰í„° '{selector}' ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    continue
            
            await browser.close()
            await playwright.stop()
            
            logger.info(f"âœ… ì´ {len(post_links)}ê°œ ê²Œì‹œê¸€ ë§í¬ ìˆ˜ì§‘ ì™„ë£Œ")
            return post_links
            
        except Exception as e:
            logger.error(f"ğŸ’¥ ë©”ì¸ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
            return []
    
    async def scrape_post_with_official_scraper(self, post_url: str) -> Optional[Dict[str, Any]]:
        """ì •ì‹ ìŠ¤í¬ë˜í¼ë¡œ ê²Œì‹œê¸€ ìƒì„¸ ìŠ¤í¬ë˜í•‘"""
        try:
            logger.info(f"ğŸ” ì •ì‹ ìŠ¤í¬ë˜í¼ë¡œ ìƒì„¸ ìŠ¤í¬ë˜í•‘: {post_url}")
            
            # ì •ì‹ ìŠ¤í¬ë˜í¼ ì‚¬ìš©
            async with FMKoreaScraper() as scraper:
                experiment_data = await scraper.scrape_post_detail(post_url)
                
                if experiment_data and 'error' not in experiment_data:
                    # CommunityPost ëª¨ë¸ êµ¬ì¡°ë¡œ ë³€í™˜
                    community_post = self.convert_to_community_post(experiment_data)
                    return community_post
                else:
                    logger.warning(f"ì •ì‹ ìŠ¤í¬ë˜í¼ ì‹¤íŒ¨: {experiment_data.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
                    return None
                    
        except Exception as e:
            logger.error(f"ğŸ’¥ ì •ì‹ ìŠ¤í¬ë˜í¼ ì‚¬ìš© ì‹¤íŒ¨: {e}")
            return None
    
    def convert_to_community_post(self, experiment_data: Dict[str, Any]) -> Dict[str, Any]:
        """ì •ì‹ ìŠ¤í¬ë˜í¼ ê²°ê³¼ë¥¼ CommunityPost ëª¨ë¸ë¡œ ë³€í™˜"""
        try:
            now = datetime.now()
            
            community_post = {
                'post_id': experiment_data.get('post_id'),
                'post_url': experiment_data.get('post_url'),
                'site': 'fmkorea',
                'scraped_at': experiment_data.get('scraped_at', now.isoformat()),
                'metadata': {
                    'title': experiment_data.get('metadata', {}).get('title', 'ì œëª© ì—†ìŒ'),
                    'author': experiment_data.get('metadata', {}).get('author', 'ìµëª…'),
                    'date': experiment_data.get('metadata', {}).get('date', now.isoformat()),
                    'view_count': experiment_data.get('metadata', {}).get('view_count', 0),
                    'like_count': experiment_data.get('metadata', {}).get('like_count', 0),
                    'dislike_count': experiment_data.get('metadata', {}).get('dislike_count', 0),
                    'comment_count': experiment_data.get('metadata', {}).get('comment_count', 0)
                },
                'content': experiment_data.get('content', []),
                'comments': experiment_data.get('comments', []),
                'created_at': now,
                'updated_at': now
            }
            
            return community_post
            
        except Exception as e:
            logger.error(f"ğŸ’¥ CommunityPost ë³€í™˜ ì‹¤íŒ¨: {e}")
            return {}
    
    async def save_post(self, post: Dict[str, Any]) -> bool:
        """ê²Œì‹œê¸€ ì €ì¥"""
        try:
            # ì¤‘ë³µ í™•ì¸
            existing = self.collection.find_one({
                'site': 'fmkorea',
                'post_id': post['post_id']
            })
            
            if existing:
                logger.debug(f"ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ê²Œì‹œê¸€: {post['post_id']}")
                return False
            
            # ì €ì¥
            result = self.collection.insert_one(post)
            
            if result.inserted_id:
                logger.info(f"âœ… ê²Œì‹œê¸€ ì €ì¥ ì„±ê³µ: {post['post_id']} - {post['metadata']['title'][:30]}...")
                return True
            else:
                logger.error(f"âŒ ê²Œì‹œê¸€ ì €ì¥ ì‹¤íŒ¨: {post['post_id']}")
                return False
                
        except Exception as e:
            logger.error(f"ğŸ’¥ ê²Œì‹œê¸€ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    async def scrape_and_save(self, max_posts: int = 5) -> Dict[str, int]:
        """ìŠ¤í¬ë˜í•‘ ë° ì €ì¥ ì‹¤í–‰"""
        stats = {
            'total_found': 0,
            'total_scraped': 0,
            'total_saved': 0,
            'errors': 0
        }
        
        try:
            logger.info(f"ğŸš€ Playwright ê¸°ë°˜ CommunityPost ìŠ¤í¬ë˜í•‘ ì‹œì‘ (ìµœëŒ€ {max_posts}ê°œ)")
            
            # 1. ë©”ì¸ í˜ì´ì§€ì—ì„œ ê²Œì‹œê¸€ ë§í¬ ìˆ˜ì§‘
            post_links = await self.scrape_main_page_posts()
            stats['total_found'] = len(post_links)
            
            if not post_links:
                logger.warning("âš ï¸ ê²Œì‹œê¸€ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return stats
            
            # 2. ê° ê²Œì‹œê¸€ ìƒì„¸ ìŠ¤í¬ë˜í•‘
            processed_count = 0
            for post_link in post_links[:max_posts]:
                try:
                    logger.info(f"ğŸ“ ì²˜ë¦¬ ì¤‘: {processed_count + 1}/{min(max_posts, len(post_links))}")
                    
                    # ì •ì‹ ìŠ¤í¬ë˜í¼ë¡œ ìƒì„¸ ìŠ¤í¬ë˜í•‘
                    community_post = await self.scrape_post_with_official_scraper(post_link['post_url'])
                    
                    if community_post:
                        stats['total_scraped'] += 1
                        
                        # ì €ì¥
                        if await self.save_post(community_post):
                            stats['total_saved'] += 1
                    else:
                        stats['errors'] += 1
                    
                    processed_count += 1
                    
                    # ë”œë ˆì´ (ì„œë²„ ë¶€í•˜ ë°©ì§€)
                    await asyncio.sleep(3)
                    
                except Exception as e:
                    logger.error(f"ğŸ’¥ ê²Œì‹œê¸€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    stats['errors'] += 1
                    continue
            
            logger.info(f"âœ… ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: {stats}")
            return stats
            
        except Exception as e:
            logger.error(f"ğŸ’¥ ìŠ¤í¬ë˜í•‘ ì¤‘ ì˜¤ë¥˜: {e}")
            raise
    
    def get_collection_stats(self) -> Dict[str, int]:
        """ì €ì¥ëœ ë°ì´í„° í†µê³„"""
        try:
            total_count = self.collection.count_documents({'site': 'fmkorea'})
            
            # ìµœê·¼ ì €ì¥ëœ ê²Œì‹œê¸€
            recent_posts = list(self.collection.find(
                {'site': 'fmkorea'}, 
                {'post_id': 1, 'metadata.title': 1, 'created_at': 1}
            ).sort('created_at', -1).limit(5))
            
            return {
                'total_posts': total_count,
                'recent_posts': recent_posts
            }
            
        except Exception as e:
            logger.error(f"ğŸ’¥ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {'total_posts': 0, 'recent_posts': []}
    
    def close(self):
        """ì—°ê²° ì¢…ë£Œ"""
        self.client.close()


async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    mongodb_uri = "mongodb+srv://nadle:miQXsXpzayvMQAzE@cluster0.bh7mhfi.mongodb.net/"
    
    scraper = CommunityPostPlaywrightScraper(mongodb_uri)
    
    try:
        logger.info("ğŸš€ Playwright ê¸°ë°˜ CommunityPost ìŠ¤í¬ë˜í•‘ í”„ë¡œì„¸ìŠ¤ ì‹œì‘")
        
        # 1. í˜„ì¬ ìƒíƒœ í™•ì¸
        logger.info("1ï¸âƒ£ í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ í™•ì¸")
        current_stats = scraper.get_collection_stats()
        logger.info(f"í˜„ì¬ ì €ì¥ëœ ê²Œì‹œê¸€: {current_stats['total_posts']}ê°œ")
        
        # 2. ìŠ¤í¬ë˜í•‘ ì„¤ì • í™•ì¸
        max_posts = 3  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ 3ê°œë§Œ
        
        logger.info(f"ìŠ¤í¬ë˜í•‘ ì„¤ì •: ìµœëŒ€ {max_posts}ê°œ ê²Œì‹œê¸€")
        
        user_input = input(f"\nPlaywright ìŠ¤í¬ë˜í•‘ì„ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ")
        if user_input.lower() != 'y':
            logger.info("ìŠ¤í¬ë˜í•‘ ì·¨ì†Œë¨")
            return
        
        # 3. ìŠ¤í¬ë˜í•‘ ì‹¤í–‰
        logger.info("2ï¸âƒ£ Playwright ìŠ¤í¬ë˜í•‘ ì‹¤í–‰")
        scraping_stats = await scraper.scrape_and_save(max_posts=max_posts)
        
        # 4. ê²°ê³¼ í™•ì¸
        logger.info("3ï¸âƒ£ ìŠ¤í¬ë˜í•‘ ê²°ê³¼ í™•ì¸")
        final_stats = scraper.get_collection_stats()
        
        logger.info("âœ… Playwright ìŠ¤í¬ë˜í•‘ ì™„ë£Œ!")
        logger.info(f"ìŠ¤í¬ë˜í•‘ í†µê³„: {scraping_stats}")
        logger.info(f"ìµœì¢… ì €ì¥ëœ ê²Œì‹œê¸€: {final_stats['total_posts']}ê°œ")
        
        if final_stats['recent_posts']:
            logger.info("ìµœê·¼ ì €ì¥ëœ ê²Œì‹œê¸€:")
            for post in final_stats['recent_posts']:
                logger.info(f"  - {post['post_id']}: {post['metadata']['title'][:50]}...")
        
    except Exception as e:
        logger.error(f"ğŸ’¥ ìŠ¤í¬ë˜í•‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise
    finally:
        scraper.close()


if __name__ == "__main__":
    asyncio.run(main()) 