"""
CommunityPost ëª¨ë¸ë¡œ ì§ì ‘ ìŠ¤í¬ë˜í•‘ ë° ì €ì¥

ì£¼ìš” ê¸°ëŠ¥:
- FMì½”ë¦¬ì•„ ìŠ¤í¬ë˜í•‘ (line 20-60)
- CommunityPost êµ¬ì¡°ë¡œ ì§ì ‘ ì €ì¥ (line 61-120)
- ìƒˆë¡œìš´ ë©”íŠ¸ë¦­ êµ¬ì¡° ì§€ì› (line 121-160)
- ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™© í‘œì‹œ (line 161-200)

ì‘ì„±ì: AI Assistant
ì‘ì„±ì¼: 2025-01-28
ëª©ì : ìƒˆë¡œìš´ ëª¨ë¸ êµ¬ì¡°ë¡œ ì§ì ‘ ìŠ¤í¬ë˜í•‘
"""

import asyncio
import aiohttp
import pymongo
from datetime import datetime
from typing import Dict, List, Any, Optional
from loguru import logger
from bs4 import BeautifulSoup
import re
import time
import random


class CommunityPostScraper:
    """CommunityPost ëª¨ë¸ë¡œ ì§ì ‘ ìŠ¤í¬ë˜í•‘í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, mongodb_uri: str):
        self.client = pymongo.MongoClient(mongodb_uri)
        self.db = self.client.political_public_sphere
        self.collection = self.db.community_posts
        
        # ìŠ¤í¬ë˜í•‘ ì„¤ì •
        self.base_url = "https://www.fmkorea.com"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
    async def scrape_post_list(self, session: aiohttp.ClientSession, page: int = 1) -> List[Dict[str, Any]]:
        """ê²Œì‹œê¸€ ëª©ë¡ ìŠ¤í¬ë˜í•‘"""
        try:
            # FMì½”ë¦¬ì•„ ììœ ê²Œì‹œíŒ URL
            url = f"{self.base_url}/index.php?mid=best&page={page}"
            
            async with session.get(url, headers=self.headers) as response:
                if response.status != 200:
                    logger.error(f"í˜ì´ì§€ {page} ìš”ì²­ ì‹¤íŒ¨: {response.status}")
                    return []
                
                html = await response.text()
                soup = BeautifulSoup(html, 'html.parser')
                
                posts = []
                
                # ê²Œì‹œê¸€ ëª©ë¡ íŒŒì‹±
                post_elements = soup.select('.bd_lst_wrp .bd_lst')
                
                for element in post_elements:
                    try:
                        # ê²Œì‹œê¸€ ë§í¬ ì¶”ì¶œ
                        link_elem = element.select_one('.hx a')
                        if not link_elem:
                            continue
                            
                        post_url = link_elem.get('href')
                        if not post_url:
                            continue
                            
                        # ì ˆëŒ€ URLë¡œ ë³€í™˜
                        if post_url.startswith('/'):
                            post_url = self.base_url + post_url
                        
                        # post_id ì¶”ì¶œ
                        post_id_match = re.search(r'document_srl=(\d+)', post_url)
                        if not post_id_match:
                            continue
                        
                        post_id = post_id_match.group(1)
                        
                        # ì œëª© ì¶”ì¶œ
                        title = link_elem.get_text(strip=True)
                        
                        # ì‘ì„±ì ì¶”ì¶œ
                        author_elem = element.select_one('.author')
                        author = author_elem.get_text(strip=True) if author_elem else 'ìµëª…'
                        
                        # í†µê³„ ì •ë³´ ì¶”ì¶œ
                        stats = self.extract_post_stats(element)
                        
                        post_data = {
                            'post_id': post_id,
                            'post_url': post_url,
                            'title': title,
                            'author': author,
                            'stats': stats
                        }
                        
                        posts.append(post_data)
                        
                    except Exception as e:
                        logger.warning(f"ê²Œì‹œê¸€ íŒŒì‹± ì‹¤íŒ¨: {e}")
                        continue
                
                logger.info(f"í˜ì´ì§€ {page}: {len(posts)}ê°œ ê²Œì‹œê¸€ ë°œê²¬")
                return posts
                
        except Exception as e:
            logger.error(f"í˜ì´ì§€ {page} ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
            return []
    
    def extract_post_stats(self, element) -> Dict[str, int]:
        """ê²Œì‹œê¸€ í†µê³„ ì •ë³´ ì¶”ì¶œ"""
        stats = {
            'view_count': 0,
            'like_count': 0,
            'dislike_count': 0,
            'comment_count': 0
        }
        
        try:
            # ì¡°íšŒìˆ˜ ì¶”ì¶œ
            view_elem = element.select_one('.view')
            if view_elem:
                view_text = view_elem.get_text(strip=True)
                view_match = re.search(r'(\d+)', view_text)
                if view_match:
                    stats['view_count'] = int(view_match.group(1))
            
            # ì¶”ì²œìˆ˜ ì¶”ì¶œ
            like_elem = element.select_one('.vote_up')
            if like_elem:
                like_text = like_elem.get_text(strip=True)
                like_match = re.search(r'(\d+)', like_text)
                if like_match:
                    stats['like_count'] = int(like_match.group(1))
            
            # ëŒ“ê¸€ìˆ˜ ì¶”ì¶œ
            comment_elem = element.select_one('.comment')
            if comment_elem:
                comment_text = comment_elem.get_text(strip=True)
                comment_match = re.search(r'(\d+)', comment_text)
                if comment_match:
                    stats['comment_count'] = int(comment_match.group(1))
                    
        except Exception as e:
            logger.warning(f"í†µê³„ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        return stats
    
    async def scrape_post_detail(self, session: aiohttp.ClientSession, post_data: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """ê²Œì‹œê¸€ ìƒì„¸ ë‚´ìš© ìŠ¤í¬ë˜í•‘"""
        try:
            async with session.get(post_data['post_url'], headers=self.headers) as response:
                if response.status != 200:
                    logger.error(f"ê²Œì‹œê¸€ {post_data['post_id']} ìš”ì²­ ì‹¤íŒ¨: {response.status}")
                    return None
                
                html = await response.text()
                soup = BeautifulSoup(html, 'html.parser')
                
                # ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œ
                content = self.extract_post_content(soup)
                
                # ëŒ“ê¸€ ì¶”ì¶œ
                comments = self.extract_comments(soup)
                
                # CommunityPost êµ¬ì¡°ë¡œ ë³€í™˜
                community_post = self.convert_to_community_post(post_data, content, comments)
                
                return community_post
                
        except Exception as e:
            logger.error(f"ê²Œì‹œê¸€ {post_data['post_id']} ìƒì„¸ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
            return None
    
    def extract_post_content(self, soup: BeautifulSoup) -> List[Dict[str, Any]]:
        """ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œ"""
        content = []
        
        try:
            # ê²Œì‹œê¸€ ë³¸ë¬¸ ì˜ì—­ ì°¾ê¸°
            content_area = soup.select_one('.rd_body')
            if not content_area:
                return content
            
            order = 0
            
            # í…ìŠ¤íŠ¸ ë‚´ìš© ì¶”ì¶œ
            text_content = content_area.get_text(strip=True)
            if text_content:
                content.append({
                    'type': 'text',
                    'order': order,
                    'data': {'text': text_content}
                })
                order += 1
            
            # ì´ë¯¸ì§€ ì¶”ì¶œ
            images = content_area.select('img')
            for img in images:
                src = img.get('src')
                if src:
                    content.append({
                        'type': 'image',
                        'order': order,
                        'data': {
                            'src': src,
                            'alt': img.get('alt', ''),
                            'width': img.get('width', ''),
                            'height': img.get('height', '')
                        }
                    })
                    order += 1
                    
        except Exception as e:
            logger.warning(f"ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        return content
    
    def extract_comments(self, soup: BeautifulSoup) -> List[Dict[str, Any]]:
        """ëŒ“ê¸€ ì¶”ì¶œ"""
        comments = []
        
        try:
            # ëŒ“ê¸€ ì˜ì—­ ì°¾ê¸°
            comment_elements = soup.select('.cmt_lst .cmt')
            
            for idx, element in enumerate(comment_elements):
                try:
                    # ì‘ì„±ì ì¶”ì¶œ
                    author_elem = element.select_one('.author')
                    author = author_elem.get_text(strip=True) if author_elem else 'ìµëª…'
                    
                    # ëŒ“ê¸€ ë‚´ìš© ì¶”ì¶œ
                    content_elem = element.select_one('.cmt_content')
                    content = content_elem.get_text(strip=True) if content_elem else ''
                    
                    # ë‚ ì§œ ì¶”ì¶œ
                    date_elem = element.select_one('.date')
                    date = date_elem.get_text(strip=True) if date_elem else ''
                    
                    comment = {
                        'id': str(idx + 1),
                        'author': author,
                        'content': content,
                        'date': date,
                        'depth': 0,
                        'is_reply': False,
                        'parent_id': None,
                        'like_count': 0,
                        'dislike_count': 0,
                        'is_best': False,
                        'is_author': False
                    }
                    
                    comments.append(comment)
                    
                except Exception as e:
                    logger.warning(f"ëŒ“ê¸€ {idx} ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                    continue
                    
        except Exception as e:
            logger.warning(f"ëŒ“ê¸€ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        return comments
    
    def convert_to_community_post(self, post_data: Dict[str, Any], content: List[Dict[str, Any]], comments: List[Dict[str, Any]]) -> Dict[str, Any]:
        """CommunityPost êµ¬ì¡°ë¡œ ë³€í™˜"""
        now = datetime.now()
        
        community_post = {
            'post_id': post_data['post_id'],
            'post_url': post_data['post_url'],
            'site': 'fmkorea',
            'scraped_at': now,
            'metadata': {
                'title': post_data['title'],
                'author': post_data['author'],
                'date': now.isoformat(),
                'view_count': post_data['stats']['view_count'],
                'like_count': post_data['stats']['like_count'],
                'dislike_count': post_data['stats']['dislike_count'],
                'comment_count': len(comments)
            },
            'content': content,
            'comments': comments,
            'created_at': now,
            'updated_at': now
        }
        
        return community_post
    
    async def save_post(self, post: Dict[str, Any]) -> bool:
        """ê²Œì‹œê¸€ ì €ì¥"""
        try:
            # ì¤‘ë³µ í™•ì¸
            existing = self.collection.find_one({
                'site': 'fmkorea',
                'post_id': post['post_id']
            })
            
            if existing:
                logger.debug(f"ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ê²Œì‹œê¸€: {post['post_id']}")
                return False
            
            # ì €ì¥
            result = self.collection.insert_one(post)
            
            if result.inserted_id:
                logger.debug(f"ê²Œì‹œê¸€ ì €ì¥ ì„±ê³µ: {post['post_id']}")
                return True
            else:
                logger.error(f"ê²Œì‹œê¸€ ì €ì¥ ì‹¤íŒ¨: {post['post_id']}")
                return False
                
        except Exception as e:
            logger.error(f"ê²Œì‹œê¸€ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    async def scrape_and_save(self, max_pages: int = 5, delay: float = 1.0) -> Dict[str, int]:
        """ìŠ¤í¬ë˜í•‘ ë° ì €ì¥ ì‹¤í–‰"""
        stats = {
            'total_found': 0,
            'total_scraped': 0,
            'total_saved': 0,
            'errors': 0
        }
        
        try:
            logger.info(f"ğŸš€ CommunityPost ëª¨ë¸ë¡œ ìŠ¤í¬ë˜í•‘ ì‹œì‘ (ìµœëŒ€ {max_pages}í˜ì´ì§€)")
            
            async with aiohttp.ClientSession() as session:
                for page in range(1, max_pages + 1):
                    try:
                        # ê²Œì‹œê¸€ ëª©ë¡ ìŠ¤í¬ë˜í•‘
                        posts = await self.scrape_post_list(session, page)
                        stats['total_found'] += len(posts)
                        
                        # ê° ê²Œì‹œê¸€ ìƒì„¸ ìŠ¤í¬ë˜í•‘
                        for post_data in posts:
                            try:
                                # ìƒì„¸ ë‚´ìš© ìŠ¤í¬ë˜í•‘
                                community_post = await self.scrape_post_detail(session, post_data)
                                
                                if community_post:
                                    stats['total_scraped'] += 1
                                    
                                    # ì €ì¥
                                    if await self.save_post(community_post):
                                        stats['total_saved'] += 1
                                        logger.info(f"ì €ì¥ ì™„ë£Œ: {community_post['post_id']} - {community_post['metadata']['title'][:50]}...")
                                
                                # ë”œë ˆì´
                                await asyncio.sleep(delay)
                                
                            except Exception as e:
                                logger.error(f"ê²Œì‹œê¸€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                                stats['errors'] += 1
                        
                        logger.info(f"í˜ì´ì§€ {page} ì™„ë£Œ: {len(posts)}ê°œ ë°œê²¬, {stats['total_saved']}ê°œ ì €ì¥ë¨")
                        
                        # í˜ì´ì§€ ê°„ ë”œë ˆì´
                        await asyncio.sleep(delay * 2)
                        
                    except Exception as e:
                        logger.error(f"í˜ì´ì§€ {page} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                        stats['errors'] += 1
            
            logger.info(f"ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: {stats}")
            return stats
            
        except Exception as e:
            logger.error(f"ìŠ¤í¬ë˜í•‘ ì¤‘ ì˜¤ë¥˜: {e}")
            raise
    
    def get_collection_stats(self) -> Dict[str, int]:
        """ì €ì¥ëœ ë°ì´í„° í†µê³„"""
        try:
            total_count = self.collection.count_documents({'site': 'fmkorea'})
            
            # ìµœê·¼ ì €ì¥ëœ ê²Œì‹œê¸€
            recent_posts = list(self.collection.find(
                {'site': 'fmkorea'}, 
                {'post_id': 1, 'metadata.title': 1, 'created_at': 1}
            ).sort('created_at', -1).limit(5))
            
            return {
                'total_posts': total_count,
                'recent_posts': recent_posts
            }
            
        except Exception as e:
            logger.error(f"í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {'total_posts': 0, 'recent_posts': []}
    
    def close(self):
        """ì—°ê²° ì¢…ë£Œ"""
        self.client.close()


async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    mongodb_uri = "mongodb+srv://nadle:miQXsXpzayvMQAzE@cluster0.bh7mhfi.mongodb.net/"
    
    scraper = CommunityPostScraper(mongodb_uri)
    
    try:
        logger.info("ğŸš€ CommunityPost ëª¨ë¸ ìŠ¤í¬ë˜í•‘ í”„ë¡œì„¸ìŠ¤ ì‹œì‘")
        
        # 1. í˜„ì¬ ìƒíƒœ í™•ì¸
        logger.info("1ï¸âƒ£ í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ í™•ì¸")
        current_stats = scraper.get_collection_stats()
        logger.info(f"í˜„ì¬ ì €ì¥ëœ ê²Œì‹œê¸€: {current_stats['total_posts']}ê°œ")
        
        # 2. ìŠ¤í¬ë˜í•‘ ì„¤ì • í™•ì¸
        max_pages = 3  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ 3í˜ì´ì§€ë§Œ
        delay = 1.5    # ì„œë²„ ë¶€í•˜ ë°©ì§€
        
        logger.info(f"ìŠ¤í¬ë˜í•‘ ì„¤ì •: ìµœëŒ€ {max_pages}í˜ì´ì§€, ë”œë ˆì´ {delay}ì´ˆ")
        
        user_input = input(f"\nìŠ¤í¬ë˜í•‘ì„ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ")
        if user_input.lower() != 'y':
            logger.info("ìŠ¤í¬ë˜í•‘ ì·¨ì†Œë¨")
            return
        
        # 3. ìŠ¤í¬ë˜í•‘ ì‹¤í–‰
        logger.info("2ï¸âƒ£ ìŠ¤í¬ë˜í•‘ ì‹¤í–‰")
        scraping_stats = await scraper.scrape_and_save(max_pages=max_pages, delay=delay)
        
        # 4. ê²°ê³¼ í™•ì¸
        logger.info("3ï¸âƒ£ ìŠ¤í¬ë˜í•‘ ê²°ê³¼ í™•ì¸")
        final_stats = scraper.get_collection_stats()
        
        logger.info("âœ… ìŠ¤í¬ë˜í•‘ ì™„ë£Œ!")
        logger.info(f"ìŠ¤í¬ë˜í•‘ í†µê³„: {scraping_stats}")
        logger.info(f"ìµœì¢… ì €ì¥ëœ ê²Œì‹œê¸€: {final_stats['total_posts']}ê°œ")
        
        if final_stats['recent_posts']:
            logger.info("ìµœê·¼ ì €ì¥ëœ ê²Œì‹œê¸€:")
            for post in final_stats['recent_posts']:
                logger.info(f"  - {post['post_id']}: {post['metadata']['title'][:50]}...")
        
    except Exception as e:
        logger.error(f"ìŠ¤í¬ë˜í•‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise
    finally:
        scraper.close()


if __name__ == "__main__":
    asyncio.run(main()) 