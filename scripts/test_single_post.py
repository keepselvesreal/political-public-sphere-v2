"""
ê°œë³„ ê²Œì‹œê¸€ ìƒì„¸ ìŠ¤í¬ë˜í•‘ í…ŒìŠ¤íŠ¸

ì£¼ìš” ê¸°ëŠ¥:
- íŠ¹ì • ê²Œì‹œê¸€ URLë¡œ ìƒì„¸ ìŠ¤í¬ë˜í•‘ í…ŒìŠ¤íŠ¸ (line 20-60)
- ìŠ¤í¬ë˜í•‘ ê³¼ì • ìƒì„¸ ë¡œê¹… (line 62-100)
- ë¬¸ì œì  ì§„ë‹¨ ë° í•´ê²° ë°©ì•ˆ ì œì‹œ (line 102-150)

ì‘ì„±ì: AI Assistant
ì‘ì„±ì¼: 2025-01-28
ëª©ì : ìƒì„¸ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨ ì›ì¸ ë¶„ì„
"""

import asyncio
import json
import sys
from pathlib import Path
from loguru import logger

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from scraping.scrapers.fmkorea_scraper_v2 import FMKoreaScraper
from scraping.scrapers.ruliweb_scraper_v2 import RuliwebScraper


async def test_fmkorea_single_post():
    """FMì½”ë¦¬ì•„ ê°œë³„ ê²Œì‹œê¸€ ìƒì„¸ í…ŒìŠ¤íŠ¸"""
    logger.info("ğŸ§ª FMì½”ë¦¬ì•„ ê°œë³„ ê²Œì‹œê¸€ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    # í…ŒìŠ¤íŠ¸í•  ê²Œì‹œê¸€ URL (ìµœê·¼ ìŠ¤í¬ë˜í•‘ëœ ê²ƒ ì¤‘ í•˜ë‚˜)
    test_url = "https://www.fmkorea.com/8468491390"
    
    try:
        async with FMKoreaScraper() as scraper:
            logger.info(f"ğŸ”— í…ŒìŠ¤íŠ¸ URL: {test_url}")
            
            # í˜ì´ì§€ ì´ë™
            logger.info("ğŸ“„ í˜ì´ì§€ ì´ë™ ì¤‘...")
            await scraper.navigate_to_post(test_url)
            
            # í˜ì´ì§€ ë¡œë”© í™•ì¸
            page_title = await scraper.page.title()
            logger.info(f"ğŸ“‹ í˜ì´ì§€ ì œëª©: {page_title}")
            
            # ê²Œì‹œê¸€ ìš”ì†Œ ëŒ€ê¸°
            logger.info("â³ ê²Œì‹œê¸€ ìš”ì†Œ ëŒ€ê¸° ì¤‘...")
            await scraper.wait_for_post_elements()
            
            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ í…ŒìŠ¤íŠ¸
            logger.info("ğŸ“Š ë©”íƒ€ë°ì´í„° ì¶”ì¶œ í…ŒìŠ¤íŠ¸...")
            metadata = await scraper.extract_post_metadata()
            logger.info(f"âœ… ë©”íƒ€ë°ì´í„°: {metadata}")
            
            # ë³¸ë¬¸ ì»¨í…Œì´ë„ˆ í™•ì¸
            logger.info("ğŸ“ ë³¸ë¬¸ ì»¨í…Œì´ë„ˆ í™•ì¸...")
            for selector in scraper.site_config.selectors['post_container']:
                try:
                    element = await scraper.page.query_selector(selector)
                    if element:
                        text_content = await element.inner_text()
                        logger.info(f"âœ… ì»¨í…Œì´ë„ˆ ë°œê²¬ ({selector}): {len(text_content)}ì")
                        logger.info(f"   ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {text_content[:100]}...")
                        break
                    else:
                        logger.warning(f"âŒ ì»¨í…Œì´ë„ˆ ì—†ìŒ: {selector}")
                except Exception as e:
                    logger.warning(f"ğŸ’¥ ì»¨í…Œì´ë„ˆ í™•ì¸ ì‹¤íŒ¨ ({selector}): {e}")
            
            # ë³¸ë¬¸ ì½˜í…ì¸  ì¶”ì¶œ í…ŒìŠ¤íŠ¸
            logger.info("ğŸ“„ ë³¸ë¬¸ ì½˜í…ì¸  ì¶”ì¶œ í…ŒìŠ¤íŠ¸...")
            content = await scraper.extract_content_in_order()
            logger.info(f"âœ… ë³¸ë¬¸ ì½˜í…ì¸ : {len(content)}ê°œ ìš”ì†Œ")
            
            if content:
                for i, item in enumerate(content[:3]):  # ì²˜ìŒ 3ê°œë§Œ ì¶œë ¥
                    logger.info(f"   [{i+1}] {item.get('type', 'unknown')}: {str(item.get('data', {}))[:100]}...")
            
            # ëŒ“ê¸€ ì»¨í…Œì´ë„ˆ í™•ì¸
            logger.info("ğŸ’¬ ëŒ“ê¸€ ì»¨í…Œì´ë„ˆ í™•ì¸...")
            for selector in scraper.site_config.selectors['comment_container']:
                try:
                    element = await scraper.page.query_selector(selector)
                    if element:
                        comment_elements = await element.query_selector_all('*')
                        logger.info(f"âœ… ëŒ“ê¸€ ì»¨í…Œì´ë„ˆ ë°œê²¬ ({selector}): {len(comment_elements)}ê°œ í•˜ìœ„ ìš”ì†Œ")
                        break
                    else:
                        logger.warning(f"âŒ ëŒ“ê¸€ ì»¨í…Œì´ë„ˆ ì—†ìŒ: {selector}")
                except Exception as e:
                    logger.warning(f"ğŸ’¥ ëŒ“ê¸€ ì»¨í…Œì´ë„ˆ í™•ì¸ ì‹¤íŒ¨ ({selector}): {e}")
            
            # ëŒ“ê¸€ ì¶”ì¶œ í…ŒìŠ¤íŠ¸
            logger.info("ğŸ’¬ ëŒ“ê¸€ ì¶”ì¶œ í…ŒìŠ¤íŠ¸...")
            comments = await scraper.extract_comments_data()
            logger.info(f"âœ… ëŒ“ê¸€: {len(comments)}ê°œ")
            
            if comments:
                for i, comment in enumerate(comments[:2]):  # ì²˜ìŒ 2ê°œë§Œ ì¶œë ¥
                    logger.info(f"   [{i+1}] {comment.get('author', 'Unknown')}: {comment.get('content', '')[:50]}...")
            
            # ì „ì²´ ìƒì„¸ ìŠ¤í¬ë˜í•‘ í…ŒìŠ¤íŠ¸
            logger.info("ğŸ¯ ì „ì²´ ìƒì„¸ ìŠ¤í¬ë˜í•‘ í…ŒìŠ¤íŠ¸...")
            post_detail = await scraper.scrape_post_detail(test_url)
            
            if post_detail:
                logger.info("âœ… ì „ì²´ ìƒì„¸ ìŠ¤í¬ë˜í•‘ ì„±ê³µ!")
                logger.info(f"   - ë©”íƒ€ë°ì´í„°: {len(post_detail.get('metadata', {}))}ê°œ í•„ë“œ")
                logger.info(f"   - ë³¸ë¬¸: {len(post_detail.get('content', []))}ê°œ ìš”ì†Œ")
                logger.info(f"   - ëŒ“ê¸€: {len(post_detail.get('comments', []))}ê°œ")
                return post_detail
            else:
                logger.error("âŒ ì „ì²´ ìƒì„¸ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨")
                return None
            
    except Exception as e:
        logger.error(f"ğŸ’¥ FMì½”ë¦¬ì•„ ê°œë³„ ê²Œì‹œê¸€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return None


async def test_ruliweb_single_post():
    """ë£¨ë¦¬ì›¹ ê°œë³„ ê²Œì‹œê¸€ ìƒì„¸ í…ŒìŠ¤íŠ¸"""
    logger.info("ğŸ§ª ë£¨ë¦¬ì›¹ ê°œë³„ ê²Œì‹œê¸€ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    # í…ŒìŠ¤íŠ¸í•  ê²Œì‹œê¸€ URL (ìµœê·¼ ìŠ¤í¬ë˜í•‘ëœ ê²ƒ ì¤‘ í•˜ë‚˜)
    test_url = "https://bbs.ruliweb.com/community/board/300148/read/38050322"
    
    try:
        async with RuliwebScraper() as scraper:
            logger.info(f"ğŸ”— í…ŒìŠ¤íŠ¸ URL: {test_url}")
            
            # í˜ì´ì§€ ì´ë™
            logger.info("ğŸ“„ í˜ì´ì§€ ì´ë™ ì¤‘...")
            await scraper.navigate_to_post(test_url)
            
            # í˜ì´ì§€ ë¡œë”© í™•ì¸
            page_title = await scraper.page.title()
            logger.info(f"ğŸ“‹ í˜ì´ì§€ ì œëª©: {page_title}")
            
            # ê²Œì‹œê¸€ ìš”ì†Œ ëŒ€ê¸°
            logger.info("â³ ê²Œì‹œê¸€ ìš”ì†Œ ëŒ€ê¸° ì¤‘...")
            await scraper.wait_for_post_elements()
            
            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ í…ŒìŠ¤íŠ¸
            logger.info("ğŸ“Š ë©”íƒ€ë°ì´í„° ì¶”ì¶œ í…ŒìŠ¤íŠ¸...")
            metadata = await scraper.extract_post_metadata()
            logger.info(f"âœ… ë©”íƒ€ë°ì´í„°: {metadata}")
            
            # ë³¸ë¬¸ ì»¨í…Œì´ë„ˆ í™•ì¸
            logger.info("ğŸ“ ë³¸ë¬¸ ì»¨í…Œì´ë„ˆ í™•ì¸...")
            for selector in scraper.site_config.selectors['post_container']:
                try:
                    element = await scraper.page.query_selector(selector)
                    if element:
                        text_content = await element.inner_text()
                        logger.info(f"âœ… ì»¨í…Œì´ë„ˆ ë°œê²¬ ({selector}): {len(text_content)}ì")
                        logger.info(f"   ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {text_content[:100]}...")
                        break
                    else:
                        logger.warning(f"âŒ ì»¨í…Œì´ë„ˆ ì—†ìŒ: {selector}")
                except Exception as e:
                    logger.warning(f"ğŸ’¥ ì»¨í…Œì´ë„ˆ í™•ì¸ ì‹¤íŒ¨ ({selector}): {e}")
            
            # ë³¸ë¬¸ ì½˜í…ì¸  ì¶”ì¶œ í…ŒìŠ¤íŠ¸
            logger.info("ğŸ“„ ë³¸ë¬¸ ì½˜í…ì¸  ì¶”ì¶œ í…ŒìŠ¤íŠ¸...")
            content = await scraper.extract_content_in_order()
            logger.info(f"âœ… ë³¸ë¬¸ ì½˜í…ì¸ : {len(content)}ê°œ ìš”ì†Œ")
            
            if content:
                for i, item in enumerate(content[:3]):  # ì²˜ìŒ 3ê°œë§Œ ì¶œë ¥
                    logger.info(f"   [{i+1}] {item.get('type', 'unknown')}: {str(item.get('data', {}))[:100]}...")
            
            # ëŒ“ê¸€ ì»¨í…Œì´ë„ˆ í™•ì¸
            logger.info("ğŸ’¬ ëŒ“ê¸€ ì»¨í…Œì´ë„ˆ í™•ì¸...")
            for selector in scraper.site_config.selectors['comment_container']:
                try:
                    element = await scraper.page.query_selector(selector)
                    if element:
                        comment_elements = await element.query_selector_all('*')
                        logger.info(f"âœ… ëŒ“ê¸€ ì»¨í…Œì´ë„ˆ ë°œê²¬ ({selector}): {len(comment_elements)}ê°œ í•˜ìœ„ ìš”ì†Œ")
                        break
                    else:
                        logger.warning(f"âŒ ëŒ“ê¸€ ì»¨í…Œì´ë„ˆ ì—†ìŒ: {selector}")
                except Exception as e:
                    logger.warning(f"ğŸ’¥ ëŒ“ê¸€ ì»¨í…Œì´ë„ˆ í™•ì¸ ì‹¤íŒ¨ ({selector}): {e}")
            
            # ëŒ“ê¸€ ì¶”ì¶œ í…ŒìŠ¤íŠ¸
            logger.info("ğŸ’¬ ëŒ“ê¸€ ì¶”ì¶œ í…ŒìŠ¤íŠ¸...")
            comments = await scraper.extract_comments_data()
            logger.info(f"âœ… ëŒ“ê¸€: {len(comments)}ê°œ")
            
            if comments:
                for i, comment in enumerate(comments[:2]):  # ì²˜ìŒ 2ê°œë§Œ ì¶œë ¥
                    logger.info(f"   [{i+1}] {comment.get('author', 'Unknown')}: {comment.get('content', '')[:50]}...")
            
            # ì „ì²´ ìƒì„¸ ìŠ¤í¬ë˜í•‘ í…ŒìŠ¤íŠ¸
            logger.info("ğŸ¯ ì „ì²´ ìƒì„¸ ìŠ¤í¬ë˜í•‘ í…ŒìŠ¤íŠ¸...")
            post_detail = await scraper.scrape_post_detail(test_url)
            
            if post_detail:
                logger.info("âœ… ì „ì²´ ìƒì„¸ ìŠ¤í¬ë˜í•‘ ì„±ê³µ!")
                logger.info(f"   - ë©”íƒ€ë°ì´í„°: {len(post_detail.get('metadata', {}))}ê°œ í•„ë“œ")
                logger.info(f"   - ë³¸ë¬¸: {len(post_detail.get('content', []))}ê°œ ìš”ì†Œ")
                logger.info(f"   - ëŒ“ê¸€: {len(post_detail.get('comments', []))}ê°œ")
                return post_detail
            else:
                logger.error("âŒ ì „ì²´ ìƒì„¸ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨")
                return None
            
    except Exception as e:
        logger.error(f"ğŸ’¥ ë£¨ë¦¬ì›¹ ê°œë³„ ê²Œì‹œê¸€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return None


async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("ğŸš€ ê°œë³„ ê²Œì‹œê¸€ ìƒì„¸ ìŠ¤í¬ë˜í•‘ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    results = {}
    
    # FMì½”ë¦¬ì•„ í…ŒìŠ¤íŠ¸
    logger.info("=" * 60)
    fmkorea_result = await test_fmkorea_single_post()
    results["fmkorea"] = fmkorea_result
    
    # ì‚¬ì´íŠ¸ ê°„ ê°„ê²©
    await asyncio.sleep(3)
    
    # ë£¨ë¦¬ì›¹ í…ŒìŠ¤íŠ¸
    logger.info("=" * 60)
    ruliweb_result = await test_ruliweb_single_post()
    results["ruliweb"] = ruliweb_result
    
    # ê²°ê³¼ ìš”ì•½
    logger.info("=" * 60)
    logger.info("ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½:")
    
    for site_name, result in results.items():
        if result:
            logger.info(f"âœ… {site_name}: ì„±ê³µ")
            logger.info(f"   - ë³¸ë¬¸: {len(result.get('content', []))}ê°œ ìš”ì†Œ")
            logger.info(f"   - ëŒ“ê¸€: {len(result.get('comments', []))}ê°œ")
        else:
            logger.error(f"âŒ {site_name}: ì‹¤íŒ¨")
    
    logger.info("ğŸ‰ ê°œë³„ ê²Œì‹œê¸€ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


if __name__ == "__main__":
    asyncio.run(main()) 