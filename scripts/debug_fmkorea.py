"""
FMì½”ë¦¬ì•„ HTML êµ¬ì¡° ë””ë²„ê¹… ìŠ¤í¬ë¦½íŠ¸

ëª©ì : ì‹¤ì œ HTML êµ¬ì¡°ë¥¼ ë¶„ì„í•˜ì—¬ ì˜¬ë°”ë¥¸ ì…€ë ‰í„° ì°¾ê¸°
"""

import asyncio
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from scraping.scrapers.fmkorea_scraper_v2 import FMKoreaScraper
from loguru import logger


async def debug_fmkorea_structure():
    """FMì½”ë¦¬ì•„ HTML êµ¬ì¡° ë””ë²„ê¹…"""
    logger.info("ğŸ” FMì½”ë¦¬ì•„ HTML êµ¬ì¡° ë””ë²„ê¹… ì‹œì‘")
    
    board_url = "https://www.fmkorea.com/best"
    
    try:
        async with FMKoreaScraper() as scraper:
            await scraper.page.goto(board_url, wait_until='networkidle')
            await asyncio.sleep(3)
            
            # í˜ì´ì§€ ê¸°ë³¸ ì •ë³´
            title = await scraper.page.title()
            logger.info(f"í˜ì´ì§€ ì œëª©: {title}")
            
            # í…Œì´ë¸” êµ¬ì¡° í™•ì¸
            tables = await scraper.page.query_selector_all('table')
            logger.info(f"í…Œì´ë¸” ê°œìˆ˜: {len(tables)}")
            
            for i, table in enumerate(tables):
                try:
                    table_class = await table.get_attribute('class')
                    table_id = await table.get_attribute('id')
                    logger.info(f"í…Œì´ë¸” {i+1}: class='{table_class}', id='{table_id}'")
                    
                    # í…Œì´ë¸” ë‚´ tr ê°œìˆ˜ í™•ì¸
                    rows = await table.query_selector_all('tr')
                    logger.info(f"  - í–‰ ê°œìˆ˜: {len(rows)}")
                    
                    if len(rows) > 0:
                        # ì²« ë²ˆì§¸ í–‰ì˜ êµ¬ì¡° í™•ì¸
                        first_row = rows[0]
                        cells = await first_row.query_selector_all('td, th')
                        logger.info(f"  - ì²« ë²ˆì§¸ í–‰ ì…€ ê°œìˆ˜: {len(cells)}")
                        
                        for j, cell in enumerate(cells[:5]):  # ì²˜ìŒ 5ê°œ ì…€ë§Œ
                            cell_text = await cell.inner_text()
                            cell_class = await cell.get_attribute('class')
                            logger.info(f"    ì…€ {j+1}: '{cell_text[:20]}...' (class: {cell_class})")
                
                except Exception as e:
                    logger.error(f"í…Œì´ë¸” {i+1} ë¶„ì„ ì‹¤íŒ¨: {e}")
            
            # ê²Œì‹œê¸€ ë§í¬ íŒ¨í„´ í™•ì¸
            links = await scraper.page.query_selector_all('a[href*="document_srl"], a[href*="/"]')
            logger.info(f"ë§í¬ ê°œìˆ˜: {len(links)}")
            
            for i, link in enumerate(links[:10]):  # ì²˜ìŒ 10ê°œë§Œ
                try:
                    href = await link.get_attribute('href')
                    text = await link.inner_text()
                    if text and len(text.strip()) > 5:  # ì˜ë¯¸ìˆëŠ” í…ìŠ¤íŠ¸ë§Œ
                        logger.info(f"ë§í¬ {i+1}: '{text[:30]}...' -> {href}")
                except:
                    continue
            
            # íŠ¹ì • í´ë˜ìŠ¤ë‚˜ ID ìš”ì†Œ í™•ì¸
            potential_selectors = [
                '.board_list',
                '.list_table',
                '.document_list',
                '#board_list',
                '.xe-content-list',
                'tbody tr',
                '.list-group',
                '.post-list'
            ]
            
            for selector in potential_selectors:
                try:
                    elements = await scraper.page.query_selector_all(selector)
                    if elements:
                        logger.info(f"ì…€ë ‰í„° '{selector}': {len(elements)}ê°œ ìš”ì†Œ ë°œê²¬")
                        
                        if selector == 'tbody tr' and len(elements) > 0:
                            # tbody trì˜ êµ¬ì¡° ìƒì„¸ ë¶„ì„
                            for i, row in enumerate(elements[:3]):
                                cells = await row.query_selector_all('td')
                                logger.info(f"  í–‰ {i+1}: {len(cells)}ê°œ ì…€")
                                for j, cell in enumerate(cells):
                                    cell_text = await cell.inner_text()
                                    cell_class = await cell.get_attribute('class')
                                    if cell_text.strip():
                                        logger.info(f"    ì…€ {j+1}: '{cell_text[:30]}...' (class: {cell_class})")
                except:
                    continue
            
            # í˜ì´ì§€ HTML ì €ì¥ (ë””ë²„ê¹…ìš©)
            html_content = await scraper.page.content()
            with open('debug_fmkorea.html', 'w', encoding='utf-8') as f:
                f.write(html_content)
            logger.info("HTML ë‚´ìš©ì„ debug_fmkorea.htmlì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
            
    except Exception as e:
        logger.error(f"ë””ë²„ê¹… ì‹¤íŒ¨: {e}")


if __name__ == "__main__":
    asyncio.run(debug_fmkorea_structure()) 