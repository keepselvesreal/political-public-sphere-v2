"""
MongoDB ë°ì´í„° ë””ë²„ê¹… ìŠ¤í¬ë¦½íŠ¸

ì£¼ìš” ê¸°ëŠ¥:
- MongoDB ì €ì¥ëœ ë°ì´í„° ì§ì ‘ ì¡°íšŒ (line 20-40)
- ë³¸ë¬¸ê³¼ ëŒ“ê¸€ ë°ì´í„° ìƒì„¸ ë¶„ì„ (line 41-80)
- ìŠ¤í¬ë˜í•‘ ì›ë³¸ ë°ì´í„°ì™€ ë¹„êµ (line 81-120)

ì‘ì„±ì: AI Assistant
ì‘ì„±ì¼: 2025-01-28
ëª©ì : ê²Œì‹œê¸€ ë³¸ë¬¸/ëŒ“ê¸€ ëˆ„ë½ ì›ì¸ ë¶„ì„
"""

import json
import os
import sys
from datetime import datetime
from loguru import logger
import pymongo
from pprint import pprint

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))


def analyze_mongodb_data(mongodb_uri: str):
    """MongoDB ì €ì¥ëœ ë°ì´í„° ë¶„ì„"""
    try:
        logger.info("ğŸ” MongoDB ë°ì´í„° ë¶„ì„ ì‹œì‘")
        
        # MongoDB ì—°ê²°
        client = pymongo.MongoClient(mongodb_uri)
        db = client.political_public_sphere
        collection = db.community_posts
        
        # 1. ì „ì²´ í†µê³„
        total_count = collection.count_documents({})
        fmkorea_count = collection.count_documents({"site": "fmkorea"})
        ruliweb_count = collection.count_documents({"site": "ruliweb"})
        
        logger.info(f"ğŸ“Š ì „ì²´ ê²Œì‹œê¸€: {total_count}ê°œ")
        logger.info(f"ğŸ“Š FMì½”ë¦¬ì•„: {fmkorea_count}ê°œ")
        logger.info(f"ğŸ“Š ë£¨ë¦¬ì›¹: {ruliweb_count}ê°œ")
        
        # 2. ìƒ˜í”Œ ë°ì´í„° ì¡°íšŒ (ê° ì‚¬ì´íŠ¸ë³„ 1ê°œì”©)
        logger.info("\n" + "="*50)
        logger.info("ğŸ” FMì½”ë¦¬ì•„ ìƒ˜í”Œ ë°ì´í„° ë¶„ì„")
        
        fmkorea_sample = collection.find_one({"site": "fmkorea"})
        if fmkorea_sample:
            logger.info(f"ê²Œì‹œê¸€ ID: {fmkorea_sample.get('post_id')}")
            logger.info(f"ì œëª©: {fmkorea_sample.get('metadata', {}).get('title', 'N/A')}")
            
            content = fmkorea_sample.get('content', [])
            comments = fmkorea_sample.get('comments', [])
            
            logger.info(f"ë³¸ë¬¸ ì½˜í…ì¸  ê°œìˆ˜: {len(content)}")
            logger.info(f"ëŒ“ê¸€ ê°œìˆ˜: {len(comments)}")
            
            if content:
                logger.info("ë³¸ë¬¸ ì²« ë²ˆì§¸ ìš”ì†Œ:")
                pprint(content[0])
            else:
                logger.warning("âš ï¸ ë³¸ë¬¸ ì½˜í…ì¸ ê°€ ë¹„ì–´ìˆìŒ")
            
            if comments:
                logger.info("ëŒ“ê¸€ ì²« ë²ˆì§¸ ìš”ì†Œ:")
                pprint(comments[0])
            else:
                logger.warning("âš ï¸ ëŒ“ê¸€ì´ ë¹„ì–´ìˆìŒ")
        
        logger.info("\n" + "="*50)
        logger.info("ğŸ” ë£¨ë¦¬ì›¹ ìƒ˜í”Œ ë°ì´í„° ë¶„ì„")
        
        ruliweb_sample = collection.find_one({"site": "ruliweb"})
        if ruliweb_sample:
            logger.info(f"ê²Œì‹œê¸€ ID: {ruliweb_sample.get('post_id')}")
            logger.info(f"ì œëª©: {ruliweb_sample.get('metadata', {}).get('title', 'N/A')}")
            
            content = ruliweb_sample.get('content', [])
            comments = ruliweb_sample.get('comments', [])
            
            logger.info(f"ë³¸ë¬¸ ì½˜í…ì¸  ê°œìˆ˜: {len(content)}")
            logger.info(f"ëŒ“ê¸€ ê°œìˆ˜: {len(comments)}")
            
            if content:
                logger.info("ë³¸ë¬¸ ì²« ë²ˆì§¸ ìš”ì†Œ:")
                pprint(content[0])
            else:
                logger.warning("âš ï¸ ë³¸ë¬¸ ì½˜í…ì¸ ê°€ ë¹„ì–´ìˆìŒ")
            
            if comments:
                logger.info("ëŒ“ê¸€ ì²« ë²ˆì§¸ ìš”ì†Œ:")
                pprint(comments[0])
            else:
                logger.warning("âš ï¸ ëŒ“ê¸€ì´ ë¹„ì–´ìˆìŒ")
        
        # 3. ë³¸ë¬¸/ëŒ“ê¸€ í†µê³„
        logger.info("\n" + "="*50)
        logger.info("ğŸ“Š ë³¸ë¬¸/ëŒ“ê¸€ í†µê³„ ë¶„ì„")
        
        # ë³¸ë¬¸ì´ ìˆëŠ” ê²Œì‹œê¸€ ìˆ˜
        posts_with_content = collection.count_documents({
            "content": {"$exists": True, "$not": {"$size": 0}}
        })
        
        # ëŒ“ê¸€ì´ ìˆëŠ” ê²Œì‹œê¸€ ìˆ˜
        posts_with_comments = collection.count_documents({
            "comments": {"$exists": True, "$not": {"$size": 0}}
        })
        
        logger.info(f"ë³¸ë¬¸ì´ ìˆëŠ” ê²Œì‹œê¸€: {posts_with_content}/{total_count}ê°œ")
        logger.info(f"ëŒ“ê¸€ì´ ìˆëŠ” ê²Œì‹œê¸€: {posts_with_comments}/{total_count}ê°œ")
        
        client.close()
        
    except Exception as e:
        logger.error(f"ğŸ’¥ MongoDB ë°ì´í„° ë¶„ì„ ì‹¤íŒ¨: {e}")


def analyze_scraping_results():
    """ìŠ¤í¬ë˜í•‘ ì›ë³¸ ê²°ê³¼ ë¶„ì„"""
    try:
        logger.info("\n" + "="*50)
        logger.info("ğŸ” ìŠ¤í¬ë˜í•‘ ì›ë³¸ ë°ì´í„° ë¶„ì„")
        
        results_dir = "results"
        result_files = [f for f in os.listdir(results_dir) if f.endswith('_test_results.json')]
        if not result_files:
            logger.error("ìŠ¤í¬ë˜í•‘ ê²°ê³¼ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        latest_file = sorted(result_files)[-1]
        file_path = os.path.join(results_dir, latest_file)
        
        logger.info(f"ğŸ“ ë¶„ì„ íŒŒì¼: {file_path}")
        
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # FMì½”ë¦¬ì•„ ë°ì´í„° ë¶„ì„
        if 'fmkorea' in data:
            fmkorea_data = data['fmkorea']
            board_posts = fmkorea_data.get('board_posts', [])
            detail_posts = fmkorea_data.get('detail_posts', [])
            
            logger.info(f"FMì½”ë¦¬ì•„ ê²Œì‹œíŒ ëª©ë¡: {len(board_posts)}ê°œ")
            logger.info(f"FMì½”ë¦¬ì•„ ìƒì„¸ ê²Œì‹œê¸€: {len(detail_posts)}ê°œ")
            
            if detail_posts:
                sample_detail = detail_posts[0]
                logger.info(f"FMì½”ë¦¬ì•„ ìƒì„¸ ìƒ˜í”Œ - ID: {sample_detail.get('post_id')}")
                logger.info(f"ë³¸ë¬¸ ì½˜í…ì¸ : {len(sample_detail.get('content', []))}ê°œ")
                logger.info(f"ëŒ“ê¸€: {len(sample_detail.get('comments', []))}ê°œ")
                
                if sample_detail.get('content'):
                    logger.info("ë³¸ë¬¸ ì²« ë²ˆì§¸ ìš”ì†Œ:")
                    pprint(sample_detail['content'][0])
        
        # ë£¨ë¦¬ì›¹ ë°ì´í„° ë¶„ì„
        if 'ruliweb' in data:
            ruliweb_data = data['ruliweb']
            board_posts = ruliweb_data.get('board_posts', [])
            detail_posts = ruliweb_data.get('detail_posts', [])
            
            logger.info(f"ë£¨ë¦¬ì›¹ ê²Œì‹œíŒ ëª©ë¡: {len(board_posts)}ê°œ")
            logger.info(f"ë£¨ë¦¬ì›¹ ìƒì„¸ ê²Œì‹œê¸€: {len(detail_posts)}ê°œ")
            
            if detail_posts:
                sample_detail = detail_posts[0]
                logger.info(f"ë£¨ë¦¬ì›¹ ìƒì„¸ ìƒ˜í”Œ - ID: {sample_detail.get('post_id')}")
                logger.info(f"ë³¸ë¬¸ ì½˜í…ì¸ : {len(sample_detail.get('content', []))}ê°œ")
                logger.info(f"ëŒ“ê¸€: {len(sample_detail.get('comments', []))}ê°œ")
                
                if sample_detail.get('content'):
                    logger.info("ë³¸ë¬¸ ì²« ë²ˆì§¸ ìš”ì†Œ:")
                    pprint(sample_detail['content'][0])
        
    except Exception as e:
        logger.error(f"ğŸ’¥ ìŠ¤í¬ë˜í•‘ ê²°ê³¼ ë¶„ì„ ì‹¤íŒ¨: {e}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    mongodb_uri = "mongodb+srv://nadle:miQXsXpzayvMQAzE@cluster0.bh7mhfi.mongodb.net/"
    
    logger.info("ğŸš€ ë°ì´í„° ë””ë²„ê¹… ë¶„ì„ ì‹œì‘")
    
    # 1. MongoDB ì €ì¥ëœ ë°ì´í„° ë¶„ì„
    analyze_mongodb_data(mongodb_uri)
    
    # 2. ìŠ¤í¬ë˜í•‘ ì›ë³¸ ë°ì´í„° ë¶„ì„
    analyze_scraping_results()
    
    logger.info("âœ… ë°ì´í„° ë””ë²„ê¹… ë¶„ì„ ì™„ë£Œ")


if __name__ == "__main__":
    main() 