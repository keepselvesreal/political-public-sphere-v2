#!/usr/bin/env python3
"""
ìˆ˜ì •ëœ FMKorea ìŠ¤í¬ë˜í¼ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
ë‹¨ì¼ ê²Œì‹œê¸€ë¡œ ì½˜í…ì¸ ì™€ ëŒ“ê¸€ ì¶”ì¶œì´ ì œëŒ€ë¡œ ë˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
"""

import asyncio
import sys
import os
import json
from datetime import datetime

# ìŠ¤í¬ë˜í¼ ëª¨ë“ˆ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.join(os.path.dirname(__file__), 'scraping', 'scrapers'))

from fmkorea_scraper_v3 import FMKoreaScraper

async def test_single_post():
    """ë‹¨ì¼ ê²Œì‹œê¸€ í…ŒìŠ¤íŠ¸"""
    
    # í…ŒìŠ¤íŠ¸í•  URLë“¤ (ì²¨ë¶€ëœ JSONì—ì„œ ì„ íƒ)
    test_urls = [
        "https://www.fmkorea.com/8476125978",  # ê¹€ë¬¸ìˆ˜ í„±ê±¸ì´ (ì½˜í…ì¸  ìˆìŒ)
        "https://www.fmkorea.com/8476127692",  # êµ­í˜ ì¸ì‹ (ëŒ“ê¸€ 4ê°œ)
        "https://www.fmkorea.com/8476125875"   # ê¶Œì„±ë™ (ì´ë¯¸ì§€ + ëŒ“ê¸€ 5ê°œ)
    ]
    
    scraper = FMKoreaScraper()
    
    # ìŠ¤í¬ë˜í•‘ ì„¤ì •
    config = {
        'headless': True,
        'slow_mo': 1000,
        'wait_time': 3,
        'timeout': 30000
    }
    
    for i, url in enumerate(test_urls, 1):
        print(f"\n{'='*80}")
        print(f"ğŸ§ª í…ŒìŠ¤íŠ¸ {i}/{len(test_urls)}: {url}")
        print(f"{'='*80}")
        
        try:
            result = await scraper.scrape_post(url, config)
            
            if result:
                print(f"âœ… ìŠ¤í¬ë˜í•‘ ì„±ê³µ!")
                
                # ê²°ê³¼ ìš”ì•½
                metadata = result.get('metadata', {})
                content = result.get('content', [])
                comments = result.get('comments', [])
                
                print(f"\nğŸ“‹ ë©”íƒ€ë°ì´í„°:")
                print(f"  ì œëª©: {metadata.get('title', 'N/A')}")
                print(f"  ì‘ì„±ì: {metadata.get('author', 'N/A')}")
                print(f"  ì¡°íšŒìˆ˜: {metadata.get('view_count', 0):,}")
                print(f"  ì¶”ì²œìˆ˜: {metadata.get('up_count', 0):,}")
                print(f"  ëŒ“ê¸€ìˆ˜: {metadata.get('comment_count', 0):,}")
                
                print(f"\nğŸ“„ ë³¸ë¬¸ ì½˜í…ì¸  ({len(content)}ê°œ):")
                for j, item in enumerate(content):
                    if item['type'] == 'text':
                        print(f"  {j+1}. [í…ìŠ¤íŠ¸] {item['data']['text'][:100]}...")
                    elif item['type'] == 'image':
                        print(f"  {j+1}. [ì´ë¯¸ì§€] {item['data']['src']}")
                    elif item['type'] == 'video':
                        print(f"  {j+1}. [ë¹„ë””ì˜¤] {item['data']['src']}")
                
                print(f"\nğŸ’¬ ëŒ“ê¸€ ({len(comments)}ê°œ):")
                for j, comment in enumerate(comments):
                    print(f"  {j+1}. {comment['author']}: {comment['content'][:50]}...")
                
                # JSON íŒŒì¼ë¡œ ì €ì¥
                filename = f"test_result_{i}_{datetime.now().strftime('%H%M%S')}.json"
                with open(filename, 'w', encoding='utf-8') as f:
                    json.dump(result, f, ensure_ascii=False, indent=2)
                print(f"\nğŸ’¾ ê²°ê³¼ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
                
            else:
                print(f"âŒ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
        
        # ë‹¤ìŒ í…ŒìŠ¤íŠ¸ ì „ ëŒ€ê¸°
        if i < len(test_urls):
            print(f"\nâ³ ë‹¤ìŒ í…ŒìŠ¤íŠ¸ê¹Œì§€ 3ì´ˆ ëŒ€ê¸°...")
            await asyncio.sleep(3)
    
    print(f"\n{'='*80}")
    print("ğŸ ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print(f"{'='*80}")

if __name__ == "__main__":
    asyncio.run(test_single_post()) 