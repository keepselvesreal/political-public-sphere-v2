"""
ì‹¤í—˜ìš© ìŠ¤í¬ë˜í¼ í…ŒìŠ¤íŠ¸ ëª¨ë“ˆ

ì£¼ìš” ê¸°ëŠ¥:
- test_scraper_initialization: ìŠ¤í¬ë˜í¼ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸ (line 20-40)
- test_post_scraping: ê²Œì‹œê¸€ ìŠ¤í¬ë˜í•‘ í…ŒìŠ¤íŠ¸ (line 42-80)
- test_content_extraction: ì½˜í…ì¸  ì¶”ì¶œ í…ŒìŠ¤íŠ¸ (line 82-120)
- test_comment_extraction: ëŒ“ê¸€ ì¶”ì¶œ í…ŒìŠ¤íŠ¸ (line 122-160)

ì‘ì„±ì: AI Assistant
ì‘ì„±ì¼: 2025-01-28
ëª©ì : ì‹¤í—˜ìš© ìŠ¤í¬ë˜í¼ ê¸°ëŠ¥ ê²€ì¦
"""

import asyncio
import pytest
from scraping.fmkorea_experiment_scraper import FMKoreaExperimentScraper, scrape_fmkorea_experiment


class TestFMKoreaExperimentScraper:
    """ì‹¤í—˜ìš© ìŠ¤í¬ë˜í¼ í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""
    
    @pytest.mark.asyncio
    async def test_scraper_initialization(self):
        """ìŠ¤í¬ë˜í¼ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸"""
        async with FMKoreaExperimentScraper() as scraper:
            assert scraper.base_url == 'https://www.fmkorea.com'
            assert scraper.browser is not None
            assert scraper.page is not None
            print("âœ… ìŠ¤í¬ë˜í¼ ì´ˆê¸°í™” ì„±ê³µ")
    
    @pytest.mark.asyncio
    async def test_post_scraping_basic(self):
        """ê¸°ë³¸ ê²Œì‹œê¸€ ìŠ¤í¬ë˜í•‘ í…ŒìŠ¤íŠ¸"""
        # í…ŒìŠ¤íŠ¸ìš© ê²Œì‹œê¸€ URL (ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ê²Œì‹œê¸€)
        test_url = "https://www.fmkorea.com/8465873058"
        
        result = await scrape_fmkorea_experiment(test_url)
        
        # ê¸°ë³¸ êµ¬ì¡° ê²€ì¦
        assert 'post_id' in result
        assert 'post_url' in result
        assert 'scraped_at' in result
        assert 'metadata' in result
        assert 'content' in result
        assert 'comments' in result
        
        # ë©”íƒ€ë°ì´í„° ê²€ì¦
        metadata = result['metadata']
        assert 'title' in metadata
        assert 'author' in metadata
        assert 'view_count' in metadata
        assert 'like_count' in metadata
        assert 'comment_count' in metadata
        
        print(f"âœ… ê²Œì‹œê¸€ ìŠ¤í¬ë˜í•‘ ì„±ê³µ: {metadata.get('title', 'N/A')}")
        print(f"   - ì½˜í…ì¸  ìš”ì†Œ: {len(result['content'])}ê°œ")
        print(f"   - ëŒ“ê¸€: {len(result['comments'])}ê°œ")
        
        return result
    
    @pytest.mark.asyncio
    async def test_content_structure(self):
        """ì½˜í…ì¸  êµ¬ì¡° ê²€ì¦ í…ŒìŠ¤íŠ¸"""
        test_url = "https://www.fmkorea.com/8465873058"
        result = await scrape_fmkorea_experiment(test_url)
        
        content_list = result.get('content', [])
        assert len(content_list) > 0, "ì½˜í…ì¸ ê°€ ì¶”ì¶œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"
        
        # ê° ì½˜í…ì¸  ìš”ì†Œ êµ¬ì¡° ê²€ì¦
        for i, content in enumerate(content_list):
            assert 'type' in content, f"ì½˜í…ì¸  {i}: type í•„ë“œ ëˆ„ë½"
            assert 'order' in content, f"ì½˜í…ì¸  {i}: order í•„ë“œ ëˆ„ë½"
            assert 'data' in content, f"ì½˜í…ì¸  {i}: data í•„ë“œ ëˆ„ë½"
            
            content_type = content['type']
            assert content_type in ['image', 'text', 'video'], f"ì½˜í…ì¸  {i}: ì˜ëª»ëœ íƒ€ì… {content_type}"
            
            # íƒ€ì…ë³„ í•„ìˆ˜ í•„ë“œ ê²€ì¦
            data = content['data']
            if content_type == 'image':
                assert 'src' in data, f"ì´ë¯¸ì§€ ì½˜í…ì¸  {i}: src í•„ë“œ ëˆ„ë½"
            elif content_type == 'text':
                assert 'text' in data, f"í…ìŠ¤íŠ¸ ì½˜í…ì¸  {i}: text í•„ë“œ ëˆ„ë½"
            elif content_type == 'video':
                assert 'src' in data, f"ë¹„ë””ì˜¤ ì½˜í…ì¸  {i}: src í•„ë“œ ëˆ„ë½"
        
        print(f"âœ… ì½˜í…ì¸  êµ¬ì¡° ê²€ì¦ ì™„ë£Œ: {len(content_list)}ê°œ ìš”ì†Œ")
        
        # ì¤‘ë³µ ì´ë¯¸ì§€ ê²€ì‚¬
        image_srcs = []
        for content in content_list:
            if content['type'] == 'image':
                src = content['data'].get('src')
                if src:
                    assert src not in image_srcs, f"ì¤‘ë³µ ì´ë¯¸ì§€ ë°œê²¬: {src}"
                    image_srcs.append(src)
        
        print(f"âœ… ì¤‘ë³µ ì´ë¯¸ì§€ ê²€ì‚¬ ì™„ë£Œ: {len(image_srcs)}ê°œ ê³ ìœ  ì´ë¯¸ì§€")
    
    @pytest.mark.asyncio
    async def test_comment_structure(self):
        """ëŒ“ê¸€ êµ¬ì¡° ê²€ì¦ í…ŒìŠ¤íŠ¸"""
        test_url = "https://www.fmkorea.com/8465878065"  # ëŒ“ê¸€ì´ ë§ì€ ê²Œì‹œê¸€
        result = await scrape_fmkorea_experiment(test_url)
        
        comments = result.get('comments', [])
        if len(comments) == 0:
            print("âš ï¸ ëŒ“ê¸€ì´ ì—†ëŠ” ê²Œì‹œê¸€ì…ë‹ˆë‹¤")
            return
        
        # ëŒ“ê¸€ êµ¬ì¡° ê²€ì¦
        for i, comment in enumerate(comments):
            assert 'id' in comment, f"ëŒ“ê¸€ {i}: id í•„ë“œ ëˆ„ë½"
            assert 'author' in comment, f"ëŒ“ê¸€ {i}: author í•„ë“œ ëˆ„ë½"
            assert 'content' in comment, f"ëŒ“ê¸€ {i}: content í•„ë“œ ëˆ„ë½"
            assert 'date' in comment, f"ëŒ“ê¸€ {i}: date í•„ë“œ ëˆ„ë½"
            assert 'is_reply' in comment, f"ëŒ“ê¸€ {i}: is_reply í•„ë“œ ëˆ„ë½"
            assert 'depth' in comment, f"ëŒ“ê¸€ {i}: depth í•„ë“œ ëˆ„ë½"
        
        # ëŒ€ëŒ“ê¸€ êµ¬ì¡° ê²€ì¦
        reply_comments = [c for c in comments if c.get('is_reply', False)]
        if reply_comments:
            for reply in reply_comments:
                assert reply.get('depth', 0) > 0, f"ëŒ€ëŒ“ê¸€ì˜ depthê°€ 0ì…ë‹ˆë‹¤: {reply.get('id')}"
                print(f"   ëŒ€ëŒ“ê¸€ ë°œê²¬: {reply.get('author')} (depth: {reply.get('depth')})")
        
        print(f"âœ… ëŒ“ê¸€ êµ¬ì¡° ê²€ì¦ ì™„ë£Œ: {len(comments)}ê°œ ëŒ“ê¸€, {len(reply_comments)}ê°œ ëŒ€ëŒ“ê¸€")


def run_tests():
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import sys
    import os
    
    # í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
    project_root = os.path.dirname(os.path.abspath(__file__))
    sys.path.insert(0, project_root)
    
    async def run_all_tests():
        """ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        test_instance = TestFMKoreaExperimentScraper()
        
        try:
            print("ğŸ§ª ì‹¤í—˜ìš© ìŠ¤í¬ë˜í¼ í…ŒìŠ¤íŠ¸ ì‹œì‘")
            print("=" * 50)
            
            # 1. ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸
            print("1ï¸âƒ£ ìŠ¤í¬ë˜í¼ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸")
            await test_instance.test_scraper_initialization()
            
            # 2. ê¸°ë³¸ ìŠ¤í¬ë˜í•‘ í…ŒìŠ¤íŠ¸
            print("\n2ï¸âƒ£ ê¸°ë³¸ ê²Œì‹œê¸€ ìŠ¤í¬ë˜í•‘ í…ŒìŠ¤íŠ¸")
            result = await test_instance.test_post_scraping_basic()
            
            # 3. ì½˜í…ì¸  êµ¬ì¡° í…ŒìŠ¤íŠ¸
            print("\n3ï¸âƒ£ ì½˜í…ì¸  êµ¬ì¡° ê²€ì¦ í…ŒìŠ¤íŠ¸")
            await test_instance.test_content_structure()
            
            # 4. ëŒ“ê¸€ êµ¬ì¡° í…ŒìŠ¤íŠ¸
            print("\n4ï¸âƒ£ ëŒ“ê¸€ êµ¬ì¡° ê²€ì¦ í…ŒìŠ¤íŠ¸")
            await test_instance.test_comment_structure()
            
            print("\n" + "=" * 50)
            print("âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼! ì‹¤í—˜ìš© ìŠ¤í¬ë˜í¼ê°€ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤.")
            return True
            
        except Exception as e:
            print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    # ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    return asyncio.run(run_all_tests())


if __name__ == "__main__":
    success = run_tests()
    exit(0 if success else 1) 