"""
FMì½”ë¦¬ì•„ ì‹¤í—˜ìš© ìŠ¤í¬ë˜í¼ ëª¨ë“ˆ

ì£¼ìš” ê¸°ëŠ¥:
- FMKoreaExperimentScraper: ì‹¤í—˜ìš© ìŠ¤í¬ë˜í¼ í´ë˜ìŠ¤ (line 35-80)
- scrape_post_experiment: ê°œë³„ ê²Œì‹œê¸€ ìƒì„¸ ìŠ¤í¬ë˜í•‘ (line 82-150)
- extract_content_in_order: ë³¸ë¬¸ ë‚´ìš©ì„ ìˆœì„œëŒ€ë¡œ ì¶”ì¶œ (line 152-250)
- extract_post_metadata: ê²Œì‹œê¸€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (line 252-320)
- parse_post_id_from_url: URLì—ì„œ ê²Œì‹œê¸€ ID ì¶”ì¶œ (line 322-350)

ì‘ì„±ì: AI Assistant
ì‘ì„±ì¼: 2025-05-29 17:00 KST
ëª©ì : FMì½”ë¦¬ì•„ ê²Œì‹œê¸€ ì¬í˜„ ì‹¤í—˜ìš© ì •ë°€ ìŠ¤í¬ë˜í•‘
"""

import asyncio
import re
import json
from datetime import datetime
from typing import List, Dict, Optional, Any
from urllib.parse import urljoin, urlparse
import pytz

from playwright.async_api import async_playwright, Page, Browser
from playwright_stealth import stealth_async
from loguru import logger


class FMKoreaExperimentScraper:
    """
    FMì½”ë¦¬ì•„ ì‹¤í—˜ìš© ìŠ¤í¬ë˜í¼ í´ë˜ìŠ¤
    
    ê°œë³„ ê²Œì‹œê¸€ì˜ ìƒì„¸ ì •ë³´ë¥¼ ìˆœì„œëŒ€ë¡œ ìŠ¤í¬ë˜í•‘í•˜ì—¬ 
    ì›ë³¸ ê²Œì‹œê¸€ ì¬í˜„ì´ ê°€ëŠ¥í•˜ë„ë¡ ì •ë°€í•˜ê²Œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    """
    
    def __init__(self):
        self.base_url = 'https://www.fmkorea.com'
        self.browser: Optional[Browser] = None
        self.page: Optional[Page] = None
        
        # í•œêµ­ ì‹œê°„ëŒ€ ì„¤ì •
        self.kst = pytz.timezone('Asia/Seoul')
        
        # ìŠ¤í¬ë˜í•‘ ì„¤ì • (ê¸°ì¡´ ì„±ê³µí•œ ì„¤ì • ì ìš©)
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        ]
        
        # ëŒ€ê¸° ì‹œê°„ ì„¤ì • (ê¸°ì¡´ ì„±ê³µí•œ ì„¤ì •)
        self.wait_timeout = 15000  # 15ì´ˆ
        self.navigation_timeout = 30000  # 30ì´ˆ
    
    async def __aenter__(self):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§„ì…"""
        await self.setup_browser()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ"""
        await self.close_browser()
    
    async def setup_browser(self):
        """ë¸Œë¼ìš°ì € ì„¤ì • ë° ì´ˆê¸°í™” (ê¸°ì¡´ ì„±ê³µí•œ ì„¤ì •ê³¼ ë™ì¼)"""
        try:
            playwright = await async_playwright().start()
            
            # ë¸Œë¼ìš°ì € ì˜µì…˜ ì„¤ì • (í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ í•´ì œ)
            self.browser = await playwright.chromium.launch(
                headless=False,  # í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ í•´ì œí•˜ì—¬ ë´‡ ì°¨ë‹¨ ìš°íšŒ
                args=[
                    '--no-sandbox',
                    '--disable-dev-shm-usage',
                    '--disable-blink-features=AutomationControlled',
                    '--window-size=1920,1080'
                ]
            )
            
            # ìƒˆ í˜ì´ì§€ ìƒì„±
            self.page = await self.browser.new_page()
            
            # Stealth ëª¨ë“œ ì ìš©
            await stealth_async(self.page)
            
            # ì¶”ê°€ ì„¤ì • (ê¸°ì¡´ ì„±ê³µí•œ ì„¤ì •ê³¼ ë™ì¼)
            await self.page.set_viewport_size({"width": 1920, "height": 1080})
            await self.page.set_extra_http_headers({
                'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8'
            })
            
            logger.info("âœ… ì‹¤í—˜ìš© ë¸Œë¼ìš°ì € ì„¤ì • ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ğŸ’¥ ë¸Œë¼ìš°ì € ì„¤ì • ì‹¤íŒ¨: {e}")
            raise
    
    async def close_browser(self):
        """ë¸Œë¼ìš°ì € ì¢…ë£Œ (I/O íŒŒì´í”„ ì˜¤ë¥˜ ë°©ì§€)"""
        try:
            if self.page:
                await self.page.close()
                self.page = None
                
            if self.browser:
                # ëª¨ë“  ì»¨í…ìŠ¤íŠ¸ì™€ í˜ì´ì§€ë¥¼ ì•ˆì „í•˜ê²Œ ì¢…ë£Œ
                contexts = self.browser.contexts
                for context in contexts:
                    try:
                        await context.close()
                    except:
                        pass
                
                # ë¸Œë¼ìš°ì € í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ
                await self.browser.close()
                self.browser = None
                
            # ì¶”ê°€ ëŒ€ê¸° ì‹œê°„ìœ¼ë¡œ ì™„ì „í•œ ì¢…ë£Œ ë³´ì¥
            await asyncio.sleep(0.5)
            
            logger.info("âœ… ë¸Œë¼ìš°ì € ì¢…ë£Œ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âš ï¸ ë¸Œë¼ìš°ì € ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: {e}")
            # ê°•ì œ ì¢…ë£Œ ì‹œë„
            try:
                if self.browser:
                    await self.browser.close()
            except:
                pass
    
    async def scrape_post_experiment(self, post_url: str) -> Dict:
        """
        ì‹¤í—˜ìš© ê²Œì‹œê¸€ ìƒì„¸ ìŠ¤í¬ë˜í•‘ (ê¸°ì¡´ ìŠ¤í¬ë˜í¼ì™€ ë™ì¼í•œ ë°©ì‹)
        ì¬ì‹œë„ ë¡œì§ ì œê±°í•˜ì—¬ ë¹ ë¥¸ í”¼ë“œë°± ì œê³µ
        
        Args:
            post_url: ê²Œì‹œê¸€ URL
        
        Returns:
            Dict: ì‹¤í—˜ìš© ê²Œì‹œê¸€ ë°ì´í„° (ìˆœì„œ ë³´ì¡´)
        """
        try:
            logger.info(f"ğŸ§ª ì‹¤í—˜ìš© ê²Œì‹œê¸€ ìŠ¤í¬ë˜í•‘ ì‹œì‘: {post_url}")
            
            # ë¸Œë¼ìš°ì €ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì€ ê²½ìš° ì²˜ë¦¬
            if self.page is None:
                logger.error("ğŸ’¥ ë¸Œë¼ìš°ì €ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return {}
            
            # í˜ì´ì§€ ì´ë™ (ê¸°ì¡´ ìŠ¤í¬ë˜í¼ì™€ ë™ì¼í•œ ë°©ì‹)
            await self.page.goto(post_url, wait_until="networkidle", timeout=self.navigation_timeout)
            
            # ê²Œì‹œê¸€ ë³¸ë¬¸ ì˜ì—­ì´ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°
            try:
                await self.page.wait_for_selector('article, .xe_content, .rd_body', timeout=self.wait_timeout)
            except:
                logger.warning("âš ï¸ ê²Œì‹œê¸€ ë³¸ë¬¸ ì˜ì—­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
            
            # í˜ì´ì§€ ë¡œë“œ í™•ì¸
            page_title = await self.page.title()
            logger.info(f"ğŸ“„ í˜ì´ì§€ ì œëª©: {page_title}")
            
            # ê²Œì‹œê¸€ ID ì¶”ì¶œ
            post_id = self.parse_post_id_from_url(post_url)
            
            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            metadata = await self.extract_post_metadata()
            
            # ë³¸ë¬¸ ë‚´ìš©ì„ ìˆœì„œëŒ€ë¡œ ì¶”ì¶œ
            content_list = await self.extract_content_in_order()
            
            # ëŒ“ê¸€ ë°ì´í„° ì¶”ì¶œ
            comments = await self.extract_comments_data()
            
            # ì‹¤ì œ ì¶”ì¶œëœ ëŒ“ê¸€ ìˆ˜ë¡œ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            metadata['comment_count'] = len(comments)
            
            experiment_data = {
                'post_id': post_id,
                'post_url': post_url,
                'scraped_at': datetime.now(self.kst).isoformat(),
                'metadata': metadata,
                'content': content_list,
                'comments': comments,
                'experiment_purpose': 'fmkorea_post_reproduction',
                'page_title': page_title
            }
            
            logger.info(f"âœ… ì‹¤í—˜ìš© ê²Œì‹œê¸€ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: {len(content_list)}ê°œ ì½˜í…ì¸  ìš”ì†Œ")
            return experiment_data
            
        except Exception as e:
            logger.error(f"ğŸ’¥ ì‹¤í—˜ìš© ê²Œì‹œê¸€ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
            return {
                'post_id': self.parse_post_id_from_url(post_url),
                'post_url': post_url,
                'scraped_at': datetime.now(self.kst).isoformat(),
                'error': str(e),
                'experiment_purpose': 'fmkorea_post_reproduction'
            }

    async def extract_content_in_order(self) -> List[Dict]:
        """
        ê²Œì‹œê¸€ ë³¸ë¬¸ ë‚´ìš©ì„ DOM ìˆœì„œëŒ€ë¡œ ì¶”ì¶œ (ê°œì„ ëœ ë²„ì „)
        ì¤‘ë³µ ì´ë¯¸ì§€ ë¬¸ì œ í•´ê²° ë° ì •í™•í•œ ì½˜í…ì¸  ì¶”ì¶œ
        
        Returns:
            List[Dict]: ìˆœì„œê°€ ë³´ì¡´ëœ ì½˜í…ì¸  ë¦¬ìŠ¤íŠ¸
        """
        try:
            content_list: List[Dict] = []
            order = 0
            
            # ë¸Œë¼ìš°ì €ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì€ ê²½ìš° ì²˜ë¦¬
            if self.page is None:
                logger.error("ğŸ’¥ ë¸Œë¼ìš°ì €ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return []
            
            # FMì½”ë¦¬ì•„ ê²Œì‹œê¸€ ë³¸ë¬¸ ì»¨í…Œì´ë„ˆ ì°¾ê¸° (ì‹¤ì œ HTML êµ¬ì¡° ê¸°ë°˜)
            article_selectors = [
                'article div.xe_content',  # ì‹¤ì œ êµ¬ì¡°
                'article .xe_content',
                'div.xe_content',
                'article'
            ]
            
            article_element = None
            for selector in article_selectors:
                try:
                    element = await self.page.query_selector(selector)
                    if element:
                        article_element = element
                        logger.info(f"âœ… ë³¸ë¬¸ ì»¨í…Œì´ë„ˆ ë°œê²¬: {selector}")
                        break
                except:
                    continue
            
            if not article_element:
                logger.warning("âš ï¸ ê²Œì‹œê¸€ ë³¸ë¬¸ ì»¨í…Œì´ë„ˆë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return []
            
            # ê°œì„ ëœ ì½˜í…ì¸  ì¶”ì¶œ: ì¤‘ë³µ ë°©ì§€ ë° ìˆœì„œ ë³´ì¥
            order = await self.extract_elements_improved(article_element, content_list, order)
            
            logger.info(f"âœ… ì´ {len(content_list)}ê°œ ì½˜í…ì¸  ìš”ì†Œ ìˆœì„œëŒ€ë¡œ ì¶”ì¶œ ì™„ë£Œ")
            return content_list
            
        except Exception as e:
            logger.error(f"ğŸ’¥ ìˆœì„œëŒ€ë¡œ ì½˜í…ì¸  ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []

    async def extract_elements_improved(self, parent_element, content_list: List[Dict], order_start: int) -> int:
        """
        ê°œì„ ëœ ìš”ì†Œ ì¶”ì¶œ (ì¤‘ë³µ ì´ë¯¸ì§€ ë¬¸ì œ í•´ê²°)
        
        Args:
            parent_element: ë¶€ëª¨ ìš”ì†Œ
            content_list: ì½˜í…ì¸  ë¦¬ìŠ¤íŠ¸ (ì°¸ì¡°ë¡œ ìˆ˜ì •ë¨)
            order_start: ì‹œì‘ ìˆœì„œ ë²ˆí˜¸
            
        Returns:
            int: ë‹¤ìŒ ìˆœì„œ ë²ˆí˜¸
        """
        current_order = order_start
        processed_images = set()  # ì²˜ë¦¬ëœ ì´ë¯¸ì§€ src ì¶”ì 
        
        try:
            # ì§ì ‘ ìì‹ ìš”ì†Œë“¤ì„ ìˆœì„œëŒ€ë¡œ ì²˜ë¦¬
            child_elements = await parent_element.query_selector_all('> *')
            
            for element in child_elements:
                try:
                    tag_name = await element.evaluate('el => el.tagName.toLowerCase()')
                    
                    # 1. ë§í¬ ë‚´ë¶€ ì´ë¯¸ì§€ ì²˜ë¦¬ (a.highslide > img)
                    if tag_name == 'a':
                        href = await element.get_attribute('href')
                        class_name = await element.get_attribute('class') or ''
                        
                        # highslide í´ë˜ìŠ¤ê°€ ìˆëŠ” ë§í¬ë§Œ ì²˜ë¦¬ (ì‹¤ì œ HTML êµ¬ì¡°)
                        if 'highslide' in class_name:
                            img_elements = await element.query_selector_all('img')
                            for img in img_elements:
                                src = await img.get_attribute('src')
                                data_original = await img.get_attribute('data-original')
                                image_src = data_original or src
                                
                                if image_src and image_src not in processed_images:
                                    img_data = await self.extract_image_data(element, img, current_order)
                                    if img_data:
                                        content_list.append(img_data)
                                        processed_images.add(image_src)
                                        current_order += 1
                    
                    # 2. ë…ë¦½ì ì¸ ì´ë¯¸ì§€ ì²˜ë¦¬ (ì´ë¯¸ ë§í¬ë¡œ ì²˜ë¦¬ë˜ì§€ ì•Šì€ ê²ƒë§Œ)
                    elif tag_name == 'img':
                        src = await element.get_attribute('src')
                        data_original = await element.get_attribute('data-original')
                        image_src = data_original or src
                        
                        if image_src and image_src not in processed_images:
                            img_data = await self.extract_image_data(None, element, current_order)
                            if img_data:
                                content_list.append(img_data)
                                processed_images.add(image_src)
                                current_order += 1
                    
                    # 3. ë™ì˜ìƒ ì²˜ë¦¬
                    elif tag_name == 'video':
                        video_data = await self.extract_video_data(element, current_order)
                        if video_data:
                            content_list.append(video_data)
                            current_order += 1
                    
                    # 4. í…ìŠ¤íŠ¸ ìš”ì†Œ ì²˜ë¦¬ (ë¯¸ë””ì–´ê°€ ì—†ëŠ” ìˆœìˆ˜ í…ìŠ¤íŠ¸ë§Œ)
                    elif tag_name in ['p', 'div', 'span', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'br']:
                        # br íƒœê·¸ëŠ” ê±´ë„ˆë›°ê¸°
                        if tag_name == 'br':
                            continue
                            
                        # í•˜ìœ„ì— ì´ë¯¸ì§€ë‚˜ ë¹„ë””ì˜¤ê°€ ìˆëŠ”ì§€ í™•ì¸
                        has_media = await element.query_selector('img, video, a.highslide')
                        if not has_media:
                            text_content = await element.evaluate('el => el.textContent?.trim()')
                            if text_content and len(text_content.strip()) > 0:
                                # ë¹„ë””ì˜¤ fallback í…ìŠ¤íŠ¸ ì œì™¸
                                if "Video íƒœê·¸ë¥¼ ì§€ì›í•˜ì§€ ì•ŠëŠ” ë¸Œë¼ìš°ì €ì…ë‹ˆë‹¤" not in text_content:
                                    text_data = await self.extract_text_data(element, current_order)
                                    if text_data:
                                        content_list.append(text_data)
                                        current_order += 1
                        else:
                            # ë¯¸ë””ì–´ê°€ ìˆëŠ” ìš”ì†ŒëŠ” ì¬ê·€ì ìœ¼ë¡œ ì²˜ë¦¬
                            current_order = await self.extract_elements_improved(element, content_list, current_order)
                
                except Exception as e:
                    logger.warning(f"âš ï¸ ìš”ì†Œ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    continue
            
            return current_order
            
        except Exception as e:
            logger.error(f"ğŸ’¥ ê°œì„ ëœ ìš”ì†Œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return current_order

    async def extract_image_data(self, link_element, img_element, order: int) -> Optional[Dict]:
        """ì´ë¯¸ì§€ ë°ì´í„° ì¶”ì¶œ (data-original ì†ì„± ê°•í™”)"""
        try:
            # ì´ë¯¸ì§€ ì†ŒìŠ¤ ìš°ì„ ìˆœìœ„: data-original > src
            src = await img_element.get_attribute('data-original')
            if not src:
                src = await img_element.get_attribute('src')
            
            if not src:
                return None
            
            # ì ˆëŒ€ URLë¡œ ë³€í™˜
            if src.startswith('//'):
                src = 'https:' + src
            elif src.startswith('/'):
                src = urljoin(self.base_url, src)
            
            # ì›ë³¸ srcë„ ë³´ì¡´ (fallbackìš©)
            original_src = await img_element.get_attribute('src')
            if original_src and original_src.startswith('//'):
                original_src = 'https:' + original_src
            elif original_src and original_src.startswith('/'):
                original_src = urljoin(self.base_url, original_src)
            
            img_data = {
                'type': 'image',
                'order': order,
                'data': {
                    'src': src,
                    'original_src': original_src if original_src != src else '',
                    'width': await img_element.get_attribute('width'),
                    'height': await img_element.get_attribute('height'),
                    'style': await img_element.get_attribute('style') or '',
                    'alt': await img_element.get_attribute('alt') or '',
                    'class': await img_element.get_attribute('class') or '',
                    'title': await img_element.get_attribute('title') or '',
                    'data_original': await img_element.get_attribute('data-original') or '',
                    'data_file_srl': await img_element.get_attribute('data-file-srl') or ''
                }
            }
            
            # ë§í¬ ì •ë³´ ì¶”ê°€
            if link_element:
                href = await link_element.get_attribute('href')
                if href:
                    if href.startswith('//'):
                        href = 'https:' + href
                    elif href.startswith('/'):
                        href = urljoin(self.base_url, href)
                    img_data['data']['href'] = href
                    img_data['data']['link_class'] = await link_element.get_attribute('class') or ''
                    img_data['data']['link_rel'] = await link_element.get_attribute('rel') or ''
            
            return img_data
            
        except Exception as e:
            logger.warning(f"âš ï¸ ì´ë¯¸ì§€ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None

    async def extract_video_data(self, video_element, order: int) -> Optional[Dict]:
        """ë™ì˜ìƒ ë°ì´í„° ì¶”ì¶œ (ìë™ì¬ìƒ ê°ì§€ ê°•í™”)"""
        try:
            # ë™ì˜ìƒ ì†ŒìŠ¤ ì¶”ì¶œ
            src = await video_element.get_attribute('src')
            if not src:
                # source íƒœê·¸ì—ì„œ ì¶”ì¶œ ì‹œë„
                source = await video_element.query_selector('source')
                if source:
                    src = await source.get_attribute('src')
            
            if not src:
                return None
            
            # ì ˆëŒ€ URLë¡œ ë³€í™˜
            if src.startswith('//'):
                src = 'https:' + src
            elif src.startswith('/'):
                src = urljoin(self.base_url, src)
            
            # ìë™ì¬ìƒ ì†ì„± ê°•í™” ê°ì§€
            autoplay = False
            autoplay_attr = await video_element.get_attribute('autoplay')
            if autoplay_attr is not None:
                autoplay = True
            else:
                # data-autoplay ê°™ì€ ì†ì„±ë„ í™•ì¸
                data_autoplay = await video_element.get_attribute('data-autoplay')
                if data_autoplay:
                    autoplay = True
            
            # ìŒì†Œê±° ì†ì„± ê°ì§€
            muted = False
            muted_attr = await video_element.get_attribute('muted')
            if muted_attr is not None:
                muted = True
            else:
                # ìë™ì¬ìƒì´ë©´ ìŒì†Œê±° ì²˜ë¦¬ (ë¸Œë¼ìš°ì € ì •ì±…)
                if autoplay:
                    muted = True
            
            video_data = {
                'type': 'video',
                'order': order,
                'data': {
                    'src': src,
                    'poster': await video_element.get_attribute('poster') or '',
                    'width': await video_element.get_attribute('width'),
                    'height': await video_element.get_attribute('height'),
                    'autoplay': autoplay,
                    'loop': await video_element.get_attribute('loop') is not None,
                    'muted': muted,
                    'controls': await video_element.get_attribute('controls') is not None,
                    'class': await video_element.get_attribute('class') or '',
                    'preload': await video_element.get_attribute('preload') or 'metadata'
                }
            }
            
            return video_data
            
        except Exception as e:
            logger.warning(f"âš ï¸ ë™ì˜ìƒ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None

    async def extract_text_data(self, text_element, order: int) -> Optional[Dict]:
        """í…ìŠ¤íŠ¸ ë°ì´í„° ì¶”ì¶œ"""
        try:
            text_content = await text_element.evaluate('el => el.textContent?.trim()')
            if not text_content or len(text_content.strip()) == 0:
                return None
            
            tag_name = await text_element.evaluate('el => el.tagName.toLowerCase()')
            
            text_data = {
                'type': 'text',
                'order': order,
                'data': {
                    'tag': tag_name,
                    'text': text_content,
                    'id': await text_element.get_attribute('id') or '',
                    'class': await text_element.get_attribute('class') or '',
                    'style': await text_element.get_attribute('style') or '',
                    'innerHTML': await text_element.inner_html()
                }
            }
            
            return text_data
            
        except Exception as e:
            logger.warning(f"âš ï¸ í…ìŠ¤íŠ¸ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None

    async def extract_post_metadata(self) -> Dict:
        """ê²Œì‹œê¸€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (ì‹¤ì œ HTML êµ¬ì¡° ê¸°ë°˜)"""
        try:
            metadata = {}
            
            # ë¸Œë¼ìš°ì €ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì€ ê²½ìš° ì²˜ë¦¬
            if self.page is None:
                logger.error("ğŸ’¥ ë¸Œë¼ìš°ì €ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return {}
            
            # ì œëª© ì¶”ì¶œ (ì‹¤ì œ HTML êµ¬ì¡°: h1.np_18px span.np_18px_span)
            title_selectors = [
                'h1.np_18px span.np_18px_span',  # ì‹¤ì œ êµ¬ì¡°
                'h1.np_18px',
                'h1 span',
                'h1',
                '.document_title h1',
                'article h1'
            ]
            
            for selector in title_selectors:
                try:
                    title_element = await self.page.query_selector(selector)
                    if title_element:
                        title_text = await title_element.inner_text()
                        if title_text and title_text.strip():
                            metadata['title'] = title_text.strip()
                            logger.info(f"âœ… ì œëª© ì¶”ì¶œ ì„±ê³µ: {title_text.strip()}")
                            break
                except:
                    continue
            
            # ì‘ì„±ì ì¶”ì¶œ (ì‹¤ì œ HTML êµ¬ì¡°: .member_plate)
            author_selectors = [
                '.btm_area .side .member_plate',  # ì‹¤ì œ êµ¬ì¡°
                '.member_plate',
                '.side .member_plate'
            ]
            
            for selector in author_selectors:
                try:
                    author_element = await self.page.query_selector(selector)
                    if author_element:
                        author_text = await author_element.inner_text()
                        if author_text and author_text.strip():
                            metadata['author'] = author_text.strip()
                            break
                except:
                    continue
            
            # ì‘ì„± ì‹œê°„ ì¶”ì¶œ (ì‹¤ì œ HTML êµ¬ì¡°: .top_area .date.m_no)
            date_selectors = [
                '.top_area .date.m_no',  # ì‹¤ì œ êµ¬ì¡°
                '.date.m_no',
                '.date',
                'span.date'
            ]
            
            for selector in date_selectors:
                try:
                    date_element = await self.page.query_selector(selector)
                    if date_element:
                        date_text = await date_element.inner_text()
                        if date_text and date_text.strip():
                            metadata['date'] = date_text.strip()
                            break
                except:
                    continue
            
            # í†µê³„ ì •ë³´ ì¶”ì¶œ (ì‹¤ì œ HTML êµ¬ì¡°: .btm_area .side.fr span)
            stats_selectors = [
                '.btm_area .side.fr span',  # ì‹¤ì œ êµ¬ì¡°
                '.side.fr span',
                '.btm_area span'
            ]
            
            view_count = 0
            like_count = 0
            dislike_count = 0
            comment_count = 0
            
            for selector in stats_selectors:
                try:
                    stat_elements = await self.page.query_selector_all(selector)
                    for element in stat_elements:
                        text = await element.inner_text()
                        if 'ì¡°íšŒ ìˆ˜' in text:
                            numbers = re.findall(r'\d+', text)
                            if numbers:
                                view_count = int(numbers[-1])
                        elif 'ì¶”ì²œ ìˆ˜' in text:
                            numbers = re.findall(r'\d+', text)
                            if numbers:
                                like_count = int(numbers[-1])
                        elif 'ë¹„ì¶”ì²œ' in text:
                            numbers = re.findall(r'\d+', text)
                            if numbers:
                                dislike_count = int(numbers[-1])
                        elif 'ëŒ“ê¸€' in text:
                            numbers = re.findall(r'\d+', text)
                            if numbers:
                                comment_count = int(numbers[-1])
                    break
                except:
                    continue
            
            metadata.update({
                'view_count': view_count,
                'like_count': like_count,
                'dislike_count': dislike_count,
                'comment_count': comment_count
            })
            
            logger.info(f"âœ… ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ: ì¡°íšŒìˆ˜ {view_count}, ì¶”ì²œ {like_count}, ëŒ“ê¸€ {comment_count}")
            return metadata
            
        except Exception as e:
            logger.error(f"ğŸ’¥ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {}

    async def extract_comments_data(self) -> List[Dict]:
        """ëŒ“ê¸€ ë°ì´í„° ì¶”ì¶œ (ì‹¤ì œ HTML êµ¬ì¡° ê¸°ë°˜)"""
        try:
            comments = []
            
            # ë¸Œë¼ìš°ì €ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì€ ê²½ìš° ì²˜ë¦¬
            if self.page is None:
                logger.error("ğŸ’¥ ë¸Œë¼ìš°ì €ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return []
            
            # ëŒ“ê¸€ ì»¨í…Œì´ë„ˆ ì°¾ê¸° (ì‹¤ì œ HTML êµ¬ì¡°)
            comment_selectors = [
                'ul.fdb_lst_ul li.fdb_itm',  # ì‹¤ì œ êµ¬ì¡°
                '.fdb_lst_ul .fdb_itm',
                '.fdb_itm',
                'li[id^="comment_"]'
            ]
            
            comment_elements = []
            for selector in comment_selectors:
                try:
                    elements = await self.page.query_selector_all(selector)
                    if elements:
                        comment_elements = elements
                        logger.info(f"âœ… ëŒ“ê¸€ ìš”ì†Œ ë°œê²¬: {len(elements)}ê°œ ({selector})")
                        break
                except:
                    continue
            
            if not comment_elements:
                logger.warning("âš ï¸ ëŒ“ê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return []
            
            # ê° ëŒ“ê¸€ ìš”ì†Œ ì²˜ë¦¬
            for i, comment_element in enumerate(comment_elements):
                try:
                    comment_data = await self.extract_single_comment_improved(comment_element, i)
                    if comment_data:
                        comments.append(comment_data)
                except Exception as e:
                    logger.warning(f"âš ï¸ ëŒ“ê¸€ {i+1} ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                    continue
            
            logger.info(f"âœ… ì´ {len(comments)}ê°œ ëŒ“ê¸€ ì¶”ì¶œ ì™„ë£Œ")
            return comments
            
        except Exception as e:
            logger.error(f"ğŸ’¥ ëŒ“ê¸€ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []

    async def extract_single_comment_improved(self, comment_element, index: int) -> Optional[Dict]:
        """ê°œë³„ ëŒ“ê¸€ ë°ì´í„° ì¶”ì¶œ (ì‹¤ì œ HTML êµ¬ì¡° ê¸°ë°˜)"""
        try:
            # ëŒ“ê¸€ ID ì¶”ì¶œ
            comment_id = await comment_element.get_attribute('id')
            if not comment_id:
                comment_id = f'comment_{index}'
            
            # ëŒ“ê¸€ ê³„ì¸µ êµ¬ì¡° íŒŒì•… (margin-left ìŠ¤íƒ€ì¼ ê¸°ë°˜)
            style = await comment_element.get_attribute('style') or ''
            depth = 0
            is_reply = False
            
            if 'margin-left' in style:
                # margin-left: 2%, 4%, 6% ë“±ì—ì„œ depth ê³„ì‚°
                import re
                margin_match = re.search(r'margin-left:\s*(\d+)%', style)
                if margin_match:
                    margin_percent = int(margin_match.group(1))
                    depth = margin_percent // 2  # 2% = depth 1, 4% = depth 2, 6% = depth 3
                    is_reply = depth > 0
            
            # ì‘ì„±ì ì¶”ì¶œ (ì‹¤ì œ êµ¬ì¡°: .member_plate)
            author = 'ìµëª…'
            try:
                author_element = await comment_element.query_selector('.member_plate')
                if author_element:
                    author_text = await author_element.inner_text()
                    if author_text and author_text.strip():
                        author = author_text.strip()
            except:
                pass
            
            # ëŒ“ê¸€ ë‚´ìš© ì¶”ì¶œ (ì‹¤ì œ êµ¬ì¡°: .comment-content .xe_content)
            content = ''
            try:
                content_selectors = [
                    '.comment-content .xe_content',
                    '.xe_content',
                    '.comment-content',
                    '.fdb_itm_content'
                ]
                
                for selector in content_selectors:
                    content_element = await comment_element.query_selector(selector)
                    if content_element:
                        content_text = await content_element.inner_text()
                        if content_text and content_text.strip():
                            content = content_text.strip()
                            break
            except:
                pass
            
            # ì‘ì„± ì‹œê°„ ì¶”ì¶œ (ì‹¤ì œ êµ¬ì¡°: .meta .date)
            date = ''
            try:
                date_element = await comment_element.query_selector('.meta .date, .date')
                if date_element:
                    date_text = await date_element.inner_text()
                    if date_text and date_text.strip():
                        date = date_text.strip()
            except:
                pass
            
            # ì¶”ì²œ/ë¹„ì¶”ì²œ ìˆ˜ ì¶”ì¶œ (ì‹¤ì œ êµ¬ì¡°: .vote .voted_count, .blamed_count)
            like_count = 0
            dislike_count = 0
            
            try:
                # ì¶”ì²œìˆ˜
                voted_element = await comment_element.query_selector('.voted_count')
                if voted_element:
                    voted_text = await voted_element.inner_text()
                    if voted_text and voted_text.strip().isdigit():
                        like_count = int(voted_text.strip())
                
                # ë¹„ì¶”ì²œìˆ˜
                blamed_element = await comment_element.query_selector('.blamed_count')
                if blamed_element:
                    blamed_text = await blamed_element.inner_text()
                    if blamed_text and blamed_text.strip().isdigit():
                        dislike_count = int(blamed_text.strip())
            except:
                pass
            
            # ë¶€ëª¨ ëŒ“ê¸€ ì •ë³´ ì¶”ì¶œ (ëŒ€ëŒ“ê¸€ì¸ ê²½ìš°)
            parent_comment = ''
            if is_reply:
                try:
                    parent_link = await comment_element.query_selector('.findParent')
                    if parent_link:
                        parent_text = await parent_link.inner_text()
                        if parent_text and parent_text.strip():
                            parent_comment = parent_text.strip()
                except:
                    pass
            
            # ëŒ“ê¸€ ë°ì´í„° êµ¬ì„±
            comment_data = {
                'id': comment_id,
                'author': author,
                'content': content,
                'date': date,
                'like_count': like_count,
                'dislike_count': dislike_count,
                'is_reply': is_reply,
                'depth': depth,
                'parent_id': parent_comment,
                'is_best': False,  # FMì½”ë¦¬ì•„ëŠ” ë² ìŠ¤íŠ¸ ëŒ“ê¸€ ì‹œìŠ¤í…œì´ ë‹¤ë¦„
                'is_author': False  # ì‘ì„±ì ëŒ“ê¸€ êµ¬ë¶„ ë¡œì§ ì¶”ê°€ ê°€ëŠ¥
            }
            
            # ì´ë¯¸ì§€ë‚˜ ë¹„ë””ì˜¤ê°€ ìˆëŠ” ëŒ“ê¸€ ì²˜ë¦¬
            try:
                img_element = await comment_element.query_selector('img')
                if img_element:
                    img_src = await img_element.get_attribute('src')
                    if img_src:
                        comment_data['image_url'] = img_src
                        
                        # ì´ë¯¸ì§€ ë§í¬ í™•ì¸
                        img_link = await comment_element.query_selector('a')
                        if img_link:
                            href = await img_link.get_attribute('href')
                            if href:
                                comment_data['image_link'] = href
                
                video_element = await comment_element.query_selector('video')
                if video_element:
                    video_src = await video_element.get_attribute('src')
                    if video_src:
                        comment_data['video_url'] = video_src
                        comment_data['video_autoplay'] = await video_element.get_attribute('autoplay') is not None
                        comment_data['video_loop'] = await video_element.get_attribute('loop') is not None
                        comment_data['video_muted'] = await video_element.get_attribute('muted') is not None
            except:
                pass
            
            return comment_data
            
        except Exception as e:
            logger.warning(f"âš ï¸ ê°œë³„ ëŒ“ê¸€ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None

    def parse_post_id_from_url(self, url: str) -> str:
        """URLì—ì„œ ê²Œì‹œê¸€ ID ì¶”ì¶œ"""
        try:
            # FMì½”ë¦¬ì•„ URL íŒ¨í„´ë“¤
            # https://www.fmkorea.com/8449897319
            # https://www.fmkorea.com/index.php?mid=politics&document_srl=8450144891
            
            if '/index.php' in url:
                # URL íŒŒë¼ë¯¸í„°ì—ì„œ document_srl ì¶”ì¶œ
                from urllib.parse import parse_qs, urlparse
                parsed = urlparse(url)
                params = parse_qs(parsed.query)
                if 'document_srl' in params:
                    return params['document_srl'][0]
            else:
                # ì§ì ‘ URLì—ì„œ ìˆ«ì ì¶”ì¶œ
                match = re.search(r'/(\d+)/?$', url)
                if match:
                    return match.group(1)
            
            # ìµœí›„ ìˆ˜ë‹¨: URLì—ì„œ ê°€ì¥ ê¸´ ìˆ«ì ì‹œí€€ìŠ¤ ì¶”ì¶œ
            numbers = re.findall(r'\d+', url)
            if numbers:
                return max(numbers, key=len)
            
            return 'unknown'
            
        except Exception as e:
            logger.error(f"ğŸ’¥ ê²Œì‹œê¸€ ID ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return 'unknown'


# í¸ì˜ í•¨ìˆ˜
async def scrape_fmkorea_experiment(post_url: str) -> Dict:
    """
    FMì½”ë¦¬ì•„ ì‹¤í—˜ìš© ìŠ¤í¬ë˜í•‘ í¸ì˜ í•¨ìˆ˜
    
    Args:
        post_url: ê²Œì‹œê¸€ URL
    
    Returns:
        Dict: ì‹¤í—˜ìš© ìŠ¤í¬ë˜í•‘ ê²°ê³¼
    """
    async with FMKoreaExperimentScraper() as scraper:
        return await scraper.scrape_post_experiment(post_url)


async def scrape_multiple_posts_experiment(post_urls: List[str]) -> List[Dict]:
    """
    ì—¬ëŸ¬ ê²Œì‹œê¸€ ì‹¤í—˜ìš© ìŠ¤í¬ë˜í•‘ í¸ì˜ í•¨ìˆ˜
    
    Args:
        post_urls: ê²Œì‹œê¸€ URL ë¦¬ìŠ¤íŠ¸
    
    Returns:
        List[Dict]: ì‹¤í—˜ìš© ìŠ¤í¬ë˜í•‘ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    results = []
    
    async with FMKoreaExperimentScraper() as scraper:
        for url in post_urls:
            try:
                result = await scraper.scrape_post_experiment(url)
                results.append(result)
                
                # ìš”ì²­ ê°„ ëŒ€ê¸° (ì„œë²„ ë¶€í•˜ ë°©ì§€)
                await asyncio.sleep(2)
                
            except Exception as e:
                logger.error(f"ğŸ’¥ {url} ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
                results.append({
                    'post_url': url,
                    'error': str(e),
                    'scraped_at': datetime.now().isoformat()
                })
    
    return results 