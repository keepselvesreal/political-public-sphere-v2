"""
FMì½”ë¦¬ì•„ ì‹¤í—˜ìš© ìŠ¤í¬ë˜í¼ ëª¨ë“ˆ

ì£¼ìš” ê¸°ëŠ¥:
- FMKoreaExperimentScraper: ì‹¤í—˜ìš© ìŠ¤í¬ë˜í¼ í´ë˜ìŠ¤ (line 35-80)
- scrape_post_experiment: ê°œë³„ ê²Œì‹œê¸€ ìƒì„¸ ìŠ¤í¬ë˜í•‘ (line 82-150)
- extract_content_in_order: ë³¸ë¬¸ ë‚´ìš©ì„ ìˆœì„œëŒ€ë¡œ ì¶”ì¶œ (line 152-250)
- extract_post_metadata: ê²Œì‹œê¸€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (line 252-320)
- parse_post_id_from_url: URLì—ì„œ ê²Œì‹œê¸€ ID ì¶”ì¶œ (line 322-350)

ì‘ì„±ì: AI Assistant
ì‘ì„±ì¼: 2025-05-29 17:00 KST
ëª©ì : FMì½”ë¦¬ì•„ ê²Œì‹œê¸€ ì¬í˜„ ì‹¤í—˜ìš© ì •ë°€ ìŠ¤í¬ë˜í•‘
"""

import asyncio
import re
import json
from datetime import datetime
from typing import List, Dict, Optional, Any
from urllib.parse import urljoin, urlparse
import pytz

from playwright.async_api import async_playwright, Page, Browser
from playwright_stealth import stealth_async
from loguru import logger


class FMKoreaExperimentScraper:
    """
    FMì½”ë¦¬ì•„ ì‹¤í—˜ìš© ìŠ¤í¬ë˜í¼ í´ë˜ìŠ¤
    
    ê°œë³„ ê²Œì‹œê¸€ì˜ ìƒì„¸ ì •ë³´ë¥¼ ìˆœì„œëŒ€ë¡œ ìŠ¤í¬ë˜í•‘í•˜ì—¬ 
    ì›ë³¸ ê²Œì‹œê¸€ ì¬í˜„ì´ ê°€ëŠ¥í•˜ë„ë¡ ì •ë°€í•˜ê²Œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    """
    
    def __init__(self):
        self.base_url = 'https://www.fmkorea.com'
        self.browser: Optional[Browser] = None
        self.page: Optional[Page] = None
        
        # í•œêµ­ ì‹œê°„ëŒ€ ì„¤ì •
        self.kst = pytz.timezone('Asia/Seoul')
        
        # ìŠ¤í¬ë˜í•‘ ì„¤ì • (ê¸°ì¡´ ì„±ê³µí•œ ì„¤ì • ì ìš©)
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        ]
        
        # ëŒ€ê¸° ì‹œê°„ ì„¤ì • (ê¸°ì¡´ ì„±ê³µí•œ ì„¤ì •)
        self.wait_timeout = 15000  # 15ì´ˆ
        self.navigation_timeout = 30000  # 30ì´ˆ
    
    async def __aenter__(self):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§„ì…"""
        await self.setup_browser()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ"""
        await self.close_browser()
    
    async def setup_browser(self):
        """ë¸Œë¼ìš°ì € ì„¤ì • ë° ì´ˆê¸°í™” (ê¸°ì¡´ ì„±ê³µí•œ ì„¤ì •ê³¼ ë™ì¼)"""
        try:
            playwright = await async_playwright().start()
            
            # ë¸Œë¼ìš°ì € ì˜µì…˜ ì„¤ì • (í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ í•´ì œ)
            self.browser = await playwright.chromium.launch(
                headless=False,  # í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ í•´ì œí•˜ì—¬ ë´‡ ì°¨ë‹¨ ìš°íšŒ
                args=[
                    '--no-sandbox',
                    '--disable-dev-shm-usage',
                    '--disable-blink-features=AutomationControlled',
                    '--window-size=1920,1080'
                ]
            )
            
            # ìƒˆ í˜ì´ì§€ ìƒì„±
            self.page = await self.browser.new_page()
            
            # Stealth ëª¨ë“œ ì ìš©
            await stealth_async(self.page)
            
            # ì¶”ê°€ ì„¤ì • (ê¸°ì¡´ ì„±ê³µí•œ ì„¤ì •ê³¼ ë™ì¼)
            await self.page.set_viewport_size({"width": 1920, "height": 1080})
            await self.page.set_extra_http_headers({
                'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8'
            })
            
            logger.info("âœ… ì‹¤í—˜ìš© ë¸Œë¼ìš°ì € ì„¤ì • ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ğŸ’¥ ë¸Œë¼ìš°ì € ì„¤ì • ì‹¤íŒ¨: {e}")
            raise
    
    async def close_browser(self):
        """ë¸Œë¼ìš°ì € ì¢…ë£Œ (I/O íŒŒì´í”„ ì˜¤ë¥˜ ë°©ì§€)"""
        try:
            if self.page:
                await self.page.close()
                self.page = None
                
            if self.browser:
                # ëª¨ë“  ì»¨í…ìŠ¤íŠ¸ì™€ í˜ì´ì§€ë¥¼ ì•ˆì „í•˜ê²Œ ì¢…ë£Œ
                contexts = self.browser.contexts
                for context in contexts:
                    try:
                        await context.close()
                    except:
                        pass
                
                # ë¸Œë¼ìš°ì € í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ
                await self.browser.close()
                self.browser = None
                
            # ì¶”ê°€ ëŒ€ê¸° ì‹œê°„ìœ¼ë¡œ ì™„ì „í•œ ì¢…ë£Œ ë³´ì¥
            await asyncio.sleep(0.5)
            
            logger.info("âœ… ë¸Œë¼ìš°ì € ì¢…ë£Œ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âš ï¸ ë¸Œë¼ìš°ì € ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: {e}")
            # ê°•ì œ ì¢…ë£Œ ì‹œë„
            try:
                if self.browser:
                    await self.browser.close()
            except:
                pass
    
    async def scrape_post_experiment(self, post_url: str) -> Dict:
        """
        ì‹¤í—˜ìš© ê²Œì‹œê¸€ ìƒì„¸ ìŠ¤í¬ë˜í•‘ (ê¸°ì¡´ ìŠ¤í¬ë˜í¼ì™€ ë™ì¼í•œ ë°©ì‹)
        ì¬ì‹œë„ ë¡œì§ ì œê±°í•˜ì—¬ ë¹ ë¥¸ í”¼ë“œë°± ì œê³µ
        
        Args:
            post_url: ê²Œì‹œê¸€ URL
        
        Returns:
            Dict: ì‹¤í—˜ìš© ê²Œì‹œê¸€ ë°ì´í„° (ìˆœì„œ ë³´ì¡´)
        """
        try:
            logger.info(f"ğŸ§ª ì‹¤í—˜ìš© ê²Œì‹œê¸€ ìŠ¤í¬ë˜í•‘ ì‹œì‘: {post_url}")
            
            # ë¸Œë¼ìš°ì €ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì€ ê²½ìš° ì²˜ë¦¬
            if self.page is None:
                logger.error("ğŸ’¥ ë¸Œë¼ìš°ì €ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return {}
            
            # í˜ì´ì§€ ì´ë™ (ê¸°ì¡´ ìŠ¤í¬ë˜í¼ì™€ ë™ì¼í•œ ë°©ì‹)
            await self.page.goto(post_url, wait_until="networkidle", timeout=self.navigation_timeout)
            
            # ê²Œì‹œê¸€ ë³¸ë¬¸ ì˜ì—­ì´ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°
            try:
                await self.page.wait_for_selector('article, .xe_content, .rd_body', timeout=self.wait_timeout)
            except:
                logger.warning("âš ï¸ ê²Œì‹œê¸€ ë³¸ë¬¸ ì˜ì—­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
            
            # í˜ì´ì§€ ë¡œë“œ í™•ì¸
            page_title = await self.page.title()
            logger.info(f"ğŸ“„ í˜ì´ì§€ ì œëª©: {page_title}")
            
            # ê²Œì‹œê¸€ ID ì¶”ì¶œ
            post_id = self.parse_post_id_from_url(post_url)
            
            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            metadata = await self.extract_post_metadata()
            
            # ë³¸ë¬¸ ë‚´ìš©ì„ ìˆœì„œëŒ€ë¡œ ì¶”ì¶œ
            content_list = await self.extract_content_in_order()
            
            # ëŒ“ê¸€ ë°ì´í„° ì¶”ì¶œ
            comments = await self.extract_comments_data()
            
            # ì‹¤ì œ ì¶”ì¶œëœ ëŒ“ê¸€ ìˆ˜ë¡œ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            metadata['comment_count'] = len(comments)
            
            experiment_data = {
                'post_id': post_id,
                'post_url': post_url,
                'scraped_at': datetime.now(self.kst).isoformat(),
                'metadata': metadata,
                'content': content_list,
                'comments': comments,
                'experiment_purpose': 'fmkorea_post_reproduction',
                'page_title': page_title
            }
            
            logger.info(f"âœ… ì‹¤í—˜ìš© ê²Œì‹œê¸€ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: {len(content_list)}ê°œ ì½˜í…ì¸  ìš”ì†Œ")
            return experiment_data
            
        except Exception as e:
            logger.error(f"ğŸ’¥ ì‹¤í—˜ìš© ê²Œì‹œê¸€ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
            return {
                'post_id': self.parse_post_id_from_url(post_url),
                'post_url': post_url,
                'scraped_at': datetime.now(self.kst).isoformat(),
                'error': str(e),
                'experiment_purpose': 'fmkorea_post_reproduction'
            }

    async def extract_content_in_order(self) -> List[Dict]:
        """
        ê²Œì‹œê¸€ ë³¸ë¬¸ ë‚´ìš©ì„ DOM ìˆœì„œëŒ€ë¡œ ì¶”ì¶œ
        ì´ë¯¸ì§€, í…ìŠ¤íŠ¸, ë™ì˜ìƒì„ ë“±ì¥ ìˆœì„œëŒ€ë¡œ ìˆ˜ì§‘
        
        Returns:
            List[Dict]: ìˆœì„œê°€ ë³´ì¡´ëœ ì½˜í…ì¸  ë¦¬ìŠ¤íŠ¸
        """
        try:
            content_list: List[Dict] = []
            order = 1
            
            # ë¸Œë¼ìš°ì €ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì€ ê²½ìš° ì²˜ë¦¬
            if self.page is None:
                logger.error("ğŸ’¥ ë¸Œë¼ìš°ì €ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return []
            
            # FMì½”ë¦¬ì•„ ê²Œì‹œê¸€ ë³¸ë¬¸ ì»¨í…Œì´ë„ˆ ì°¾ê¸°
            article_selectors = [
                'article .xe_content',
                'article div[class*="document_"]',
                'div.rd_body article',
                'article'
            ]
            
            article_element = None
            for selector in article_selectors:
                try:
                    element = await self.page.query_selector(selector)
                    if element:
                        article_element = element
                        logger.info(f"âœ… ë³¸ë¬¸ ì»¨í…Œì´ë„ˆ ë°œê²¬: {selector}")
                        break
                except:
                    continue
            
            if not article_element:
                logger.warning("âš ï¸ ê²Œì‹œê¸€ ë³¸ë¬¸ ì»¨í…Œì´ë„ˆë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return []
            
            # ê°œì„ ëœ ì½˜í…ì¸  ì¶”ì¶œ: ëª¨ë“  í•˜ìœ„ ìš”ì†Œë¥¼ ìˆœíšŒí•˜ì—¬ ëˆ„ë½ ë°©ì§€
            order = await self.extract_elements_recursively(article_element, content_list, order)
            
            logger.info(f"âœ… ì´ {len(content_list)}ê°œ ì½˜í…ì¸  ìš”ì†Œ ìˆœì„œëŒ€ë¡œ ì¶”ì¶œ ì™„ë£Œ")
            return content_list
            
        except Exception as e:
            logger.error(f"ğŸ’¥ ìˆœì„œëŒ€ë¡œ ì½˜í…ì¸  ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []

    async def extract_elements_recursively(self, parent_element, content_list: List[Dict], order_start: int) -> int:
        """
        ìš”ì†Œë¥¼ ì¬ê·€ì ìœ¼ë¡œ ìˆœíšŒí•˜ì—¬ ì½˜í…ì¸  ì¶”ì¶œ (ê°œì„ ëœ ë°©ì‹)
        
        Args:
            parent_element: ë¶€ëª¨ ìš”ì†Œ
            content_list: ì½˜í…ì¸  ë¦¬ìŠ¤íŠ¸ (ì°¸ì¡°ë¡œ ìˆ˜ì •ë¨)
            order_start: ì‹œì‘ ìˆœì„œ ë²ˆí˜¸
            
        Returns:
            int: ë‹¤ìŒ ìˆœì„œ ë²ˆí˜¸
        """
        current_order = order_start
        
        try:
            # ëª¨ë“  ìì‹ ìš”ì†Œ ìˆœíšŒ
            child_elements = await parent_element.query_selector_all('*')
            processed_elements = set()  # ì¤‘ë³µ ì²˜ë¦¬ ë°©ì§€
            
            for element in child_elements:
                try:
                    # ì´ë¯¸ ì²˜ë¦¬ëœ ìš”ì†ŒëŠ” ê±´ë„ˆë›°ê¸°
                    element_handle = await element.evaluate('el => el')
                    if id(element_handle) in processed_elements:
                        continue
                    processed_elements.add(id(element_handle))
                    
                    tag_name = await element.evaluate('el => el.tagName.toLowerCase()')
                    
                    # ì´ë¯¸ì§€ ì²˜ë¦¬ (data-original ìš°ì„  í™•ì¸)
                    if tag_name == 'img':
                        # data-originalì´ë‚˜ src í™•ì¸
                        data_original = await element.get_attribute('data-original')
                        src = await element.get_attribute('src')
                        
                        if data_original or src:
                            # ë¶€ëª¨ ë§í¬ ìš”ì†Œ í™•ì¸
                            parent_a = await element.evaluate('el => el.closest("a")')
                            img_data = await self.extract_image_data(parent_a, element, current_order)
                            if img_data:
                                content_list.append(img_data)
                                current_order += 1
                    
                    # ë§í¬ ë‚´ë¶€ ì´ë¯¸ì§€ ì²˜ë¦¬ (a > img)
                    elif tag_name == 'a':
                        img_elements = await element.query_selector_all('img')
                        for img in img_elements:
                            # ì´ë¯¸ ì²˜ë¦¬ë˜ì§€ ì•Šì€ ì´ë¯¸ì§€ë§Œ ì²˜ë¦¬
                            img_handle = await img.evaluate('el => el')
                            if id(img_handle) not in processed_elements:
                                data_original = await img.get_attribute('data-original')
                                src = await img.get_attribute('src')
                                
                                if data_original or src:
                                    img_data = await self.extract_image_data(element, img, current_order)
                                    if img_data:
                                        content_list.append(img_data)
                                        current_order += 1
                                        processed_elements.add(id(img_handle))
                    
                    # ë™ì˜ìƒ ì²˜ë¦¬
                    elif tag_name == 'video':
                        video_data = await self.extract_video_data(element, current_order)
                        if video_data:
                            content_list.append(video_data)
                            current_order += 1
                    
                    # í…ìŠ¤íŠ¸ ìš”ì†Œ ì²˜ë¦¬ (ìˆœìˆ˜ í…ìŠ¤íŠ¸ë§Œ ìˆëŠ” ìš”ì†Œ)
                    elif tag_name in ['p', 'div', 'span', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
                        # í•˜ìœ„ì— imgë‚˜ videoê°€ ì—†ëŠ” ê²½ìš°ì—ë§Œ í…ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬
                        has_media = await element.query_selector('img, video')
                        if not has_media:
                            text_content = await element.evaluate('el => el.textContent?.trim()')
                            if text_content and len(text_content.strip()) > 0:
                                text_data = await self.extract_text_data(element, current_order)
                                if text_data:
                                    content_list.append(text_data)
                                    current_order += 1
                
                except Exception as e:
                    logger.warning(f"âš ï¸ ìš”ì†Œ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    continue
            
            return current_order
            
        except Exception as e:
            logger.error(f"ğŸ’¥ ì¬ê·€ì  ìš”ì†Œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return current_order

    async def extract_image_data(self, link_element, img_element, order: int) -> Optional[Dict]:
        """ì´ë¯¸ì§€ ë°ì´í„° ì¶”ì¶œ (data-original ì†ì„± ê°•í™”)"""
        try:
            # ì´ë¯¸ì§€ ì†ŒìŠ¤ ìš°ì„ ìˆœìœ„: data-original > src
            src = await img_element.get_attribute('data-original')
            if not src:
                src = await img_element.get_attribute('src')
            
            if not src:
                return None
            
            # ì ˆëŒ€ URLë¡œ ë³€í™˜
            if src.startswith('//'):
                src = 'https:' + src
            elif src.startswith('/'):
                src = urljoin(self.base_url, src)
            
            # ì›ë³¸ srcë„ ë³´ì¡´ (fallbackìš©)
            original_src = await img_element.get_attribute('src')
            if original_src and original_src.startswith('//'):
                original_src = 'https:' + original_src
            elif original_src and original_src.startswith('/'):
                original_src = urljoin(self.base_url, original_src)
            
            img_data = {
                'type': 'image',
                'order': order,
                'data': {
                    'src': src,
                    'original_src': original_src if original_src != src else '',
                    'width': await img_element.get_attribute('width'),
                    'height': await img_element.get_attribute('height'),
                    'style': await img_element.get_attribute('style') or '',
                    'alt': await img_element.get_attribute('alt') or '',
                    'class': await img_element.get_attribute('class') or '',
                    'title': await img_element.get_attribute('title') or '',
                    'data_original': await img_element.get_attribute('data-original') or '',
                    'data_file_srl': await img_element.get_attribute('data-file-srl') or ''
                }
            }
            
            # ë§í¬ ì •ë³´ ì¶”ê°€
            if link_element:
                href = await link_element.get_attribute('href')
                if href:
                    if href.startswith('//'):
                        href = 'https:' + href
                    elif href.startswith('/'):
                        href = urljoin(self.base_url, href)
                    img_data['data']['href'] = href
                    img_data['data']['link_class'] = await link_element.get_attribute('class') or ''
                    img_data['data']['link_rel'] = await link_element.get_attribute('rel') or ''
            
            return img_data
            
        except Exception as e:
            logger.warning(f"âš ï¸ ì´ë¯¸ì§€ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None

    async def extract_video_data(self, video_element, order: int) -> Optional[Dict]:
        """ë™ì˜ìƒ ë°ì´í„° ì¶”ì¶œ (ìë™ì¬ìƒ ê°ì§€ ê°•í™”)"""
        try:
            # ë™ì˜ìƒ ì†ŒìŠ¤ ì¶”ì¶œ
            src = await video_element.get_attribute('src')
            if not src:
                # source íƒœê·¸ì—ì„œ ì¶”ì¶œ ì‹œë„
                source = await video_element.query_selector('source')
                if source:
                    src = await source.get_attribute('src')
            
            if not src:
                return None
            
            # ì ˆëŒ€ URLë¡œ ë³€í™˜
            if src.startswith('//'):
                src = 'https:' + src
            elif src.startswith('/'):
                src = urljoin(self.base_url, src)
            
            # ìë™ì¬ìƒ ì†ì„± ê°•í™” ê°ì§€
            autoplay = False
            autoplay_attr = await video_element.get_attribute('autoplay')
            if autoplay_attr is not None:
                autoplay = True
            else:
                # data-autoplay ê°™ì€ ì†ì„±ë„ í™•ì¸
                data_autoplay = await video_element.get_attribute('data-autoplay')
                if data_autoplay:
                    autoplay = True
            
            # ìŒì†Œê±° ì†ì„± ê°ì§€
            muted = False
            muted_attr = await video_element.get_attribute('muted')
            if muted_attr is not None:
                muted = True
            else:
                # ìë™ì¬ìƒì´ë©´ ìŒì†Œê±° ì²˜ë¦¬ (ë¸Œë¼ìš°ì € ì •ì±…)
                if autoplay:
                    muted = True
            
            video_data = {
                'type': 'video',
                'order': order,
                'data': {
                    'src': src,
                    'poster': await video_element.get_attribute('poster') or '',
                    'width': await video_element.get_attribute('width'),
                    'height': await video_element.get_attribute('height'),
                    'autoplay': autoplay,
                    'loop': await video_element.get_attribute('loop') is not None,
                    'muted': muted,
                    'controls': await video_element.get_attribute('controls') is not None,
                    'class': await video_element.get_attribute('class') or '',
                    'preload': await video_element.get_attribute('preload') or 'metadata'
                }
            }
            
            return video_data
            
        except Exception as e:
            logger.warning(f"âš ï¸ ë™ì˜ìƒ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None

    async def extract_text_data(self, text_element, order: int) -> Optional[Dict]:
        """í…ìŠ¤íŠ¸ ë°ì´í„° ì¶”ì¶œ"""
        try:
            text_content = await text_element.evaluate('el => el.textContent?.trim()')
            if not text_content or len(text_content.strip()) == 0:
                return None
            
            tag_name = await text_element.evaluate('el => el.tagName.toLowerCase()')
            
            text_data = {
                'type': 'text',
                'order': order,
                'data': {
                    'tag': tag_name,
                    'text': text_content,
                    'id': await text_element.get_attribute('id') or '',
                    'class': await text_element.get_attribute('class') or '',
                    'style': await text_element.get_attribute('style') or '',
                    'innerHTML': await text_element.inner_html()
                }
            }
            
            return text_data
            
        except Exception as e:
            logger.warning(f"âš ï¸ í…ìŠ¤íŠ¸ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None

    async def extract_post_metadata(self) -> Dict:
        """ê²Œì‹œê¸€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (ì›ë³¸ HTML êµ¬ì¡° ê¸°ë°˜)"""
        try:
            metadata = {}
            
            # ë¸Œë¼ìš°ì €ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì€ ê²½ìš° ì²˜ë¦¬
            if self.page is None:
                logger.error("ğŸ’¥ ë¸Œë¼ìš°ì €ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return {}
            
            # ì œëª© ì¶”ì¶œ (ì›ë³¸ êµ¬ì¡°: h1.np_18px span.np_18px_span)
            title_selectors = [
                'h1.np_18px span.np_18px_span',
                'h1.np_18px',
                'h1.title',
                'h1',
                '.document_title h1',
                '.rd_hd h1',
                'div.rd_hd h1',
                'article h1'
            ]
            
            for selector in title_selectors:
                try:
                    title_element = await self.page.query_selector(selector)
                    if title_element:
                        title_text = await title_element.inner_text()
                        if title_text and title_text.strip():
                            metadata['title'] = title_text.strip()
                            break
                except:
                    continue
            
            # ì‘ì„±ì ì¶”ì¶œ (ì›ë³¸ êµ¬ì¡°: .member_plate)
            author_selectors = [
                '.btm_area .side .member_plate',
                '.member_plate',
                '.meta .member_plate',
                '.document_info .member_plate',
                '.rd_hd .member_plate',
                'td.author .member_plate',
                '.author .member_plate'
            ]
            
            for selector in author_selectors:
                try:
                    author_element = await self.page.query_selector(selector)
                    if author_element:
                        author_text = await author_element.inner_text()
                        if author_text and author_text.strip():
                            metadata['author'] = author_text.strip()
                            break
                except:
                    continue
            
            # ì‘ì„± ì‹œê°„ ì¶”ì¶œ (ì›ë³¸ êµ¬ì¡°: .top_area .date.m_no)
            date_selectors = [
                '.top_area .date.m_no',
                '.rd_hd .date.m_no',
                '.meta .date',
                '.document_info .date',
                '.rd_hd .date',
                'td.time',
                '.time',
                'span.date'
            ]
            
            for selector in date_selectors:
                try:
                    date_element = await self.page.query_selector(selector)
                    if date_element:
                        date_text = await date_element.inner_text()
                        if date_text and date_text.strip():
                            metadata['date'] = date_text.strip()
                            break
                except:
                    continue
            
            # í†µê³„ ì •ë³´ ì¶”ì¶œ (ì›ë³¸ êµ¬ì¡°: .btm_area .side.fr span)
            # ì¡°íšŒìˆ˜ ì¶”ì¶œ
            view_count = 0
            view_selectors = [
                '.btm_area .side.fr span:has-text("ì¡°íšŒ")',
                '.side.fr span:has-text("ì¡°íšŒ")',
                'td.m_no:has-text("ì¡°íšŒ")',
                '.meta .m_no:has-text("ì¡°íšŒ")',
                '.document_info .view_count',
                'span:has-text("ì¡°íšŒ")',
                '.view_count'
            ]
            
            for selector in view_selectors:
                try:
                    view_element = await self.page.query_selector(selector)
                    if view_element:
                        view_text = await view_element.inner_text()
                        # "ì¡°íšŒ ìˆ˜ 44035" ë˜ëŠ” "ì¡°íšŒ 44035" í˜•íƒœì—ì„œ ìˆ«ìë§Œ ì¶”ì¶œ
                        import re
                        numbers = re.findall(r'\d+', view_text)
                        if numbers:
                            view_count = int(numbers[-1])  # ë§ˆì§€ë§‰ ìˆ«ìê°€ ì¡°íšŒìˆ˜
                            break
                except:
                    continue
            
            metadata['view_count'] = view_count
            
            # ì¶”ì²œìˆ˜ ì¶”ì¶œ
            like_count = 0
            like_selectors = [
                '.btm_area .side.fr span:has-text("ì¶”ì²œ")',
                '.side.fr span:has-text("ì¶”ì²œ")',
                'td.m_no:has-text("ì¶”ì²œ")',
                '.meta .m_no:has-text("ì¶”ì²œ")',
                '.document_info .like_count',
                'span:has-text("ì¶”ì²œ")',
                '.like_count',
                '.voted_count'
            ]
            
            for selector in like_selectors:
                try:
                    like_element = await self.page.query_selector(selector)
                    if like_element:
                        like_text = await like_element.inner_text()
                        # "ì¶”ì²œ ìˆ˜ 229" ë˜ëŠ” "ì¶”ì²œ 229" í˜•íƒœì—ì„œ ìˆ«ìë§Œ ì¶”ì¶œ
                        import re
                        numbers = re.findall(r'\d+', like_text)
                        if numbers:
                            like_count = int(numbers[-1])  # ë§ˆì§€ë§‰ ìˆ«ìê°€ ì¶”ì²œìˆ˜
                            break
                except:
                    continue
            
            metadata['like_count'] = like_count
            
            # ë¹„ì¶”ì²œìˆ˜ ì¶”ì¶œ
            dislike_count = 0
            dislike_selectors = [
                '.btm_area .side.fr span:has-text("ë¹„ì¶”ì²œ")',
                '.side.fr span:has-text("ë¹„ì¶”ì²œ")',
                'td.m_no:has-text("ë¹„ì¶”ì²œ")',
                '.meta .m_no:has-text("ë¹„ì¶”ì²œ")',
                '.document_info .dislike_count',
                'span:has-text("ë¹„ì¶”ì²œ")',
                '.dislike_count',
                '.blamed_count'
            ]
            
            for selector in dislike_selectors:
                try:
                    dislike_element = await self.page.query_selector(selector)
                    if dislike_element:
                        dislike_text = await dislike_element.inner_text()
                        # "ë¹„ì¶”ì²œ 3" í˜•íƒœì—ì„œ ìˆ«ìë§Œ ì¶”ì¶œ
                        import re
                        numbers = re.findall(r'\d+', dislike_text)
                        if numbers:
                            dislike_count = int(numbers[-1])
                            break
                except:
                    continue
            
            metadata['dislike_count'] = dislike_count
            
            # ëŒ“ê¸€ìˆ˜ ì¶”ì¶œ (ì›ë³¸ êµ¬ì¡°: .btm_area .side.fr span:has-text("ëŒ“ê¸€"))
            comment_count = 0
            comment_count_selectors = [
                '.btm_area .side.fr span:has-text("ëŒ“ê¸€")',
                '.side.fr span:has-text("ëŒ“ê¸€")',
                'td.m_no:has-text("ëŒ“ê¸€")',
                '.meta .m_no:has-text("ëŒ“ê¸€")',
                '.document_info .comment_count',
                'span:has-text("ëŒ“ê¸€")',
                '.comment_count'
            ]
            
            for selector in comment_count_selectors:
                try:
                    comment_element = await self.page.query_selector(selector)
                    if comment_element:
                        comment_text = await comment_element.inner_text()
                        # "ëŒ“ê¸€ 90" í˜•íƒœì—ì„œ ìˆ«ìë§Œ ì¶”ì¶œ
                        import re
                        numbers = re.findall(r'\d+', comment_text)
                        if numbers:
                            comment_count = int(numbers[-1])
                            break
                except:
                    continue
            
            metadata['comment_count'] = comment_count
            
            # ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ (ê¸°ì¡´ ì„±ê³µí•œ ì…€ë ‰í„°ë“¤)
            category_selectors = [
                '.cate',
                '.category',
                '.board_category',
                'td.cate a'
            ]
            
            for selector in category_selectors:
                try:
                    category_element = await self.page.query_selector(selector)
                    if category_element:
                        category_text = await category_element.inner_text()
                        if category_text and category_text.strip():
                            metadata['category'] = category_text.strip()
                            break
                except:
                    continue
            
            logger.info(f"âœ… ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ: ì œëª©={metadata.get('title', 'N/A')}, ì‘ì„±ì={metadata.get('author', 'N/A')}, ì¡°íšŒìˆ˜={metadata.get('view_count', 0)}, ì¶”ì²œìˆ˜={metadata.get('like_count', 0)}")
            return metadata
            
        except Exception as e:
            logger.error(f"ğŸ’¥ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {}

    async def extract_comments_data(self) -> List[Dict]:
        """ëŒ“ê¸€ ë°ì´í„° ì¶”ì¶œ (ê¸°ì¡´ ì„±ê³µí•œ ë¡œì§ ê¸°ë°˜)"""
        try:
            comments = []
            
            # ë¸Œë¼ìš°ì €ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì€ ê²½ìš° ì²˜ë¦¬
            if self.page is None:
                logger.error("ğŸ’¥ ë¸Œë¼ìš°ì €ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return []
            
            # FMì½”ë¦¬ì•„ ëŒ“ê¸€ êµ¬ì¡°ì— ë§ëŠ” ì…€ë ‰í„°ë“¤ (ê¸°ì¡´ ì„±ê³µí•œ ì…€ë ‰í„°)
            comment_selectors = [
                'ul.fdb_lst_ul li',
                'div.fdb_lst_ul li',
                'ul.comment_list li',
                'div.comment_list li'
            ]
            
            comment_elements = []
            for selector in comment_selectors:
                try:
                    elements = await self.page.query_selector_all(selector)
                    if elements:
                        # ì‹¤ì œ ëŒ“ê¸€ì¸ì§€ í™•ì¸ (comment_ IDê°€ ìˆëŠ” ê²ƒë§Œ)
                        valid_elements = []
                        for element in elements:
                            element_id = await element.get_attribute('id')
                            if element_id and element_id.startswith('comment_'):
                                valid_elements.append(element)
                        
                        if valid_elements:
                            comment_elements = valid_elements
                            logger.info(f"âœ… {len(valid_elements)}ê°œ ëŒ“ê¸€ ìš”ì†Œ ë°œê²¬ (ì…€ë ‰í„°: {selector})")
                            break
                except Exception as e:
                    logger.warning(f"âš ï¸ ì…€ë ‰í„° {selector} ì‹¤íŒ¨: {e}")
                    continue
            
            if not comment_elements:
                logger.warning("âš ï¸ ëŒ“ê¸€ ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return []
            
            # ê° ëŒ“ê¸€ ìš”ì†Œì—ì„œ ë°ì´í„° ì¶”ì¶œ
            for comment_element in comment_elements:
                try:
                    comment_data = await self.extract_single_comment_data(comment_element)
                    if comment_data and comment_data.get('content'):
                        comments.append(comment_data)
                except Exception as e:
                    logger.warning(f"âš ï¸ ëŒ“ê¸€ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                    continue
            
            logger.info(f"âœ… ì´ {len(comments)}ê°œ ëŒ“ê¸€ ìˆ˜ì§‘ ì™„ë£Œ")
            return comments
            
        except Exception as e:
            logger.error(f"ğŸ’¥ ëŒ“ê¸€ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []

    async def extract_single_comment_data(self, comment_element) -> Optional[Dict]:
        """ê°œë³„ ëŒ“ê¸€ ë°ì´í„° ì¶”ì¶œ (ê¸°ì¡´ ì„±ê³µí•œ ë¡œì§ ê¸°ë°˜ + ë ˆë²¨ ì •ë³´ ì¶”ê°€)"""
        try:
            comment_data = {}
            
            # ëŒ“ê¸€ ID
            comment_data['comment_id'] = await comment_element.get_attribute('id') or ''
            
            # ì‘ì„±ì (ê¸°ì¡´ ì„±ê³µí•œ ì…€ë ‰í„°ë“¤)
            author_selectors = [
                'div.side a.member_plate', 
                'div.meta a.member_plate', 
                'div.meta a',
                '.member_plate',
                '.comment_author',
                '.author'
            ]
            
            author_found = False
            for selector in author_selectors:
                try:
                    author_element = await comment_element.query_selector(selector)
                    if author_element:
                        author_text = await author_element.inner_text()
                        if author_text and author_text.strip():
                            comment_data['author'] = author_text.strip()
                            author_found = True
                            break
                except:
                    continue
            
            if not author_found:
                comment_data['author'] = 'ìµëª…'
            
            # ëŒ“ê¸€ ë‚´ìš© (ê¸°ì¡´ ì„±ê³µí•œ ì…€ë ‰í„°ë“¤)
            content_selectors = [
                'div.comment-content div',
                'div.comment-content',
                '.comment_content',
                '.content',
                '.comment_text'
            ]
            
            content_found = False
            for selector in content_selectors:
                try:
                    content_element = await comment_element.query_selector(selector)
                    if content_element:
                        content_text = await content_element.inner_text()
                        if content_text and content_text.strip():
                            comment_data['content'] = content_text.strip()
                            content_found = True
                            break
                except:
                    continue
            
            if not content_found:
                comment_data['content'] = ''
            
            # ì‘ì„± ì‹œê°„ (ê¸°ì¡´ ì„±ê³µí•œ ì…€ë ‰í„°ë“¤)
            date_selectors = [
                'div.meta span.date',
                'span.date.m_no',
                '.date',
                '.comment_date',
                '.time'
            ]
            
            date_found = False
            for selector in date_selectors:
                try:
                    date_element = await comment_element.query_selector(selector)
                    if date_element:
                        date_text = await date_element.inner_text()
                        if date_text and date_text.strip():
                            comment_data['date'] = date_text.strip()
                            date_found = True
                            break
                except:
                    continue
            
            if not date_found:
                comment_data['date'] = ''
            
            # ëŒ“ê¸€ ë ˆë²¨ ì¶”ì¶œ (ë“¤ì—¬ì“°ê¸° ê¸°ì¤€) - ì¤‘ìš”í•œ ì¶”ê°€ ë¶€ë¶„
            try:
                # style ì†ì„±ì—ì„œ margin-left í™•ì¸
                style_attr = await comment_element.get_attribute('style')
                class_attr = await comment_element.get_attribute('class')
                
                level = 0
                
                # margin-leftë¡œ ë ˆë²¨ ê³„ì‚°
                if style_attr and 'margin-left:' in style_attr:
                    import re
                    margin_match = re.search(r'margin-left:(\d+)%', style_attr)
                    if margin_match:
                        margin_left = int(margin_match.group(1))
                        level = margin_left // 2  # 2%ë‹¹ 1ë ˆë²¨
                
                # í´ë˜ìŠ¤ëª…ìœ¼ë¡œë„ í™•ì¸
                if class_attr and ('re' in class_attr or 'reply' in class_attr or 'depth' in class_attr):
                    level = max(level, 1)
                
                # ëŒ€ëŒ“ê¸€ í‘œì‹œ í™•ì¸ (FMì½”ë¦¬ì•„ íŠ¹ì„±)
                reply_indicators = await comment_element.query_selector_all('a.findParent, .reply_to, .parent_comment')
                if reply_indicators:
                    level = max(level, 1)
                
                comment_data['level'] = level
                comment_data['is_reply'] = level > 0
                
            except:
                comment_data['level'] = 0
                comment_data['is_reply'] = False
            
            # ë¶€ëª¨ ëŒ“ê¸€ ì •ë³´ (ëŒ€ëŒ“ê¸€ì¸ ê²½ìš°)
            try:
                if comment_data['is_reply']:
                    parent_selectors = [
                        'a.findParent',
                        '.parent_comment',
                        '.reply_to'
                    ]
                    
                    parent_comment = ''
                    for selector in parent_selectors:
                        try:
                            parent_element = await comment_element.query_selector(selector)
                            if parent_element:
                                parent_text = await parent_element.inner_text()
                                if parent_text and parent_text.strip():
                                    parent_comment = parent_text.strip()
                                    break
                        except:
                            continue
                    
                    comment_data['parent_comment'] = parent_comment
                else:
                    comment_data['parent_comment'] = ''
                    
            except:
                comment_data['parent_comment'] = ''
            
            # ì¶”ì²œìˆ˜/ë¹„ì¶”ì²œìˆ˜ ì¶”ì¶œ (ê¸°ì¡´ ì„±ê³µí•œ ë¡œì§)
            try:
                # ì¶”ì²œìˆ˜
                vote_selectors = [
                    'span.vote span.voted_count',
                    'span.voted_count',
                    '.comment_likes',
                    '.likes',
                    '.vote_up'
                ]
                
                vote_count = 0
                for selector in vote_selectors:
                    try:
                        vote_element = await comment_element.query_selector(selector)
                        if vote_element:
                            vote_text = await vote_element.inner_text()
                            if vote_text and vote_text.strip().isdigit():
                                vote_count = int(vote_text.strip())
                                break
                    except:
                        continue
                
                comment_data['vote_count'] = vote_count
                
            except:
                comment_data['vote_count'] = 0
            
            try:
                # ë¹„ì¶”ì²œìˆ˜
                blame_selectors = [
                    'span.vote span.blamed_count',
                    'span.blamed_count',
                    '.comment_dislikes',
                    '.dislikes',
                    '.vote_down'
                ]
                
                blame_count = 0
                for selector in blame_selectors:
                    try:
                        blame_element = await comment_element.query_selector(selector)
                        if blame_element:
                            blame_text = await blame_element.inner_text()
                            if blame_text and blame_text.strip().isdigit():
                                blame_count = int(blame_text.strip())
                                break
                    except:
                        continue
                
                comment_data['blame_count'] = blame_count
                
            except:
                comment_data['blame_count'] = 0
            
            return comment_data
            
        except Exception as e:
            logger.error(f"ğŸ’¥ ê°œë³„ ëŒ“ê¸€ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None

    def parse_post_id_from_url(self, url: str) -> str:
        """URLì—ì„œ ê²Œì‹œê¸€ ID ì¶”ì¶œ"""
        try:
            # FMì½”ë¦¬ì•„ URL íŒ¨í„´ë“¤
            # https://www.fmkorea.com/8449897319
            # https://www.fmkorea.com/index.php?mid=politics&document_srl=8450144891
            
            if '/index.php' in url:
                # URL íŒŒë¼ë¯¸í„°ì—ì„œ document_srl ì¶”ì¶œ
                from urllib.parse import parse_qs, urlparse
                parsed = urlparse(url)
                params = parse_qs(parsed.query)
                if 'document_srl' in params:
                    return params['document_srl'][0]
            else:
                # ì§ì ‘ URLì—ì„œ ìˆ«ì ì¶”ì¶œ
                match = re.search(r'/(\d+)/?$', url)
                if match:
                    return match.group(1)
            
            # ìµœí›„ ìˆ˜ë‹¨: URLì—ì„œ ê°€ì¥ ê¸´ ìˆ«ì ì‹œí€€ìŠ¤ ì¶”ì¶œ
            numbers = re.findall(r'\d+', url)
            if numbers:
                return max(numbers, key=len)
            
            return 'unknown'
            
        except Exception as e:
            logger.error(f"ğŸ’¥ ê²Œì‹œê¸€ ID ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return 'unknown'


# í¸ì˜ í•¨ìˆ˜
async def scrape_fmkorea_experiment(post_url: str) -> Dict:
    """
    FMì½”ë¦¬ì•„ ì‹¤í—˜ìš© ìŠ¤í¬ë˜í•‘ í¸ì˜ í•¨ìˆ˜
    
    Args:
        post_url: ê²Œì‹œê¸€ URL
    
    Returns:
        Dict: ì‹¤í—˜ìš© ìŠ¤í¬ë˜í•‘ ê²°ê³¼
    """
    async with FMKoreaExperimentScraper() as scraper:
        return await scraper.scrape_post_experiment(post_url)


async def scrape_multiple_posts_experiment(post_urls: List[str]) -> List[Dict]:
    """
    ì—¬ëŸ¬ ê²Œì‹œê¸€ ì‹¤í—˜ìš© ìŠ¤í¬ë˜í•‘ í¸ì˜ í•¨ìˆ˜
    
    Args:
        post_urls: ê²Œì‹œê¸€ URL ë¦¬ìŠ¤íŠ¸
    
    Returns:
        List[Dict]: ì‹¤í—˜ìš© ìŠ¤í¬ë˜í•‘ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    results = []
    
    async with FMKoreaExperimentScraper() as scraper:
        for url in post_urls:
            try:
                result = await scraper.scrape_post_experiment(url)
                results.append(result)
                
                # ìš”ì²­ ê°„ ëŒ€ê¸° (ì„œë²„ ë¶€í•˜ ë°©ì§€)
                await asyncio.sleep(2)
                
            except Exception as e:
                logger.error(f"ğŸ’¥ {url} ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
                results.append({
                    'post_url': url,
                    'error': str(e),
                    'scraped_at': datetime.now().isoformat()
                })
    
    return results 