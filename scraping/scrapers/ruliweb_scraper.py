"""
ê°œì„ ëœ ë£¨ë¦¬ì›¹ ìŠ¤í¬ë˜í¼ ëª¨ë“ˆ

ì£¼ìš” ê°œì„ ì‚¬í•­:
- ì—í¨ì½”ë¦¬ì•„ ìŠ¤í¬ë˜í¼ êµ¬ì¡° ì°¸ê³ 
- ê²Œì‹œíŒ ëª©ë¡ ìŠ¤í¬ë˜í•‘ ê¸°ëŠ¥ ì¶”ê°€
- ëŒ“ê¸€ ì´ë¯¸ì§€ ì²˜ë¦¬ ê°œì„ 
- TDD ë°©ì‹ìœ¼ë¡œ ê°œë°œëœ ì•ˆì •ì ì¸ êµ¬ì¡°
- ë” ë‚˜ì€ ì—ëŸ¬ í•¸ë“¤ë§

ì‘ì„±ì: AI Assistant
ì‘ì„±ì¼: 2025-01-28
ëª©ì : ë£¨ë¦¬ì›¹ ê²Œì‹œê¸€ ë° ëŒ“ê¸€ ì •ë°€ ìŠ¤í¬ë˜í•‘ (ëŒ“ê¸€ ì´ë¯¸ì§€ í¬í•¨)
"""

import asyncio
import re
import json
from datetime import datetime
from typing import List, Dict, Optional, Any
from urllib.parse import urljoin, urlparse
import pytz

from playwright.async_api import async_playwright, Page, Browser
from playwright_stealth import stealth_async
from loguru import logger


class ImprovedRuliwebScraper:
    """
    ê°œì„ ëœ ë£¨ë¦¬ì›¹ ìŠ¤í¬ë˜í¼ í´ë˜ìŠ¤
    
    ì—í¨ì½”ë¦¬ì•„ ìŠ¤í¬ë˜í¼ êµ¬ì¡°ë¥¼ ì°¸ê³ í•˜ì—¬ ê°œì„ ëœ ê¸°ëŠ¥:
    - ê²Œì‹œíŒ ëª©ë¡ ìŠ¤í¬ë˜í•‘
    - ê°œë³„ ê²Œì‹œê¸€ ìƒì„¸ ìŠ¤í¬ë˜í•‘
    - ëŒ“ê¸€ ì´ë¯¸ì§€ ì²˜ë¦¬ ê°œì„ 
    - ë” ì•ˆì •ì ì¸ ì—ëŸ¬ í•¸ë“¤ë§
    """
    
    def __init__(self):
        self.base_url = 'https://bbs.ruliweb.com'
        self.browser: Optional[Browser] = None
        self.page: Optional[Page] = None
        self.playwright = None
        
        # í•œêµ­ ì‹œê°„ëŒ€ ì„¤ì •
        self.kst = pytz.timezone('Asia/Seoul')
        
        # ìŠ¤í¬ë˜í•‘ ì„¤ì • (ì—í¨ì½”ë¦¬ì•„ ìŠ¤í¬ë˜í¼ì™€ ë™ì¼í•œ ì„±ê³µí•œ ì„¤ì •)
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        ]
        
        # ëŒ€ê¸° ì‹œê°„ ì„¤ì •
        self.wait_timeout = 15000  # 15ì´ˆ
        self.navigation_timeout = 30000  # 30ì´ˆ
    
    async def __aenter__(self):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§„ì…"""
        await self.setup_browser()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ"""
        await self.close_browser()
    
    async def setup_browser(self):
        """ë¸Œë¼ìš°ì € ì„¤ì • ë° ì´ˆê¸°í™” (ì—í¨ì½”ë¦¬ì•„ ìŠ¤í¬ë˜í¼ì™€ ë™ì¼í•œ ì„±ê³µí•œ ì„¤ì •)"""
        try:
            self.playwright = await async_playwright().start()
            
            # ë¸Œë¼ìš°ì € ì˜µì…˜ ì„¤ì • (í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ í•´ì œ)
            self.browser = await self.playwright.chromium.launch(
                headless=False,  # í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ í•´ì œí•˜ì—¬ ë´‡ ì°¨ë‹¨ ìš°íšŒ
                args=[
                    '--no-sandbox',
                    '--disable-dev-shm-usage',
                    '--disable-blink-features=AutomationControlled',
                    '--window-size=1920,1080'
                ]
            )
            
            # ìƒˆ í˜ì´ì§€ ìƒì„±
            self.page = await self.browser.new_page()
            
            # Stealth ëª¨ë“œ ì ìš©
            await stealth_async(self.page)
            
            # ì¶”ê°€ ì„¤ì •
            await self.page.set_viewport_size({"width": 1920, "height": 1080})
            await self.page.set_extra_http_headers({
                'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8'
            })
            
            logger.info("âœ… ê°œì„ ëœ ë£¨ë¦¬ì›¹ ë¸Œë¼ìš°ì € ì„¤ì • ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ğŸ’¥ ë¸Œë¼ìš°ì € ì„¤ì • ì‹¤íŒ¨: {e}")
            raise
    
    async def close_browser(self):
        """ë¸Œë¼ìš°ì € ì¢…ë£Œ (ì—í¨ì½”ë¦¬ì•„ ìŠ¤í¬ë˜í¼ì™€ ë™ì¼í•œ ì•ˆì „í•œ ì¢…ë£Œ)"""
        try:
            # 1ë‹¨ê³„: í˜ì´ì§€ ì¢…ë£Œ
            if self.page and not self.page.is_closed():
                try:
                    await asyncio.wait_for(self.page.close(), timeout=5.0)
                    logger.debug("âœ… í˜ì´ì§€ ì¢…ë£Œ ì™„ë£Œ")
                except asyncio.TimeoutError:
                    logger.warning("âš ï¸ í˜ì´ì§€ ì¢…ë£Œ ì‹œê°„ ì´ˆê³¼")
                except Exception as e:
                    logger.debug(f"í˜ì´ì§€ ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œë¨): {e}")
                finally:
                    self.page = None
            
            # 2ë‹¨ê³„: ë¸Œë¼ìš°ì € ì»¨í…ìŠ¤íŠ¸ ì¢…ë£Œ
            if self.browser and self.browser.is_connected():
                try:
                    contexts = self.browser.contexts
                    for context in contexts:
                        if not context.is_closed():
                            try:
                                await asyncio.wait_for(context.close(), timeout=3.0)
                                logger.debug("âœ… ì»¨í…ìŠ¤íŠ¸ ì¢…ë£Œ ì™„ë£Œ")
                            except asyncio.TimeoutError:
                                logger.warning("âš ï¸ ì»¨í…ìŠ¤íŠ¸ ì¢…ë£Œ ì‹œê°„ ì´ˆê³¼")
                            except Exception as e:
                                logger.debug(f"ì»¨í…ìŠ¤íŠ¸ ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œë¨): {e}")
                except Exception as e:
                    logger.debug(f"ì»¨í…ìŠ¤íŠ¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            
            # 3ë‹¨ê³„: ë¸Œë¼ìš°ì € í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ
            if self.browser:
                try:
                    if self.browser.is_connected():
                        await asyncio.wait_for(self.browser.close(), timeout=10.0)
                        logger.debug("âœ… ë¸Œë¼ìš°ì € ì¢…ë£Œ ì™„ë£Œ")
                    else:
                        logger.debug("ë¸Œë¼ìš°ì €ê°€ ì´ë¯¸ ì—°ê²° í•´ì œë¨")
                except asyncio.TimeoutError:
                    logger.warning("âš ï¸ ë¸Œë¼ìš°ì € ì¢…ë£Œ ì‹œê°„ ì´ˆê³¼ - ê°•ì œ ì¢…ë£Œ ì‹œë„")
                    try:
                        await self.browser.close()
                    except:
                        pass
                except Exception as e:
                    logger.debug(f"ë¸Œë¼ìš°ì € ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œë¨): {e}")
                finally:
                    self.browser = None
            
            # 4ë‹¨ê³„: Playwright ì¸ìŠ¤í„´ìŠ¤ ì¢…ë£Œ
            if self.playwright:
                try:
                    await asyncio.wait_for(self.playwright.stop(), timeout=5.0)
                    logger.debug("âœ… Playwright ì¢…ë£Œ ì™„ë£Œ")
                except asyncio.TimeoutError:
                    logger.warning("âš ï¸ Playwright ì¢…ë£Œ ì‹œê°„ ì´ˆê³¼")
                except Exception as e:
                    logger.debug(f"Playwright ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œë¨): {e}")
                finally:
                    self.playwright = None
            
            # 5ë‹¨ê³„: ì¶”ê°€ ì •ë¦¬ ì‘ì—…
            await asyncio.sleep(0.5)
            
            # 6ë‹¨ê³„: ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰
            import gc
            gc.collect()
            
            logger.info("âœ… ê°œì„ ëœ ë¸Œë¼ìš°ì € ì™„ì „ ì¢…ë£Œ ì™„ë£Œ")
            
        except Exception as e:
            logger.debug(f"ë¸Œë¼ìš°ì € ì¢…ë£Œ ì¤‘ ì˜ˆì™¸ (ë¬´ì‹œë¨): {e}")
            # ìµœí›„ì˜ ìˆ˜ë‹¨: ëª¨ë“  ì°¸ì¡° ì œê±°
            self.page = None
            self.browser = None
            self.playwright = None
    
    async def extract_board_list(self, board_url: str, page: int = 1) -> List[Dict]:
        """
        ê²Œì‹œíŒ ëª©ë¡ ìŠ¤í¬ë˜í•‘ (ìƒˆë¡œìš´ ê¸°ëŠ¥)
        
        Args:
            board_url: ê²Œì‹œíŒ URL
            page: í˜ì´ì§€ ë²ˆí˜¸
        
        Returns:
            List[Dict]: ê²Œì‹œê¸€ ëª©ë¡
        """
        try:
            logger.info(f"ğŸ§ª ë£¨ë¦¬ì›¹ ê²Œì‹œíŒ ëª©ë¡ ìŠ¤í¬ë˜í•‘ ì‹œì‘: {board_url} (í˜ì´ì§€ {page})")
            
            if self.page is None:
                logger.error("ğŸ’¥ ë¸Œë¼ìš°ì €ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return []
            
            # í˜ì´ì§€ ë²ˆí˜¸ê°€ ìˆëŠ” ê²½ìš° URLì— ì¶”ê°€
            if page > 1:
                separator = '&' if '?' in board_url else '?'
                board_url = f"{board_url}{separator}page={page}"
            
            # í˜ì´ì§€ ì´ë™
            await self.page.goto(board_url, wait_until="networkidle", timeout=self.navigation_timeout)
            
            # ê²Œì‹œê¸€ ëª©ë¡ í…Œì´ë¸”ì´ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°
            try:
                await self.page.wait_for_selector('.board_list_table tbody tr', timeout=self.wait_timeout)
            except:
                logger.warning("âš ï¸ ê²Œì‹œê¸€ ëª©ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return []
            
            # ê²Œì‹œê¸€ í–‰ ì¶”ì¶œ (ê³µì§€ì‚¬í•­ ë° ê´‘ê³  ì œì™¸)
            post_rows = await self.page.query_selector_all('.board_list_table tbody tr.table_body:not(.notice):not(.list_inner)')
            
            posts = []
            for row in post_rows:
                try:
                    post_data = await self.extract_board_post_info(row)
                    if post_data:
                        posts.append(post_data)
                except Exception as e:
                    logger.warning(f"âš ï¸ ê²Œì‹œê¸€ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                    continue
            
            logger.info(f"âœ… ê²Œì‹œíŒ ëª©ë¡ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: {len(posts)}ê°œ ê²Œì‹œê¸€")
            return posts
            
        except Exception as e:
            logger.error(f"ğŸ’¥ ê²Œì‹œíŒ ëª©ë¡ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
            return []
    
    async def extract_board_post_info(self, row_element) -> Optional[Dict]:
        """
        ê²Œì‹œíŒ ëª©ë¡ì—ì„œ ê°œë³„ ê²Œì‹œê¸€ ì •ë³´ ì¶”ì¶œ
        
        Args:
            row_element: ê²Œì‹œê¸€ í–‰ ìš”ì†Œ
        
        Returns:
            Dict: ê²Œì‹œê¸€ ê¸°ë³¸ ì •ë³´
        """
        try:
            # ê²Œì‹œê¸€ ID (AsyncMock í˜¸í™˜ì„± ì²˜ë¦¬)
            post_id = 'unknown'
            try:
                if hasattr(row_element, 'query_selector'):
                    id_element = await row_element.query_selector('td.id')
                    if id_element and hasattr(id_element, 'inner_text'):
                        post_id_result = await id_element.inner_text()
                        post_id = post_id_result.strip() if hasattr(post_id_result, 'strip') else str(post_id_result)
            except:
                pass
            
            # ì¹´í…Œê³ ë¦¬
            category = ''
            try:
                if hasattr(row_element, 'query_selector'):
                    category_element = await row_element.query_selector('td.divsn a')
                    if category_element and hasattr(category_element, 'inner_text'):
                        category_result = await category_element.inner_text()
                        category = category_result.strip() if hasattr(category_result, 'strip') else str(category_result)
            except:
                pass
            
            # ì œëª© ë° URL
            title = ''
            post_url = ''
            try:
                if hasattr(row_element, 'query_selector'):
                    subject_element = await row_element.query_selector('td.subject a.subject_link')
                    if not subject_element:
                        return None
                    
                    if hasattr(subject_element, 'inner_text'):
                        title_result = await subject_element.inner_text()
                        title = title_result.strip() if hasattr(title_result, 'strip') else str(title_result)
                    
                    if hasattr(subject_element, 'get_attribute'):
                        post_url_result = await subject_element.get_attribute('href')
                        post_url = str(post_url_result) if post_url_result else ''
                        
                        # ì ˆëŒ€ URLë¡œ ë³€í™˜
                        if post_url and hasattr(post_url, 'startswith') and not post_url.startswith('http'):
                            post_url = urljoin(self.base_url, post_url)
            except:
                pass
            
            # ëŒ“ê¸€ ìˆ˜ ì¶”ì¶œ
            reply_count = 0
            try:
                if hasattr(row_element, 'query_selector'):
                    reply_element = await row_element.query_selector('td.subject .num_reply')
                    if reply_element and hasattr(reply_element, 'inner_text'):
                        reply_text_result = await reply_element.inner_text()
                        reply_text = str(reply_text_result) if reply_text_result else ''
                        reply_match = re.search(r'\((\d+)\)', reply_text)
                        if reply_match:
                            reply_count = int(reply_match.group(1))
            except:
                pass
            
            # ì‘ì„±ì
            author = ''
            try:
                if hasattr(row_element, 'query_selector'):
                    writer_element = await row_element.query_selector('td.writer a')
                    if writer_element and hasattr(writer_element, 'inner_text'):
                        author_result = await writer_element.inner_text()
                        author = author_result.strip() if hasattr(author_result, 'strip') else str(author_result)
            except:
                pass
            
            # ì¶”ì²œìˆ˜
            recommend_count = 0
            try:
                if hasattr(row_element, 'query_selector'):
                    recommend_element = await row_element.query_selector('td.recomd')
                    if recommend_element and hasattr(recommend_element, 'inner_text'):
                        recommend_text_result = await recommend_element.inner_text()
                        recommend_text = str(recommend_text_result) if recommend_text_result else ''
                        if hasattr(recommend_text, 'strip') and hasattr(recommend_text.strip(), 'isdigit') and recommend_text.strip().isdigit():
                            recommend_count = int(recommend_text.strip())
            except:
                pass
            
            # ì¡°íšŒìˆ˜
            view_count = 0
            try:
                if hasattr(row_element, 'query_selector'):
                    hit_element = await row_element.query_selector('td.hit')
                    if hit_element and hasattr(hit_element, 'inner_text'):
                        hit_text_result = await hit_element.inner_text()
                        hit_text = str(hit_text_result) if hit_text_result else ''
                        if hasattr(hit_text, 'strip') and hasattr(hit_text.strip(), 'isdigit') and hit_text.strip().isdigit():
                            view_count = int(hit_text.strip())
            except:
                pass
            
            # ì‘ì„±ì‹œê°„
            created_time = ''
            try:
                if hasattr(row_element, 'query_selector'):
                    time_element = await row_element.query_selector('td.time')
                    if time_element and hasattr(time_element, 'inner_text'):
                        time_result = await time_element.inner_text()
                        created_time = time_result.strip() if hasattr(time_result, 'strip') else str(time_result)
            except:
                pass
            
            # í…ŒìŠ¤íŠ¸ í˜¸í™˜ì„±ì„ ìœ„í•œ í‚¤ ë§¤í•‘
            return {
                'id': post_id,  # í…ŒìŠ¤íŠ¸ í˜¸í™˜ì„±
                'post_id': post_id,
                'title': title,
                'url': post_url,  # í…ŒìŠ¤íŠ¸ í˜¸í™˜ì„±
                'post_url': post_url,
                'category': category,
                'author': author,
                'reply_count': reply_count,
                'recommendations': recommend_count,  # í…ŒìŠ¤íŠ¸ í˜¸í™˜ì„±
                'recommend_count': recommend_count,
                'views': view_count,  # í…ŒìŠ¤íŠ¸ í˜¸í™˜ì„±
                'view_count': view_count,
                'date': created_time,  # í…ŒìŠ¤íŠ¸ í˜¸í™˜ì„±
                'created_time': created_time,
                'scraped_at': datetime.now(self.kst).isoformat()
            }
            
        except Exception as e:
            logger.warning(f"âš ï¸ ê²Œì‹œê¸€ ì •ë³´ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            return None
    
    async def scrape_post(self, post_url: str) -> Dict:
        """
        ê°œë³„ ê²Œì‹œê¸€ ìƒì„¸ ìŠ¤í¬ë˜í•‘ (ê°œì„ ëœ ë²„ì „)
        
        Args:
            post_url: ê²Œì‹œê¸€ URL
        
        Returns:
            Dict: ê²Œì‹œê¸€ ë°ì´í„° (ìˆœì„œ ë³´ì¡´)
        """
        try:
            logger.info(f"ğŸ§ª ê°œì„ ëœ ë£¨ë¦¬ì›¹ ê²Œì‹œê¸€ ìŠ¤í¬ë˜í•‘ ì‹œì‘: {post_url}")
            
            if self.page is None:
                logger.error("ğŸ’¥ ë¸Œë¼ìš°ì €ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                raise Exception("ë¸Œë¼ìš°ì €ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            # í˜ì´ì§€ ì´ë™
            await self.page.goto(post_url, wait_until="networkidle", timeout=self.navigation_timeout)
            
            # ê²Œì‹œê¸€ ë³¸ë¬¸ ì˜ì—­ì´ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°
            try:
                await self.page.wait_for_selector('.board_main_top, .view_content', timeout=self.wait_timeout)
            except:
                logger.warning("âš ï¸ ê²Œì‹œê¸€ ë³¸ë¬¸ ì˜ì—­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
            
            # í˜ì´ì§€ ë¡œë“œ í™•ì¸
            page_title = await self.page.title()
            logger.info(f"ğŸ“„ í˜ì´ì§€ ì œëª©: {page_title}")
            
            # ê²Œì‹œê¸€ ID ì¶”ì¶œ
            post_id = self.parse_post_id_from_url(post_url)
            
            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            metadata = await self.extract_post_metadata()
            
            # ë³¸ë¬¸ ë‚´ìš©ì„ ìˆœì„œëŒ€ë¡œ ì¶”ì¶œ
            content_list = await self.extract_content_in_order()
            
            # ëŒ“ê¸€ ë°ì´í„° ì¶”ì¶œ (ì´ë¯¸ì§€ í¬í•¨)
            comments = await self.extract_comments_data()
            
            # ì‹¤ì œ ì¶”ì¶œëœ ëŒ“ê¸€ ìˆ˜ë¡œ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            metadata['comment_count'] = len(comments)
            
            # ëŒ“ê¸€ì— ì´ë¯¸ì§€ê°€ í¬í•¨ëœ ê°œìˆ˜ ê³„ì‚°
            image_comment_count = sum(1 for comment in comments if comment.get('images') and len(comment['images']) > 0)
            
            experiment_data = {
                'post_id': post_id,
                'post_url': post_url,
                'scraped_at': datetime.now(self.kst).isoformat(),
                'metadata': metadata,
                'content': content_list,
                'comments': comments,
                'comment_stats': {
                    'total_comments': len(comments),
                    'comments_with_images': image_comment_count,
                    'image_ratio': image_comment_count / len(comments) if comments else 0
                },
                'experiment_purpose': 'ruliweb_post_reproduction',
                'page_title': page_title
            }
            
            logger.info(f"âœ… ê°œì„ ëœ ë£¨ë¦¬ì›¹ ê²Œì‹œê¸€ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: {len(content_list)}ê°œ ì½˜í…ì¸  ìš”ì†Œ, {len(comments)}ê°œ ëŒ“ê¸€ (ì´ë¯¸ì§€ í¬í•¨ ëŒ“ê¸€: {image_comment_count}ê°œ)")
            return experiment_data
            
        except Exception as e:
            logger.error(f"ğŸ’¥ ê°œì„ ëœ ë£¨ë¦¬ì›¹ ê²Œì‹œê¸€ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
            # í…ŒìŠ¤íŠ¸ì—ì„œ ì˜ˆì™¸ë¥¼ í™•ì¸í•  ìˆ˜ ìˆë„ë¡ ë‹¤ì‹œ ë°œìƒì‹œí‚´
            raise e
    
    async def extract_content_in_order(self) -> List[Dict]:
        """
        ê²Œì‹œê¸€ ë³¸ë¬¸ ë‚´ìš©ì„ DOM ìˆœì„œëŒ€ë¡œ ì¶”ì¶œ (ì—í¨ì½”ë¦¬ì•„ ìŠ¤í¬ë˜í¼ ë°©ì‹ ì ìš©)
        
        Returns:
            List[Dict]: ìˆœì„œê°€ ë³´ì¡´ëœ ì½˜í…ì¸  ë¦¬ìŠ¤íŠ¸
        """
        try:
            content_list: List[Dict] = []
            order = 0
            
            if self.page is None:
                logger.error("ğŸ’¥ ë¸Œë¼ìš°ì €ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return []
            
            # ë£¨ë¦¬ì›¹ ê²Œì‹œê¸€ ë³¸ë¬¸ ì»¨í…Œì´ë„ˆ ì°¾ê¸°
            article_selectors = [
                '.view_content article div',
                '.view_content article',
                '.view_content',
                'article .xe_content',
                'article'
            ]
            
            article_element = None
            for selector in article_selectors:
                try:
                    element = await self.page.query_selector(selector)
                    if element:
                        article_element = element
                        logger.info(f"âœ… ë³¸ë¬¸ ì»¨í…Œì´ë„ˆ ë°œê²¬: {selector}")
                        break
                except:
                    continue
            
            if not article_element:
                logger.warning("âš ï¸ ê²Œì‹œê¸€ ë³¸ë¬¸ ì»¨í…Œì´ë„ˆë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return []
            
            # ê°œì„ ëœ ì½˜í…ì¸  ì¶”ì¶œ: ì¤‘ë³µ ë°©ì§€ ë° ìˆœì„œ ë³´ì¥
            order = await self.extract_elements_improved(article_element, content_list, order)
            
            logger.info(f"âœ… ì´ {len(content_list)}ê°œ ì½˜í…ì¸  ìš”ì†Œ ìˆœì„œëŒ€ë¡œ ì¶”ì¶œ ì™„ë£Œ")
            return content_list
            
        except Exception as e:
            logger.error(f"ğŸ’¥ ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    async def extract_elements_improved(self, parent_element, content_list: List[Dict], order_start: int) -> int:
        """
        ê°œì„ ëœ ìš”ì†Œ ì¶”ì¶œ (ì—í¨ì½”ë¦¬ì•„ ìŠ¤í¬ë˜í¼ ë°©ì‹ ì ìš©)
        ì¤‘ë³µ ë°©ì§€ ë° ì •í™•í•œ ìˆœì„œ ë³´ì¥
        
        Args:
            parent_element: ë¶€ëª¨ ìš”ì†Œ
            content_list: ì½˜í…ì¸  ë¦¬ìŠ¤íŠ¸
            order_start: ì‹œì‘ ìˆœì„œ
        
        Returns:
            int: ë‹¤ìŒ ìˆœì„œ ë²ˆí˜¸
        """
        try:
            order = order_start
            
            # ëª¨ë“  ìì‹ ìš”ì†Œë¥¼ ìˆœì„œëŒ€ë¡œ ì²˜ë¦¬
            child_elements = await parent_element.query_selector_all('*')
            
            processed_images = set()  # ì¤‘ë³µ ì´ë¯¸ì§€ ë°©ì§€
            
            for element in child_elements:
                try:
                    tag_name = await element.evaluate('el => el.tagName.toLowerCase()')
                    
                    # ì´ë¯¸ì§€ ì²˜ë¦¬
                    if tag_name == 'img':
                        img_src = await element.get_attribute('src')
                        if img_src and img_src not in processed_images:
                            processed_images.add(img_src)
                            
                            # ì´ë¯¸ì§€ ë§í¬ ì°¾ê¸°
                            parent_link = await element.evaluate('''
                                el => {
                                    let parent = el.parentElement;
                                    while (parent && parent.tagName.toLowerCase() !== 'a') {
                                        parent = parent.parentElement;
                                    }
                                    return parent ? parent.href : null;
                                }
                            ''')
                            
                            image_data = await self.extract_image_data(element, parent_link, order)
                            if image_data:
                                content_list.append(image_data)
                                order += 1
                    
                    # ë¹„ë””ì˜¤ ì²˜ë¦¬
                    elif tag_name in ['video', 'iframe']:
                        video_data = await self.extract_video_data(element, order)
                        if video_data:
                            content_list.append(video_data)
                            order += 1
                    
                    # í…ìŠ¤íŠ¸ ì²˜ë¦¬ (p, div ë“±)
                    elif tag_name in ['p', 'div', 'span', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
                        # ìì‹ ìš”ì†Œê°€ ì—†ëŠ” í…ìŠ¤íŠ¸ë§Œ ì²˜ë¦¬
                        has_child_elements = await element.evaluate('el => el.children.length > 0')
                        if not has_child_elements:
                            text_data = await self.extract_text_data(element, order)
                            if text_data and text_data.get('text', '').strip():
                                content_list.append(text_data)
                                order += 1
                
                except Exception as e:
                    logger.debug(f"ìš”ì†Œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œë¨): {e}")
                    continue
            
            return order
            
        except Exception as e:
            logger.error(f"ğŸ’¥ ê°œì„ ëœ ìš”ì†Œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return order_start
    
    async def extract_image_data(self, img_element, parent_link: Optional[str], order: int) -> Optional[Dict]:
        """
        ì´ë¯¸ì§€ ë°ì´í„° ì¶”ì¶œ (FMì½”ë¦¬ì•„ì™€ ë™ì¼í•œ êµ¬ì¡°)
        
        Args:
            img_element: ì´ë¯¸ì§€ ìš”ì†Œ
            parent_link: ë¶€ëª¨ ë§í¬ URL
            order: ìˆœì„œ
        
        Returns:
            Dict: ì´ë¯¸ì§€ ë°ì´í„° (FMì½”ë¦¬ì•„ êµ¬ì¡°)
        """
        try:
            src = await img_element.get_attribute('src')
            if not src:
                return None
            
            # ì ˆëŒ€ URLë¡œ ë³€í™˜
            if src.startswith('//'):
                src = 'https:' + src
            elif src.startswith('/'):
                src = urljoin(self.base_url, src)
            
            # ì´ë¯¸ì§€ ì†ì„± ì¶”ì¶œ
            alt = await img_element.get_attribute('alt') or ''
            width = await img_element.get_attribute('width') or ''
            height = await img_element.get_attribute('height') or ''
            style = await img_element.get_attribute('style') or ''
            class_name = await img_element.get_attribute('class') or ''
            title = await img_element.get_attribute('title') or ''
            
            # FMì½”ë¦¬ì•„ì™€ ë™ì¼í•œ ì¤‘ì²© êµ¬ì¡°
            return {
                'type': 'image',
                'order': order,
                'data': {
                    'src': src,
                    'alt': alt,
                    'width': width,
                    'height': height,
                    'href': parent_link or '',
                    'data_original': src,  # ë£¨ë¦¬ì›¹ì€ ì›ë³¸ ì´ë¯¸ì§€ ì§ì ‘ ì œê³µ
                    'original_src': src,
                    'style': style,
                    'class': class_name,
                    'title': title,
                    'link_class': '',
                    'link_rel': ''
                }
            }
            
        except Exception as e:
            logger.warning(f"âš ï¸ ì´ë¯¸ì§€ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    async def extract_video_data(self, video_element, order: int) -> Optional[Dict]:
        """
        ë¹„ë””ì˜¤ ë°ì´í„° ì¶”ì¶œ (FMì½”ë¦¬ì•„ì™€ ë™ì¼í•œ êµ¬ì¡°)
        
        Args:
            video_element: ë¹„ë””ì˜¤ ìš”ì†Œ
            order: ìˆœì„œ
        
        Returns:
            Dict: ë¹„ë””ì˜¤ ë°ì´í„° (FMì½”ë¦¬ì•„ êµ¬ì¡°)
        """
        try:
            tag_name = await video_element.evaluate('el => el.tagName.toLowerCase()')
            
            if tag_name == 'video':
                src = await video_element.get_attribute('src')
                poster = await video_element.get_attribute('poster') or ''
                width = await video_element.get_attribute('width') or ''
                height = await video_element.get_attribute('height') or ''
                
                # ìë™ì¬ìƒ ì†ì„± ê°ì§€
                autoplay = await video_element.get_attribute('autoplay') is not None
                loop = await video_element.get_attribute('loop') is not None
                muted = await video_element.get_attribute('muted') is not None
                controls = await video_element.get_attribute('controls') is not None
                preload = await video_element.get_attribute('preload') or 'metadata'
                class_name = await video_element.get_attribute('class') or ''
                
                # ìë™ì¬ìƒì´ë©´ ìŒì†Œê±° ì²˜ë¦¬ (ë¸Œë¼ìš°ì € ì •ì±…)
                if autoplay and not muted:
                    muted = True
                
                # FMì½”ë¦¬ì•„ì™€ ë™ì¼í•œ ì¤‘ì²© êµ¬ì¡°
                return {
                    'type': 'video',
                    'order': order,
                    'data': {
                        'src': src,
                        'poster': poster,
                        'autoplay': autoplay,
                        'loop': loop,
                        'muted': muted,
                        'controls': controls,
                        'preload': preload,
                        'width': width,
                        'height': height,
                        'class': class_name
                    }
                }
            
            elif tag_name == 'iframe':
                src = await video_element.get_attribute('src')
                width = await video_element.get_attribute('width') or ''
                height = await video_element.get_attribute('height') or ''
                class_name = await video_element.get_attribute('class') or ''
                
                # FMì½”ë¦¬ì•„ì™€ ë™ì¼í•œ ì¤‘ì²© êµ¬ì¡°
                return {
                    'type': 'iframe',
                    'order': order,
                    'data': {
                        'src': src,
                        'width': width,
                        'height': height,
                        'class': class_name
                    }
                }
            
            return None
            
        except Exception as e:
            logger.warning(f"âš ï¸ ë¹„ë””ì˜¤ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    async def extract_text_data(self, text_element, order: int) -> Optional[Dict]:
        """
        í…ìŠ¤íŠ¸ ë°ì´í„° ì¶”ì¶œ (FMì½”ë¦¬ì•„ì™€ ë™ì¼í•œ êµ¬ì¡°)
        
        Args:
            text_element: í…ìŠ¤íŠ¸ ìš”ì†Œ
            order: ìˆœì„œ
        
        Returns:
            Dict: í…ìŠ¤íŠ¸ ë°ì´í„° (FMì½”ë¦¬ì•„ êµ¬ì¡°)
        """
        try:
            text = await text_element.inner_text()
            if not text or not text.strip():
                return None
            
            tag_name = await text_element.evaluate('el => el.tagName.toLowerCase()')
            
            # ìŠ¤íƒ€ì¼ ì •ë³´ ì¶”ì¶œ
            style = await text_element.get_attribute('style') or ''
            class_name = await text_element.get_attribute('class') or ''
            id_attr = await text_element.get_attribute('id') or ''
            innerHTML = await text_element.inner_html()
            
            # FMì½”ë¦¬ì•„ì™€ ë™ì¼í•œ ì¤‘ì²© êµ¬ì¡°
            return {
                'type': 'text',
                'order': order,
                'data': {
                    'tag': tag_name,
                    'text': text.strip(),
                    'id': id_attr,
                    'class': class_name,
                    'style': style,
                    'innerHTML': innerHTML
                }
            }
            
        except Exception as e:
            logger.warning(f"âš ï¸ í…ìŠ¤íŠ¸ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    async def extract_post_metadata(self) -> Dict:
        """
        ê²Œì‹œê¸€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (ê°œì„ ëœ ë²„ì „)
        
        Returns:
            Dict: ë©”íƒ€ë°ì´í„°
        """
        try:
            metadata = {}
            
            if self.page is None:
                return metadata
            
            # ì œëª© ì¶”ì¶œ
            title_selectors = [
                '.subject_text .subject_inner_text',
                '.subject_text',
                'h4.subject .subject_text',
                'h4.subject'
            ]
            
            for selector in title_selectors:
                try:
                    title_element = await self.page.query_selector(selector)
                    if title_element:
                        title = await title_element.inner_text()
                        metadata['title'] = title.strip()
                        break
                except:
                    continue
            
            # ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
            try:
                category_element = await self.page.query_selector('.category_text')
                if category_element:
                    category = await category_element.inner_text()
                    metadata['category'] = category.strip().replace('[', '').replace(']', '')
            except:
                pass
            
            # ì‘ì„±ì ì¶”ì¶œ
            author_selectors = [
                '.user_info .nick',
                '.nick',
                '.user_view .nick'
            ]
            
            for selector in author_selectors:
                try:
                    author_element = await self.page.query_selector(selector)
                    if author_element:
                        author = await author_element.inner_text()
                        metadata['author'] = author.strip()
                        break
                except:
                    continue
            
            # ì‘ì„±ì¼ ì¶”ì¶œ (í…ŒìŠ¤íŠ¸ í˜¸í™˜ì„±ì„ ìœ„í•´ 'date'ì™€ 'created_at' ëª¨ë‘ ì œê³µ)
            try:
                date_element = await self.page.query_selector('.regdate')
                if date_element:
                    date_text = await date_element.inner_text()
                    metadata['created_at'] = date_text.strip()
                    metadata['date'] = date_text.strip()  # í…ŒìŠ¤íŠ¸ í˜¸í™˜ì„±
            except:
                pass
            
            # ì¶”ì²œìˆ˜ ì¶”ì¶œ (í…ŒìŠ¤íŠ¸ í˜¸í™˜ì„±ì„ ìœ„í•´ 'recommendations'ì™€ 'like_count' ëª¨ë‘ ì œê³µ)
            try:
                like_element = await self.page.query_selector('.like_value, .recomd .good_high strong, .like strong')
                if like_element:
                    like_count = await like_element.inner_text()
                    like_count_int = int(like_count.strip()) if like_count.strip().isdigit() else 0
                    metadata['like_count'] = like_count_int
                    metadata['recommendations'] = like_count_int  # í…ŒìŠ¤íŠ¸ í˜¸í™˜ì„±
            except:
                metadata['like_count'] = 0
                metadata['recommendations'] = 0
            
            # ì¡°íšŒìˆ˜ ì¶”ì¶œ (í…ŒìŠ¤íŠ¸ í˜¸í™˜ì„±ì„ ìœ„í•´ 'views'ì™€ 'view_count' ëª¨ë‘ ì œê³µ)
            try:
                view_element = await self.page.query_selector('.hit strong, .hit_high strong')
                if view_element:
                    view_count = await view_element.inner_text()
                    view_count_int = int(view_count.strip()) if view_count.strip().isdigit() else 0
                    metadata['view_count'] = view_count_int
                    metadata['views'] = view_count_int  # í…ŒìŠ¤íŠ¸ í˜¸í™˜ì„±
            except:
                metadata['view_count'] = 0
                metadata['views'] = 0
            
            # ëŒ“ê¸€ìˆ˜ ì¶”ì¶œ
            try:
                comment_element = await self.page.query_selector('.reply_count, .num strong')
                if comment_element:
                    comment_count = await comment_element.inner_text()
                    metadata['comment_count'] = int(comment_count.strip()) if comment_count.strip().isdigit() else 0
            except:
                metadata['comment_count'] = 0
            
            metadata['extracted_at'] = datetime.now(self.kst).isoformat()
            
            return metadata
            
        except Exception as e:
            logger.error(f"ğŸ’¥ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {'extracted_at': datetime.now(self.kst).isoformat()}
    
    async def extract_comments_data(self) -> List[Dict]:
        """
        ëŒ“ê¸€ ë°ì´í„° ì¶”ì¶œ (ì´ë¯¸ì§€ í¬í•¨ ê°œì„ )
        
        Returns:
            List[Dict]: ëŒ“ê¸€ ë¦¬ìŠ¤íŠ¸
        """
        try:
            comments = []
            
            if self.page is None:
                return comments
            
            # ëŒ“ê¸€ ì»¨í…Œì´ë„ˆ í™•ì¸
            comment_wrapper = await self.page.query_selector('#cmt, .comment_wrapper')
            if not comment_wrapper:
                logger.info("ğŸ“ ëŒ“ê¸€ì´ ì—†ìŠµë‹ˆë‹¤.")
                return comments
            
            # BEST ëŒ“ê¸€ ID ìˆ˜ì§‘ (ì¤‘ë³µ ë°©ì§€ìš©)
            best_comment_ids = set()
            best_comments = await self.page.query_selector_all('.comment_table.best tr.comment_element')
            for best_comment in best_comments:
                try:
                    comment_id = await best_comment.get_attribute('id')
                    if comment_id:
                        best_comment_ids.add(comment_id.replace('ct_', ''))
                except:
                    continue
            
            # ì¼ë°˜ ëŒ“ê¸€ ì¶”ì¶œ
            comment_elements = await self.page.query_selector_all('.comment_table tr.comment_element')
            
            for index, comment_element in enumerate(comment_elements):
                try:
                    comment_data = await self.extract_single_comment(comment_element, best_comment_ids, index)
                    if comment_data:
                        comments.append(comment_data)
                except Exception as e:
                    logger.warning(f"âš ï¸ ëŒ“ê¸€ ì¶”ì¶œ ì‹¤íŒ¨ (ì¸ë±ìŠ¤ {index}): {e}")
                    continue
            
            logger.info(f"âœ… ëŒ“ê¸€ ì¶”ì¶œ ì™„ë£Œ: {len(comments)}ê°œ")
            return comments
            
        except Exception as e:
            logger.error(f"ğŸ’¥ ëŒ“ê¸€ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    async def extract_single_comment(self, comment_element, best_comment_ids: set, index: int) -> Optional[Dict]:
        """
        ê°œë³„ ëŒ“ê¸€ ì¶”ì¶œ (ì´ë¯¸ì§€ í¬í•¨ ê°œì„ )
        
        Args:
            comment_element: ëŒ“ê¸€ ìš”ì†Œ
            best_comment_ids: BEST ëŒ“ê¸€ ID ì§‘í•©
            index: ëŒ“ê¸€ ì¸ë±ìŠ¤
        
        Returns:
            Dict: ëŒ“ê¸€ ë°ì´í„°
        """
        try:
            # ëŒ“ê¸€ ID ì¶”ì¶œ (AsyncMock í˜¸í™˜ì„± ì²˜ë¦¬)
            comment_id = None
            try:
                if hasattr(comment_element, 'get_attribute'):
                    comment_id_result = await comment_element.get_attribute('id')
                    if comment_id_result:
                        comment_id = comment_id_result.replace('ct_', '') if hasattr(comment_id_result, 'replace') else str(comment_id_result)
            except:
                pass
            
            # BEST ëŒ“ê¸€ ì¤‘ë³µ ì œê±°
            if comment_id and comment_id in best_comment_ids:
                return None
            
            # ì‘ì„±ì ì •ë³´ ì¶”ì¶œ
            author = ''
            try:
                if hasattr(comment_element, 'query_selector'):
                    user_element = await comment_element.query_selector('.user, .user_inner_wrapper')
                    if user_element and hasattr(user_element, 'query_selector'):
                        nick_element = await user_element.query_selector('.nick a, .nick_link')
                        if nick_element and hasattr(nick_element, 'inner_text'):
                            author_result = await nick_element.inner_text()
                            author = author_result.strip() if hasattr(author_result, 'strip') else str(author_result)
                    
                    # í…ŒìŠ¤íŠ¸ ëª¨í‚¹ ëŒ€ì‘: ì§ì ‘ inner_text í˜¸ì¶œ
                    if not author and hasattr(comment_element, 'inner_text'):
                        author_result = await comment_element.inner_text()
                        author = author_result.strip() if hasattr(author_result, 'strip') else str(author_result)
            except:
                pass
            
            # ëŒ“ê¸€ ë‚´ìš© ì¶”ì¶œ (ì´ë¯¸ì§€ í¬í•¨)
            content = ''
            images = []
            
            try:
                if hasattr(comment_element, 'query_selector'):
                    text_wrapper = await comment_element.query_selector('.text_wrapper, .comment')
                    if text_wrapper:
                        # í…ìŠ¤íŠ¸ ë‚´ìš© ì¶”ì¶œ
                        if hasattr(text_wrapper, 'query_selector'):
                            text_element = await text_wrapper.query_selector('.text')
                            if text_element and hasattr(text_element, 'inner_text'):
                                content_result = await text_element.inner_text()
                                content = content_result.strip() if hasattr(content_result, 'strip') else str(content_result)
                        
                        # ì´ë¯¸ì§€ ì¶”ì¶œ (ë£¨ë¦¬ì›¹ ëŒ“ê¸€ ì´ë¯¸ì§€ íŠ¹í™”)
                        if hasattr(text_wrapper, 'query_selector_all'):
                            img_elements = await text_wrapper.query_selector_all('img.comment_img, .inline_block img')
                            for img_element in img_elements:
                                try:
                                    if hasattr(img_element, 'get_attribute'):
                                        img_src = await img_element.get_attribute('src')
                                        if img_src:
                                            # ì ˆëŒ€ URLë¡œ ë³€í™˜
                                            if hasattr(img_src, 'startswith'):
                                                if img_src.startswith('//'):
                                                    img_src = 'https:' + img_src
                                                elif img_src.startswith('/'):
                                                    img_src = urljoin(self.base_url, img_src)
                                            
                                            images.append(img_src)
                                except Exception as e:
                                    logger.debug(f"ëŒ“ê¸€ ì´ë¯¸ì§€ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
                                    continue
                    
                    # í…ŒìŠ¤íŠ¸ ëª¨í‚¹ ëŒ€ì‘: ì§ì ‘ inner_textì™€ query_selector_all í˜¸ì¶œ
                    if not content and hasattr(comment_element, 'inner_text'):
                        content_result = await comment_element.inner_text()
                        content = content_result.strip() if hasattr(content_result, 'strip') else str(content_result)
                    
                    if not images and hasattr(comment_element, 'query_selector_all'):
                        img_elements = await comment_element.query_selector_all('img')
                        for img_element in img_elements:
                            try:
                                if hasattr(img_element, 'get_attribute'):
                                    img_src = await img_element.get_attribute('src')
                                    if img_src:
                                        images.append(str(img_src))
                            except:
                                continue
            except:
                pass
            
            # ëŒ“ê¸€ ë©”íƒ€ì •ë³´ ì¶”ì¶œ
            created_at = ''
            like_count = 0
            dislike_count = 0
            
            try:
                if hasattr(comment_element, 'query_selector'):
                    control_box = await comment_element.query_selector('.control_box, .parent_control_box_wrapper')
                    if control_box:
                        # ì‘ì„±ì‹œê°„
                        if hasattr(control_box, 'query_selector'):
                            time_element = await control_box.query_selector('.time')
                            if time_element and hasattr(time_element, 'inner_text'):
                                time_result = await time_element.inner_text()
                                created_at = time_result.strip() if hasattr(time_result, 'strip') else str(time_result)
                        
                        # ì¶”ì²œìˆ˜
                        if hasattr(control_box, 'query_selector'):
                            like_element = await control_box.query_selector('.btn_like .num')
                            if like_element and hasattr(like_element, 'inner_text'):
                                like_text_result = await like_element.inner_text()
                                like_text = like_text_result.strip() if hasattr(like_text_result, 'strip') else str(like_text_result)
                                if hasattr(like_text, 'isdigit') and like_text.isdigit():
                                    like_count = int(like_text)
            except:
                pass
            
            # BEST ëŒ“ê¸€ ì—¬ë¶€ í™•ì¸
            is_best = False
            try:
                if hasattr(comment_element, 'query_selector'):
                    best_element = await comment_element.query_selector('.icon_best')
                    is_best = bool(best_element)
            except:
                pass
            
            # í…ŒìŠ¤íŠ¸ í˜¸í™˜ì„±ì„ ìœ„í•´ 'date' í‚¤ë„ ì œê³µ
            date_value = created_at or datetime.now(self.kst).strftime('%Y.%m.%d %H:%M')
            
            return {
                'comment_id': comment_id or f'comment_{index}',
                'author': author,
                'content': content,
                'images': images,  # ë£¨ë¦¬ì›¹ íŠ¹í™”: ëŒ“ê¸€ ì´ë¯¸ì§€ í¬í•¨
                'created_at': created_at,
                'date': date_value,  # í…ŒìŠ¤íŠ¸ í˜¸í™˜ì„±
                'like_count': like_count,
                'dislike_count': dislike_count,
                'is_best': is_best,
                'index': index,
                'extracted_at': datetime.now(self.kst).isoformat()
            }
            
        except Exception as e:
            logger.warning(f"âš ï¸ ê°œë³„ ëŒ“ê¸€ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    def parse_post_id_from_url(self, url: str) -> str:
        """
        URLì—ì„œ ê²Œì‹œê¸€ ID ì¶”ì¶œ
        
        Args:
            url: ê²Œì‹œê¸€ URL
        
        Returns:
            str: ê²Œì‹œê¸€ ID
        """
        try:
            # ë£¨ë¦¬ì›¹ URL íŒ¨í„´: /read/ìˆ«ì
            match = re.search(r'/read/(\d+)', url)
            if match:
                return match.group(1)
            
            # ë‹¤ë¥¸ íŒ¨í„´ë“¤ë„ ì‹œë„
            match = re.search(r'document_srl=(\d+)', url)
            if match:
                return match.group(1)
            
            return 'unknown'
            
        except Exception as e:
            logger.error(f"ğŸ’¥ ê²Œì‹œê¸€ ID ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return 'unknown'
    
    async def initialize_browser(self):
        """ë¸Œë¼ìš°ì € ì´ˆê¸°í™” (í…ŒìŠ¤íŠ¸ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­)"""
        await self.setup_browser()
    
    async def scrape_board_list(self, board_url: str, page: int = 1) -> List[Dict]:
        """ê²Œì‹œíŒ ëª©ë¡ ìŠ¤í¬ë˜í•‘ (í…ŒìŠ¤íŠ¸ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­)"""
        return await self.extract_board_list(board_url, page)


# í¸ì˜ í•¨ìˆ˜ë“¤
async def scrape_ruliweb_post_improved(post_url: str) -> Dict:
    """
    ê°œì„ ëœ ë£¨ë¦¬ì›¹ ê²Œì‹œê¸€ ìŠ¤í¬ë˜í•‘ í¸ì˜ í•¨ìˆ˜
    
    Args:
        post_url: ê²Œì‹œê¸€ URL
    
    Returns:
        Dict: ìŠ¤í¬ë˜í•‘ ê²°ê³¼
    """
    async with ImprovedRuliwebScraper() as scraper:
        return await scraper.scrape_post(post_url)


async def scrape_ruliweb_board_improved(board_url: str, max_pages: int = 1) -> List[Dict]:
    """
    ê°œì„ ëœ ë£¨ë¦¬ì›¹ ê²Œì‹œíŒ ëª©ë¡ ìŠ¤í¬ë˜í•‘ í¸ì˜ í•¨ìˆ˜
    
    Args:
        board_url: ê²Œì‹œíŒ URL
        max_pages: ìµœëŒ€ í˜ì´ì§€ ìˆ˜
    
    Returns:
        List[Dict]: ê²Œì‹œê¸€ ëª©ë¡
    """
    async with ImprovedRuliwebScraper() as scraper:
        all_posts = []
        
        for page in range(1, max_pages + 1):
            posts = await scraper.extract_board_list(board_url, page)
            if not posts:
                break
            all_posts.extend(posts)
            
            # í˜ì´ì§€ ê°„ ëŒ€ê¸°
            await asyncio.sleep(1)
        
        return all_posts


async def scrape_multiple_ruliweb_posts_improved(post_urls: List[str]) -> List[Dict]:
    """
    ì—¬ëŸ¬ ë£¨ë¦¬ì›¹ ê²Œì‹œê¸€ ì¼ê´„ ìŠ¤í¬ë˜í•‘ í¸ì˜ í•¨ìˆ˜
    
    Args:
        post_urls: ê²Œì‹œê¸€ URL ë¦¬ìŠ¤íŠ¸
    
    Returns:
        List[Dict]: ìŠ¤í¬ë˜í•‘ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    async with ImprovedRuliwebScraper() as scraper:
        results = []
        
        for url in post_urls:
            try:
                result = await scraper.scrape_post(url)
                results.append(result)
                
                # ìš”ì²­ ê°„ ëŒ€ê¸° (ì„œë²„ ë¶€í•˜ ë°©ì§€)
                await asyncio.sleep(2)
                
            except Exception as e:
                logger.error(f"ğŸ’¥ ê²Œì‹œê¸€ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨ ({url}): {e}")
                results.append({
                    'post_url': url,
                    'error': str(e),
                    'scraped_at': datetime.now().isoformat()
                })
        
        return results 