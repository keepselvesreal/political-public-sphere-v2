"""
FMì½”ë¦¬ì•„ ìŠ¤í¬ë˜í¼ ëª¨ë“ˆ

ì£¼ìš” ê¸°ëŠ¥:
- scrape_fmkorea: FMì½”ë¦¬ì•„ ì •ì¹˜ ê²Œì‹œíŒ ìŠ¤í¬ë˜í•‘ (line 30-80)
- FMKoreaScraper: FMì½”ë¦¬ì•„ ìŠ¤í¬ë˜í¼ í´ë˜ìŠ¤ (line 82-200)
- scrape_board_posts: ê²Œì‹œíŒ ê²Œì‹œê¸€ ëª©ë¡ ìŠ¤í¬ë˜í•‘ (line 202-300)
- extract_post_info: ê°œë³„ ê²Œì‹œê¸€ ì •ë³´ ì¶”ì¶œ (line 302-400)

ì‘ì„±ì: AI Assistant
ì‘ì„±ì¼: 2025-01-28 17:00 KST
ëª©ì : FMì½”ë¦¬ì•„ ê²Œì‹œê¸€ ëª©ë¡ ìŠ¤í¬ë˜í•‘ ë° ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
"""

import asyncio
import re
import json
from datetime import datetime
from typing import List, Dict, Optional, Any
from urllib.parse import urljoin, urlparse
import pytz

from playwright.async_api import async_playwright, Page, Browser
from playwright_stealth import stealth_async
from loguru import logger


async def scrape_fmkorea() -> Dict:
    """
    FMì½”ë¦¬ì•„ ì •ì¹˜ ê²Œì‹œíŒ ìŠ¤í¬ë˜í•‘ í¸ì˜ í•¨ìˆ˜
    
    Returns:
        Dict: ìŠ¤í¬ë˜í•‘ ê²°ê³¼
    """
    async with FMKoreaScraper() as scraper:
        return await scraper.scrape_board_posts()


class FMKoreaScraper:
    """
    FMì½”ë¦¬ì•„ ìŠ¤í¬ë˜í¼ í´ë˜ìŠ¤
    
    ì •ì¹˜ ê²Œì‹œíŒì˜ ê²Œì‹œê¸€ ëª©ë¡ì„ ìŠ¤í¬ë˜í•‘í•˜ê³ 
    ê° ê²Œì‹œê¸€ì˜ ê¸°ë³¸ ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    """
    
    def __init__(self):
        self.base_url = 'https://www.fmkorea.com'
        self.politics_url = 'https://www.fmkorea.com/politics'
        self.browser: Optional[Browser] = None
        self.page: Optional[Page] = None
        
        # í•œêµ­ ì‹œê°„ëŒ€ ì„¤ì •
        self.kst = pytz.timezone('Asia/Seoul')
        
        # ìŠ¤í¬ë˜í•‘ ì„¤ì •
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        ]
        
        # ëŒ€ê¸° ì‹œê°„ ì„¤ì •
        self.wait_timeout = 15000  # 15ì´ˆ
        self.navigation_timeout = 30000  # 30ì´ˆ
    
    async def __aenter__(self):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§„ì…"""
        await self.setup_browser()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ"""
        await self.close_browser()
    
    async def setup_browser(self):
        """ë¸Œë¼ìš°ì € ì„¤ì • ë° ì´ˆê¸°í™”"""
        try:
            playwright = await async_playwright().start()
            
            # ë¸Œë¼ìš°ì € ì˜µì…˜ ì„¤ì •
            self.browser = await playwright.chromium.launch(
                headless=False,  # í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ í•´ì œí•˜ì—¬ ë´‡ ì°¨ë‹¨ ìš°íšŒ
                args=[
                    '--no-sandbox',
                    '--disable-dev-shm-usage',
                    '--disable-blink-features=AutomationControlled',
                    '--window-size=1920,1080'
                ]
            )
            
            # ìƒˆ í˜ì´ì§€ ìƒì„±
            self.page = await self.browser.new_page()
            
            # Stealth ëª¨ë“œ ì ìš©
            await stealth_async(self.page)
            
            # ì¶”ê°€ ì„¤ì •
            await self.page.set_viewport_size({"width": 1920, "height": 1080})
            await self.page.set_extra_http_headers({
                'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8'
            })
            
            logger.info("âœ… ë¸Œë¼ìš°ì € ì„¤ì • ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ğŸ’¥ ë¸Œë¼ìš°ì € ì„¤ì • ì‹¤íŒ¨: {e}")
            raise
    
    async def close_browser(self):
        """ë¸Œë¼ìš°ì € ì¢…ë£Œ"""
        try:
            if self.page:
                await self.page.close()
                self.page = None
                
            if self.browser:
                # ëª¨ë“  ì»¨í…ìŠ¤íŠ¸ì™€ í˜ì´ì§€ë¥¼ ì•ˆì „í•˜ê²Œ ì¢…ë£Œ
                contexts = self.browser.contexts
                for context in contexts:
                    try:
                        await context.close()
                    except:
                        pass
                
                # ë¸Œë¼ìš°ì € í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ
                await self.browser.close()
                self.browser = None
                
            # ì¶”ê°€ ëŒ€ê¸° ì‹œê°„ìœ¼ë¡œ ì™„ì „í•œ ì¢…ë£Œ ë³´ì¥
            await asyncio.sleep(0.5)
            
            logger.info("âœ… ë¸Œë¼ìš°ì € ì¢…ë£Œ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âš ï¸ ë¸Œë¼ìš°ì € ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: {e}")
            # ê°•ì œ ì¢…ë£Œ ì‹œë„
            try:
                if self.browser:
                    await self.browser.close()
            except:
                pass
    
    async def scrape_board_posts(self) -> Dict:
        """
        FMì½”ë¦¬ì•„ ì •ì¹˜ ê²Œì‹œíŒ ê²Œì‹œê¸€ ëª©ë¡ ìŠ¤í¬ë˜í•‘
        
        Returns:
            Dict: ìŠ¤í¬ë˜í•‘ ê²°ê³¼
        """
        try:
            logger.info("ğŸš€ FMì½”ë¦¬ì•„ ì •ì¹˜ ê²Œì‹œíŒ ìŠ¤í¬ë˜í•‘ ì‹œì‘")
            
            if self.page is None:
                logger.error("ğŸ’¥ ë¸Œë¼ìš°ì €ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return {'error': 'ë¸Œë¼ìš°ì € ì´ˆê¸°í™” ì‹¤íŒ¨'}
            
            # ì •ì¹˜ ê²Œì‹œíŒ í˜ì´ì§€ ì´ë™
            await self.page.goto(self.politics_url, wait_until="networkidle", timeout=self.navigation_timeout)
            
            # ê²Œì‹œê¸€ ëª©ë¡ì´ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°
            try:
                await self.page.wait_for_selector('table.bd_tb, .bd_tb, .board_list', timeout=self.wait_timeout)
            except:
                logger.warning("âš ï¸ ê²Œì‹œê¸€ ëª©ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
            
            # í˜ì´ì§€ ì œëª© í™•ì¸
            page_title = await self.page.title()
            logger.info(f"ğŸ“„ í˜ì´ì§€ ì œëª©: {page_title}")
            
            # ê²Œì‹œê¸€ ëª©ë¡ ì¶”ì¶œ
            posts = await self.extract_post_list()
            
            # ìƒìœ„ ê²Œì‹œê¸€ë“¤ì˜ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ (ì²˜ìŒ 5ê°œ)
            detailed_posts = []
            for i, post in enumerate(posts[:5]):
                try:
                    logger.info(f"ğŸ“ ê²Œì‹œê¸€ {i+1}/5 ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì¤‘: {post.get('title', 'N/A')}")
                    detailed_post = await self.get_post_details(post)
                    if detailed_post:
                        detailed_posts.append(detailed_post)
                    
                    # ìš”ì²­ ê°„ ëŒ€ê¸°
                    await asyncio.sleep(2)
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ ê²Œì‹œê¸€ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                    detailed_posts.append(post)
            
            # ë©”íŠ¸ë¦­ ê³„ì‚°
            top_posts = self.calculate_metrics(detailed_posts)
            
            result = {
                'scraped_at': datetime.now(self.kst),
                'total_count': len(posts),
                'all_posts': posts,
                'detailed_posts': detailed_posts,
                'top_posts': top_posts,
                'page_title': page_title,
                'source_url': self.politics_url
            }
            
            logger.info(f"âœ… ìŠ¤í¬ë˜í•‘ ì™„ë£Œ! ì´ {len(posts)}ê°œ ê²Œì‹œê¸€, {len(detailed_posts)}ê°œ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘")
            return result
            
        except Exception as e:
            logger.error(f"ğŸ’¥ ê²Œì‹œíŒ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
            return {
                'error': str(e),
                'scraped_at': datetime.now(self.kst),
                'source_url': self.politics_url
            }
    
    async def extract_post_list(self) -> List[Dict]:
        """ê²Œì‹œê¸€ ëª©ë¡ ì¶”ì¶œ"""
        try:
            posts = []
            
            # FMì½”ë¦¬ì•„ ê²Œì‹œê¸€ ëª©ë¡ ì…€ë ‰í„°ë“¤
            post_selectors = [
                'table.bd_tb tbody tr',
                '.bd_tb tbody tr',
                '.board_list tbody tr',
                'tbody tr'
            ]
            
            post_elements = []
            for selector in post_selectors:
                try:
                    elements = await self.page.query_selector_all(selector)
                    if elements:
                        # ì‹¤ì œ ê²Œì‹œê¸€ì¸ì§€ í™•ì¸ (ì œëª© ë§í¬ê°€ ìˆëŠ” ê²ƒë§Œ)
                        valid_elements = []
                        for element in elements:
                            title_link = await element.query_selector('td.title a, .title a, a[href*="/"]')
                            if title_link:
                                valid_elements.append(element)
                        
                        if valid_elements:
                            post_elements = valid_elements
                            logger.info(f"âœ… {len(valid_elements)}ê°œ ê²Œì‹œê¸€ ìš”ì†Œ ë°œê²¬ (ì…€ë ‰í„°: {selector})")
                            break
                except Exception as e:
                    logger.warning(f"âš ï¸ ì…€ë ‰í„° {selector} ì‹¤íŒ¨: {e}")
                    continue
            
            if not post_elements:
                logger.warning("âš ï¸ ê²Œì‹œê¸€ ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return []
            
            # ê° ê²Œì‹œê¸€ ìš”ì†Œì—ì„œ ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
            for element in post_elements:
                try:
                    post_info = await self.extract_post_info(element)
                    if post_info and post_info.get('title'):
                        posts.append(post_info)
                except Exception as e:
                    logger.warning(f"âš ï¸ ê²Œì‹œê¸€ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                    continue
            
            logger.info(f"âœ… ì´ {len(posts)}ê°œ ê²Œì‹œê¸€ ê¸°ë³¸ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ")
            return posts
            
        except Exception as e:
            logger.error(f"ğŸ’¥ ê²Œì‹œê¸€ ëª©ë¡ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    async def extract_post_info(self, post_element) -> Optional[Dict]:
        """ê°œë³„ ê²Œì‹œê¸€ ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ"""
        try:
            post_info = {}
            
            # ì œëª©ê³¼ ë§í¬ ì¶”ì¶œ
            title_selectors = [
                'td.title a',
                '.title a',
                'a[href*="/"]'
            ]
            
            title_found = False
            for selector in title_selectors:
                try:
                    title_element = await post_element.query_selector(selector)
                    if title_element:
                        title_text = await title_element.inner_text()
                        href = await title_element.get_attribute('href')
                        
                        if title_text and href:
                            post_info['title'] = title_text.strip()
                            
                            # ì ˆëŒ€ URLë¡œ ë³€í™˜
                            if href.startswith('/'):
                                post_info['url'] = urljoin(self.base_url, href)
                            else:
                                post_info['url'] = href
                            
                            # ê²Œì‹œê¸€ ID ì¶”ì¶œ
                            post_info['post_id'] = self.parse_post_id_from_url(post_info['url'])
                            title_found = True
                            break
                except:
                    continue
            
            if not title_found:
                return None
            
            # ì‘ì„±ì ì¶”ì¶œ
            author_selectors = [
                'td.author .member_plate',
                '.author .member_plate',
                'td.author a',
                '.author a',
                'td.author',
                '.author'
            ]
            
            for selector in author_selectors:
                try:
                    author_element = await post_element.query_selector(selector)
                    if author_element:
                        author_text = await author_element.inner_text()
                        if author_text and author_text.strip():
                            post_info['author'] = author_text.strip()
                            break
                except:
                    continue
            
            # ì‘ì„± ì‹œê°„ ì¶”ì¶œ
            date_selectors = [
                'td.time',
                '.time',
                'td.date',
                '.date'
            ]
            
            for selector in date_selectors:
                try:
                    date_element = await post_element.query_selector(selector)
                    if date_element:
                        date_text = await date_element.inner_text()
                        if date_text and date_text.strip():
                            post_info['date'] = date_text.strip()
                            break
                except:
                    continue
            
            # ì¡°íšŒìˆ˜ ì¶”ì¶œ
            view_selectors = [
                'td.m_no:has-text("ì¡°íšŒ")',
                '.view_count',
                'td:has-text("ì¡°íšŒ")'
            ]
            
            view_count = 0
            for selector in view_selectors:
                try:
                    view_element = await post_element.query_selector(selector)
                    if view_element:
                        view_text = await view_element.inner_text()
                        numbers = re.findall(r'\d+', view_text)
                        if numbers:
                            view_count = int(numbers[-1])
                            break
                except:
                    continue
            
            post_info['view_count'] = view_count
            
            # ì¶”ì²œìˆ˜ ì¶”ì¶œ
            like_selectors = [
                'td.m_no:has-text("ì¶”ì²œ")',
                '.like_count',
                'td:has-text("ì¶”ì²œ")'
            ]
            
            like_count = 0
            for selector in like_selectors:
                try:
                    like_element = await post_element.query_selector(selector)
                    if like_element:
                        like_text = await like_element.inner_text()
                        numbers = re.findall(r'\d+', like_text)
                        if numbers:
                            like_count = int(numbers[-1])
                            break
                except:
                    continue
            
            post_info['like_count'] = like_count
            
            # ëŒ“ê¸€ìˆ˜ ì¶”ì¶œ
            comment_selectors = [
                'td.m_no:has-text("ëŒ“ê¸€")',
                '.comment_count',
                'td:has-text("ëŒ“ê¸€")'
            ]
            
            comment_count = 0
            for selector in comment_selectors:
                try:
                    comment_element = await post_element.query_selector(selector)
                    if comment_element:
                        comment_text = await comment_element.inner_text()
                        numbers = re.findall(r'\d+', comment_text)
                        if numbers:
                            comment_count = int(numbers[-1])
                            break
                except:
                    continue
            
            post_info['comment_count'] = comment_count
            
            return post_info
            
        except Exception as e:
            logger.error(f"ğŸ’¥ ê²Œì‹œê¸€ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    async def get_post_details(self, post_info: Dict) -> Optional[Dict]:
        """ê²Œì‹œê¸€ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ (ì‹¤í—˜ìš© ìŠ¤í¬ë˜í¼ í™œìš©)"""
        try:
            from ..fmkorea_experiment_scraper import FMKoreaExperimentScraper
            
            # ì‹¤í—˜ìš© ìŠ¤í¬ë˜í¼ë¡œ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
            async with FMKoreaExperimentScraper() as experiment_scraper:
                detailed_data = await experiment_scraper.scrape_post_experiment(post_info['url'])
                
                if detailed_data and not detailed_data.get('error'):
                    # ê¸°ë³¸ ì •ë³´ì™€ ìƒì„¸ ì •ë³´ ë³‘í•©
                    post_info.update({
                        'experiment_data': detailed_data,
                        'content': detailed_data.get('content', []),
                        'comments': detailed_data.get('comments', []),
                        'metadata': detailed_data.get('metadata', {})
                    })
                    
                    # ë©”íƒ€ë°ì´í„°ì—ì„œ ì¶”ê°€ ì •ë³´ ì—…ë°ì´íŠ¸
                    metadata = detailed_data.get('metadata', {})
                    if metadata.get('view_count'):
                        post_info['view_count'] = metadata['view_count']
                    if metadata.get('like_count'):
                        post_info['like_count'] = metadata['like_count']
                    if metadata.get('comment_count'):
                        post_info['comment_count'] = metadata['comment_count']
                
                return post_info
                
        except Exception as e:
            logger.warning(f"âš ï¸ ê²Œì‹œê¸€ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return post_info
    
    def calculate_metrics(self, posts: List[Dict]) -> Dict:
        """ê²Œì‹œê¸€ ë©”íŠ¸ë¦­ ê³„ì‚° ë° ìƒìœ„ ê²Œì‹œê¸€ ì„ ì •"""
        try:
            # ë©”íŠ¸ë¦­ ê³„ì‚°
            for post in posts:
                view_count = post.get('view_count', 0)
                like_count = post.get('like_count', 0)
                comment_count = post.get('comment_count', 0)
                
                # ì•ˆì „í•œ ë‚˜ëˆ—ì…ˆì„ ìœ„í•œ ì²˜ë¦¬
                metrics = {}
                
                # ì¡°íšŒìˆ˜ ëŒ€ë¹„ ì¶”ì²œìˆ˜ ë¹„ìœ¨
                if view_count > 0:
                    metrics['likes_per_view'] = like_count / view_count
                else:
                    metrics['likes_per_view'] = 0
                
                # ì¡°íšŒìˆ˜ ëŒ€ë¹„ ëŒ“ê¸€ìˆ˜ ë¹„ìœ¨
                if view_count > 0:
                    metrics['comments_per_view'] = comment_count / view_count
                else:
                    metrics['comments_per_view'] = 0
                
                # ì‹œê°„ë‹¹ ì¡°íšŒìˆ˜ (ì„ì‹œë¡œ ì¡°íšŒìˆ˜ ì‚¬ìš©)
                metrics['views_per_exposure_hour'] = view_count
                
                post['metrics'] = metrics
            
            # ê° ë©”íŠ¸ë¦­ë³„ ìƒìœ„ 3ê°œ ì„ ì •
            top_posts = {}
            
            # ì¶”ì²œìˆ˜ ëŒ€ë¹„ ì¡°íšŒìˆ˜
            sorted_by_likes = sorted(posts, key=lambda x: x.get('metrics', {}).get('likes_per_view', 0), reverse=True)
            top_posts['likes_per_view'] = sorted_by_likes[:3]
            
            # ëŒ“ê¸€ìˆ˜ ëŒ€ë¹„ ì¡°íšŒìˆ˜
            sorted_by_comments = sorted(posts, key=lambda x: x.get('metrics', {}).get('comments_per_view', 0), reverse=True)
            top_posts['comments_per_view'] = sorted_by_comments[:3]
            
            # ì‹œê°„ë‹¹ ì¡°íšŒìˆ˜
            sorted_by_views = sorted(posts, key=lambda x: x.get('metrics', {}).get('views_per_exposure_hour', 0), reverse=True)
            top_posts['views_per_exposure_hour'] = sorted_by_views[:3]
            
            return top_posts
            
        except Exception as e:
            logger.error(f"ğŸ’¥ ë©”íŠ¸ë¦­ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {}
    
    def parse_post_id_from_url(self, url: str) -> str:
        """URLì—ì„œ ê²Œì‹œê¸€ ID ì¶”ì¶œ"""
        try:
            if '/index.php' in url:
                from urllib.parse import parse_qs, urlparse
                parsed = urlparse(url)
                params = parse_qs(parsed.query)
                if 'document_srl' in params:
                    return params['document_srl'][0]
            else:
                match = re.search(r'/(\d+)/?$', url)
                if match:
                    return match.group(1)
            
            numbers = re.findall(r'\d+', url)
            if numbers:
                return max(numbers, key=len)
            
            return 'unknown'
            
        except Exception as e:
            logger.error(f"ğŸ’¥ ê²Œì‹œê¸€ ID ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return 'unknown' 