# ëª©ì°¨
# 1. ëª¨ë“ˆ ì„í¬íŠ¸ ë° ì„¤ì • (1-25)
# 2. ìŠ¤í‚¤ë§ˆ ì •ì˜ (26-150)
# 3. ì…€ë ‰í„° ì •ì˜ (151-220)
# 4. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ (221-320)
# 5. ë¸Œë¼ìš°ì € ê´€ë¦¬ (321-380)
# 6. ë°ì´í„° ì¶”ì¶œ í•¨ìˆ˜ (381-580)
# 7. ê²Œì‹œê¸€ ëª©ë¡ ê´€ë ¨ í•¨ìˆ˜ (581-680)
# 8. ë©”ì¸ ìŠ¤í¬ë˜í¼ í´ë˜ìŠ¤ (681-800)
# 9. MongoDB ì—°ë™ (801-850)
# 10. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ë“¤ (851-950)
# 11. API ì „ì†¡ í•¨ìˆ˜ (951-980)

import asyncio
import json
import re
import time
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Any
from urllib.parse import urljoin, urlparse
import requests

import pytz
from playwright.async_api import Browser, Page, ElementHandle, async_playwright
from pymongo import MongoClient
from pymongo.collection import Collection
import jsonschema

# JSON ìŠ¤í‚¤ë§ˆ ì •ì˜ (ê¸°ì¡´ ìˆ˜ì •ëœ ìŠ¤í‚¤ë§ˆ ìœ ì§€)
POST_SCHEMA = {
    "type": "object",
    "properties": {
        "post_id": {"type": "string"},
        "post_url": {"type": "string"},
        "metadata": {
            "type": "object",
            "properties": {
                "title": {"type": "string"},
                "author": {"type": "string"},
                "date": {"type": "string"},
                "view_count": {"type": "integer"},
                "up_count": {"type": "integer"},
                "down_count": {"type": "integer"},
                "comment_count": {"type": "integer"},
                "category": {"type": "string"}
            },
            "required": ["title", "author", "date", "view_count", "up_count", "down_count", "comment_count"]
        },
        "content": {
            "type": "array",
            "items": {
                "type": "object",
                "properties": {
                    "type": {"enum": ["text", "image", "video"]},
                    "order": {"type": "integer"},
                    "data": {
                        "type": "object",
                        "properties": {
                            "text": {"type": "string"},
                            "src": {"type": "string"},
                            "href": {"type": "string"},
                            "alt": {"type": "string"},
                            "width": {"type": "string"},
                            "height": {"type": "string"},
                            "autoplay": {"type": "boolean"},
                            "muted": {"type": "boolean"}
                        }
                    }
                },
                "required": ["type", "order", "data"]
            }
        },
        "comments": {
            "type": "array",
            "items": {
                "type": "object",
                "properties": {
                    "comment_id": {"type": "string"},
                    "content": {"type": "string"},
                    "author": {"type": "string"},
                    "date": {"type": "string"},
                    "media": {
                        "type": "array",
                        "items": {
                            "type": "object",
                            "properties": {
                                "type": {"enum": ["image", "video"]},
                                "order": {"type": "integer"},
                                "data": {
                                    "type": "object",
                                    "properties": {
                                        "src": {"type": "string"},
                                        "href": {"type": "string"},
                                        "alt": {"type": "string"},
                                        "width": {"type": "string"},
                                        "height": {"type": "string"},
                                        "autoplay": {"type": "boolean"},
                                        "muted": {"type": "boolean"}
                                    }
                                }
                            },
                            "required": ["type", "order", "data"]
                        }
                    },
                    "level": {"type": "integer"},
                    "is_reply": {"type": "boolean"},
                    "parent_comment_id": {"type": "string"},
                    "up_count": {"type": "integer"},
                    "down_count": {"type": "integer"}
                },
                "required": ["comment_id", "content", "author", "date", "media", "level", "is_reply", "parent_comment_id", "up_count", "down_count"]
            }
        },
        "scraped_at": {"type": "string"}
    },
    "required": ["post_id", "post_url", "metadata", "content", "comments", "scraped_at"]
}

# ê²Œì‹œê¸€ ëª©ë¡ ìŠ¤í‚¤ë§ˆ
POST_LIST_SCHEMA = {
    "type": "array",
    "items": {
        "type": "object",
        "properties": {
            "post_id": {"type": "string"},
            "url": {"type": "string"},
            "title": {"type": "string"},
            "author": {"type": "string"},
            "date": {"type": "string"},
            "up_count": {"type": "integer"},
            "down_count": {"type": "integer"},
            "comment_count": {"type": "integer"},
            "view_count": {"type": "integer"}
        },
        "required": ["post_id", "url", "title", "date", "up_count", "down_count", "comment_count", "view_count"]
    }
}

# FMKorea ì…€ë ‰í„° ì •ì˜ (ìˆ˜ì •ë¨)
FMKOREA_SELECTORS = {
    "metadata": {
        # ê²Œì‹œê¸€ ì œëª©
        "title": "h1.np_18px .np_18px_span",
        # ì‘ì„±ì
        "author": ".btm_area .side a.member_plate",
        # ì‘ì„±ì¼
        "date": ".top_area .date",
        # ì¡°íšŒìˆ˜ - ë” êµ¬ì²´ì ì¸ ì…€ë ‰í„° ì‚¬ìš©
        "view_count": ".btm_area .side.fr span:nth-child(1) b",
        # ì¶”ì²œìˆ˜ - ë” êµ¬ì²´ì ì¸ ì…€ë ‰í„° ì‚¬ìš©
        "up_count": ".btm_area .side.fr span:nth-child(2) b",
        # ë¹„ì¶”ì²œìˆ˜ (FMKoreaëŠ” ë¹„ì¶”ì²œì´ ì—†ìœ¼ë¯€ë¡œ 0ìœ¼ë¡œ ì„¤ì •)
        "down_count": None,
        # ëŒ“ê¸€ìˆ˜ - ë” êµ¬ì²´ì ì¸ ì…€ë ‰í„° ì‚¬ìš©
        "comment_count": ".btm_area .side.fr span:nth-child(3) b",
        # ì¹´í…Œê³ ë¦¬
        "category": ".cate a"
    },
    "content": {
        # ë³¸ë¬¸ ì»¨í…Œì´ë„ˆ (ì‹¤ì œ í™•ì¸ëœ ì…€ë ‰í„°)
        "container": ".xe_content",
        # í…ìŠ¤íŠ¸ ìš”ì†Œ
        "text": "p:not(:has(img)):not(:has(a.highslide))",
        # ì´ë¯¸ì§€ ìš”ì†Œ
        "image": "img, a.highslide img",
        # ë¹„ë””ì˜¤ ìš”ì†Œ (FMKoreaëŠ” ì£¼ë¡œ ì´ë¯¸ì§€ ìœ„ì£¼)
        "video": "video"
    },
    "comments": {
        # ëŒ“ê¸€ ì»¨í…Œì´ë„ˆ (ì‹¤ì œ í™•ì¸ëœ ì…€ë ‰í„°)
        "container": "li[class*='comment']",
        # ëŒ“ê¸€ ID (li ìš”ì†Œì˜ id ì†ì„±)
        "comment_id": "[id^='comment_']",
        # ëŒ“ê¸€ ë‚´ìš© (ë” ì •í™•í•œ ì…€ë ‰í„° í•„ìš”)
        "content": ".xe_content, .comment_content",
        # ëŒ“ê¸€ ì‘ì„±ì
        "author": "a.member_plate",
        # ëŒ“ê¸€ ì‘ì„±ì¼
        "date": ".date",
        # ëŒ“ê¸€ ë¯¸ë””ì–´ (ì´ë¯¸ì§€/ë¹„ë””ì˜¤)
        "media": "img, video",
        # ëŒ“ê¸€ ë ˆë²¨ (FMKoreaëŠ” ëŒ€ëŒ“ê¸€ êµ¬ì¡°ê°€ ë‹¨ìˆœ)
        "level": None,
        # ëŒ€ëŒ“ê¸€ ì—¬ë¶€
        "is_reply": None,
        # ë¶€ëª¨ ëŒ“ê¸€ ID
        "parent_comment_id": None,
        # ëŒ“ê¸€ ì¶”ì²œìˆ˜
        "up_count": ".vote .voted_count",
        # ëŒ“ê¸€ ë¹„ì¶”ì²œìˆ˜
        "down_count": ".vote .blamed_count"
    },
    "post_list": {
        # ê²Œì‹œê¸€ ëª©ë¡ ì»¨í…Œì´ë„ˆ
        "container": "table.bd_lst tbody tr",
        # ì œëª© ë° URL
        "title_link": "td.title a",
        # ì‘ì„±ì
        "author": "td.author a.member_plate",
        # ì‘ì„±ì¼
        "date": "td.time",
        # ì¡°íšŒìˆ˜
        "view_count": "td.m_no:nth-last-child(2)",
        # ì¶”ì²œìˆ˜
        "up_count": "td.m_no:last-child",
        # ëŒ“ê¸€ìˆ˜ (ì œëª©ì—ì„œ ì¶”ì¶œ)
        "comment_count_in_title": "td.title .comment_count"
    },
    "pagination": {
        # ì´ì „ í˜ì´ì§€
        "prev_page": ".pagination a[title='ì´ì „ í˜ì´ì§€']",
        # ë‹¤ìŒ í˜ì´ì§€
        "next_page": ".pagination a[title='ë‹¤ìŒ í˜ì´ì§€']"
    }
}

# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
def parse_post_id(url: str) -> str:
    """URLì—ì„œ ê²Œì‹œê¸€ ID ì¶”ì¶œ"""
    # FMKorea URL íŒ¨í„´: https://www.fmkorea.com/8475910237
    parsed = urlparse(url)
    post_id = parsed.path.strip('/')
    return post_id

def validate_data(data: Dict, schema: Dict) -> bool:
    """JSON Schemaë¡œ ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬"""
    try:
        jsonschema.validate(data, schema)
        return True
    except jsonschema.ValidationError as e:
        print(f"ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨: {e}")
        return False

def clean_text(text: str) -> str:
    """í…ìŠ¤íŠ¸ ì •ë¦¬ (ê³µë°±, íŠ¹ìˆ˜ë¬¸ì ì œê±°)"""
    if not text:
        return ""
    return re.sub(r'\s+', ' ', text.strip())

def parse_number(text: str) -> int:
    """ë¬¸ìì—´ì—ì„œ ìˆ«ì ì¶”ì¶œ"""
    if not text:
        return 0
    # ìˆ«ìë§Œ ì¶”ì¶œ
    numbers = re.findall(r'\d+', text.replace(',', ''))
    return int(numbers[0]) if numbers else 0

def parse_date(date_str: str) -> str:
    """ë‚ ì§œ ë¬¸ìì—´ íŒŒì‹± ë° ISO í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    if not date_str:
        return datetime.now(pytz.timezone('Asia/Seoul')).isoformat()
    
    # FMKorea ë‚ ì§œ í˜•ì‹: "2025.06.04 15:26" ë˜ëŠ” "15:27"
    date_str = clean_text(date_str)
    
    try:
        if ':' in date_str and '.' not in date_str:
            # ì‹œê°„ë§Œ ìˆëŠ” ê²½ìš° (ì˜¤ëŠ˜ ë‚ ì§œë¡œ ê°€ì •)
            today = datetime.now(pytz.timezone('Asia/Seoul')).date()
            time_str = date_str
            datetime_str = f"{today.strftime('%Y.%m.%d')} {time_str}"
        else:
            datetime_str = date_str
            
        # ë‚ ì§œ íŒŒì‹±
        dt = datetime.strptime(datetime_str, '%Y.%m.%d %H:%M')
        dt = pytz.timezone('Asia/Seoul').localize(dt)
        return dt.isoformat()
    except ValueError:
        # íŒŒì‹± ì‹¤íŒ¨ ì‹œ í˜„ì¬ ì‹œê°„ ë°˜í™˜
        return datetime.now(pytz.timezone('Asia/Seoul')).isoformat()

def is_within_time_range(date_str: str, start_hour: int, end_hour: int, timezone: str = "Asia/Seoul") -> bool:
    """ê²Œì‹œê¸€ ì‘ì„± ì‹œê°„ì´ íŠ¹ì • ì‹œê°„ëŒ€ì¸ì§€ í™•ì¸"""
    try:
        if not date_str:
            return False
            
        # ISO í˜•ì‹ ë‚ ì§œ íŒŒì‹±
        if 'T' in date_str:
            dt = datetime.fromisoformat(date_str.replace('Z', '+00:00'))
        else:
            # ë‹¤ë¥¸ í˜•ì‹ ì‹œë„
            dt = datetime.strptime(date_str, '%Y.%m.%d %H:%M')
            dt = pytz.timezone(timezone).localize(dt)
            
        # ì‹œê°„ëŒ€ ë³€í™˜
        if dt.tzinfo is None:
            dt = pytz.timezone(timezone).localize(dt)
        else:
            dt = dt.astimezone(pytz.timezone(timezone))
            
        hour = dt.hour
        return start_hour <= hour < end_hour
    except Exception as e:
        print(f"ì‹œê°„ ë²”ìœ„ í™•ì¸ ì˜¤ë¥˜: {e}")
        return False

def select_top_posts(posts: List[Dict]) -> List[Dict]:
    """ì¶”ì²œìˆ˜ ìƒìœ„ 3ê°œ, ëŒ“ê¸€ìˆ˜ ìƒìœ„ 3ê°œ(ì¶”ì²œìˆ˜ ì œì™¸), ì¡°íšŒìˆ˜ ìƒìœ„ 3ê°œ(ì¶”ì²œìˆ˜/ëŒ“ê¸€ìˆ˜ ì œì™¸) ì„ ë³„"""
    if not posts:
        return []
    
    # ì¶”ì²œìˆ˜ ìƒìœ„ 3ê°œ
    top_up_posts = sorted(posts, key=lambda x: x.get('up_count', 0), reverse=True)[:3]
    selected_posts = top_up_posts.copy()
    
    # ëŒ“ê¸€ìˆ˜ ìƒìœ„ 3ê°œ (ì¶”ì²œìˆ˜ ìƒìœ„ ì œì™¸)
    remaining_posts = [post for post in posts if post not in selected_posts]
    top_comment_posts = sorted(remaining_posts, key=lambda x: x.get('comment_count', 0), reverse=True)[:3]
    selected_posts.extend(top_comment_posts)
    
    # ì¡°íšŒìˆ˜ ìƒìœ„ 3ê°œ (ì¶”ì²œìˆ˜/ëŒ“ê¸€ìˆ˜ ìƒìœ„ ì œì™¸)
    remaining_posts = [post for post in posts if post not in selected_posts]
    top_view_posts = sorted(remaining_posts, key=lambda x: x.get('view_count', 0), reverse=True)[:3]
    selected_posts.extend(top_view_posts)
    
    return selected_posts

# ë¸Œë¼ìš°ì € ê´€ë¦¬ í•¨ìˆ˜ë“¤
async def setup_browser(config: Optional[Dict] = None) -> Tuple[Browser, Page]:
    """ë¸Œë¼ìš°ì € ì´ˆê¸°í™”"""
    if config is None:
        config = {}
        
    playwright = await async_playwright().start()
    browser = await playwright.chromium.launch(
        headless=config.get('headless', True),
        slow_mo=config.get('slow_mo', 0)
    )
    page = await browser.new_page()
    
    # User-Agent ì„¤ì •
    await page.set_extra_http_headers({
        'User-Agent': config.get('user_agent', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')
    })
    
    return browser, page

async def close_browser(browser: Browser):
    """ë¸Œë¼ìš°ì € ì¢…ë£Œ"""
    await browser.close()

async def navigate_to_page(page: Page, url: str, config: Optional[Dict] = None) -> bool:
    """í˜ì´ì§€ ì´ë™"""
    if config is None:
        config = {}
        
    try:
        await page.goto(url, 
                       wait_until=config.get('wait_until', 'networkidle'), 
                       timeout=config.get('timeout', 30000))
        
        # ì¶”ê°€ ëŒ€ê¸° ì‹œê°„
        if config.get('wait_time', 0) > 0:
            await asyncio.sleep(config['wait_time'])
            
        return True
    except Exception as e:
        print(f"í˜ì´ì§€ ì´ë™ ì‹¤íŒ¨: {e}")
        return False

# ë°ì´í„° ì¶”ì¶œ í•¨ìˆ˜ë“¤
async def extract_text_content(element: ElementHandle) -> str:
    """ìš”ì†Œì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    try:
        text = await element.text_content()
        return clean_text(text) if text else ""
    except Exception:
        return ""

async def extract_attribute(element: ElementHandle, attr: str) -> str:
    """ìš”ì†Œì—ì„œ ì†ì„±ê°’ ì¶”ì¶œ"""
    try:
        value = await element.get_attribute(attr)
        return value if value else ""
    except Exception:
        return ""

# ê²Œì‹œê¸€ ëª©ë¡ ê´€ë ¨ í•¨ìˆ˜ë“¤
async def extract_post_list(page: Page, selectors: Dict) -> List[Dict]:
    """ê²Œì‹œê¸€ ëª©ë¡ ì¶”ì¶œ"""
    posts = []
    post_selectors = selectors.get("post_list", {})
    
    try:
        # ê²Œì‹œê¸€ ëª©ë¡ ì»¨í…Œì´ë„ˆë“¤ ì°¾ê¸°
        post_elements = await page.query_selector_all(post_selectors.get("container", ""))
        
        for element in post_elements:
            post_data = {}
            
            # ì œëª© ë° URL ì¶”ì¶œ
            title_link_element = await element.query_selector(post_selectors.get("title_link", ""))
            if title_link_element:
                title = await extract_text_content(title_link_element)
                url = await extract_attribute(title_link_element, "href")
                
                # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
                if url and url.startswith('/'):
                    url = "https://www.fmkorea.com" + url
                    
                post_data["title"] = clean_text(title)
                post_data["url"] = url
                post_data["post_id"] = parse_post_id(url) if url else ""
            else:
                continue  # ì œëª©ì´ ì—†ìœ¼ë©´ ìŠ¤í‚µ
                
            # ì‘ì„±ì ì¶”ì¶œ
            author_element = await element.query_selector(post_selectors.get("author", ""))
            if author_element:
                post_data["author"] = await extract_text_content(author_element)
            else:
                post_data["author"] = ""
                
            # ì‘ì„±ì¼ ì¶”ì¶œ
            date_element = await element.query_selector(post_selectors.get("date", ""))
            if date_element:
                date_text = await extract_text_content(date_element)
                post_data["date"] = parse_date(date_text)
            else:
                post_data["date"] = datetime.now(pytz.timezone('Asia/Seoul')).isoformat()
                
            # ì¡°íšŒìˆ˜ ì¶”ì¶œ
            view_element = await element.query_selector(post_selectors.get("view_count", ""))
            if view_element:
                view_text = await extract_text_content(view_element)
                post_data["view_count"] = parse_number(view_text)
            else:
                post_data["view_count"] = 0
                
            # ì¶”ì²œìˆ˜ ì¶”ì¶œ
            up_element = await element.query_selector(post_selectors.get("up_count", ""))
            if up_element:
                up_text = await extract_text_content(up_element)
                post_data["up_count"] = parse_number(up_text)
            else:
                post_data["up_count"] = 0
                
            # ë¹„ì¶”ì²œìˆ˜ (FMKoreaëŠ” ì—†ìŒ)
            post_data["down_count"] = 0
            
            # ëŒ“ê¸€ìˆ˜ ì¶”ì¶œ (ì œëª©ì—ì„œ ì¶”ì¶œí•˜ê±°ë‚˜ ë³„ë„ ìš”ì†Œì—ì„œ)
            comment_element = await element.query_selector(post_selectors.get("comment_count_in_title", ""))
            if comment_element:
                comment_text = await extract_text_content(comment_element)
                post_data["comment_count"] = parse_number(comment_text)
            else:
                # ì œëª©ì—ì„œ ëŒ“ê¸€ìˆ˜ ì¶”ì¶œ ì‹œë„
                title_text = post_data.get("title", "")
                comment_match = re.search(r'\[(\d+)\]', title_text)
                post_data["comment_count"] = int(comment_match.group(1)) if comment_match else 0
                
            posts.append(post_data)
            
    except Exception as e:
        print(f"ê²Œì‹œê¸€ ëª©ë¡ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        
    return posts

async def scrape_and_select_page_posts(community: str, page_url: str, config: Optional[Dict] = None) -> List[Dict]:
    """íŠ¹ì • í˜ì´ì§€ì˜ ê²Œì‹œê¸€ ëª©ë¡ ìŠ¤í¬ë˜í•‘ í›„ ì„ ë³„"""
    if config is None:
        config = {}
        
    selectors = FMKOREA_SELECTORS  # FMKorea ì „ìš©
    browser, page = await setup_browser(config)
    
    try:
        success = await navigate_to_page(page, page_url, config)
        if not success:
            return []
            
        posts = await extract_post_list(page, selectors)
        return select_top_posts(list(reversed(posts)))  # ì—­ìˆœìœ¼ë¡œ ì„ ë³„ (ì˜¤ë˜ëœ ê²Œì‹œê¸€ ë¨¼ì €)
    finally:
        await close_browser(browser)

class FMKoreaScraper:
    """FMKorea ìŠ¤í¬ë˜í¼ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.selectors = FMKOREA_SELECTORS
        self.schema = POST_SCHEMA
        
    async def extract_metadata(self, page: Page) -> Dict:
        """ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (ê°œì„ ëœ ë²„ì „)"""
        metadata = {}
        selectors = self.selectors["metadata"]
        
        try:
            # ì œëª© ì¶”ì¶œ
            title_element = await page.query_selector(selectors["title"])
            if title_element:
                metadata["title"] = await extract_text_content(title_element)
            else:
                metadata["title"] = ""
                
            # ì‘ì„±ì ì¶”ì¶œ
            author_element = await page.query_selector(selectors["author"])
            if author_element:
                metadata["author"] = await extract_text_content(author_element)
            else:
                metadata["author"] = ""
                
            # ì‘ì„±ì¼ ì¶”ì¶œ
            date_element = await page.query_selector(selectors["date"])
            if date_element:
                date_text = await extract_text_content(date_element)
                metadata["date"] = parse_date(date_text)
            else:
                metadata["date"] = datetime.now(pytz.timezone('Asia/Seoul')).isoformat()
                
            # ì¡°íšŒìˆ˜, ì¶”ì²œìˆ˜, ëŒ“ê¸€ìˆ˜ë¥¼ í…ìŠ¤íŠ¸ë¡œ ì°¾ê¸°
            side_fr_element = await page.query_selector(".btm_area .side.fr")
            if side_fr_element:
                side_text = await extract_text_content(side_fr_element)
                
                # ì¡°íšŒìˆ˜ ì¶”ì¶œ (ì˜ˆ: "ì¡°íšŒ ìˆ˜ 294")
                view_match = re.search(r'ì¡°íšŒ\s*ìˆ˜\s*(\d+)', side_text)
                metadata["view_count"] = int(view_match.group(1)) if view_match else 0
                
                # ì¶”ì²œìˆ˜ ì¶”ì¶œ (ì˜ˆ: "ì¶”ì²œ ìˆ˜ 4")
                up_match = re.search(r'ì¶”ì²œ\s*ìˆ˜\s*(\d+)', side_text)
                metadata["up_count"] = int(up_match.group(1)) if up_match else 0
                
                # ëŒ“ê¸€ìˆ˜ ì¶”ì¶œ (ì˜ˆ: "ëŒ“ê¸€ 2")
                comment_match = re.search(r'ëŒ“ê¸€\s*(\d+)', side_text)
                metadata["comment_count"] = int(comment_match.group(1)) if comment_match else 0
            else:
                metadata["view_count"] = 0
                metadata["up_count"] = 0
                metadata["comment_count"] = 0
                
            # ë¹„ì¶”ì²œìˆ˜ (FMKoreaëŠ” ì—†ìŒ)
            metadata["down_count"] = 0
                
            # ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
            category_element = await page.query_selector(selectors["category"])
            if category_element:
                metadata["category"] = await extract_text_content(category_element)
            else:
                metadata["category"] = ""
                
        except Exception as e:
            print(f"ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            # ê¸°ë³¸ê°’ìœ¼ë¡œ ì±„ìš°ê¸°
            metadata = {
                "title": "",
                "author": "",
                "date": datetime.now(pytz.timezone('Asia/Seoul')).isoformat(),
                "view_count": 0,
                "up_count": 0,
                "down_count": 0,
                "comment_count": 0,
                "category": ""
            }
            
        return metadata
        
    async def extract_content(self, page: Page) -> List[Dict]:
        """ë³¸ë¬¸ ì½˜í…ì¸  ì¶”ì¶œ (ê°œì„ ëœ ë²„ì „)"""
        content = []
        selectors = self.selectors["content"]
        order = 0
        
        try:
            # ë³¸ë¬¸ ì»¨í…Œì´ë„ˆ ì°¾ê¸°
            container = await page.query_selector(selectors["container"])
            if not container:
                print("âŒ ë³¸ë¬¸ ì»¨í…Œì´ë„ˆë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return content
                
            print("ğŸ” ë³¸ë¬¸ ì½˜í…ì¸  ì¶”ì¶œ ì¤‘...")
            
            # ëª¨ë“  í•˜ìœ„ ìš”ì†Œë¥¼ ìˆœì„œëŒ€ë¡œ ì²˜ë¦¬
            all_elements = await container.query_selector_all("*")
            
            for element in all_elements:
                tag_name = await element.evaluate("el => el.tagName.toLowerCase()")
                
                # í…ìŠ¤íŠ¸ ìš”ì†Œ ì²˜ë¦¬ (p, div íƒœê·¸)
                if tag_name in ['p', 'div']:
                    # ì´ë¯¸ì§€ë‚˜ ë§í¬ë§Œ í¬í•¨ëœ ìš”ì†ŒëŠ” ì œì™¸
                    text = await extract_text_content(element)
                    if text and text.strip():
                        # í•˜ìœ„ì— ì´ë¯¸ì§€ê°€ ìˆëŠ”ì§€ í™•ì¸
                        img_elements = await element.query_selector_all("img")
                        if len(img_elements) == 0:  # ì´ë¯¸ì§€ê°€ ì—†ëŠ” í…ìŠ¤íŠ¸ë§Œ
                            content.append({
                                "type": "text",
                                "order": order,
                                "data": {"text": text.strip()}
                            })
                            order += 1
                            print(f"  ğŸ“ í…ìŠ¤íŠ¸ ì¶”ê°€: {text[:50]}...")
                
                # ì´ë¯¸ì§€ ìš”ì†Œ ì²˜ë¦¬
                elif tag_name == 'img':
                    src = await extract_attribute(element, "src")
                    alt = await extract_attribute(element, "alt")
                    width = await extract_attribute(element, "width")
                    height = await extract_attribute(element, "height")
                    
                    if src:
                        # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
                        if src.startswith("//"):
                            src = "https:" + src
                        elif src.startswith("/"):
                            src = "https://www.fmkorea.com" + src
                            
                        content.append({
                            "type": "image",
                            "order": order,
                            "data": {
                                "src": src,
                                "alt": alt or f"image_{order}",
                                "width": width or "",
                                "height": height or ""
                            }
                        })
                        order += 1
                        print(f"  ğŸ–¼ï¸ ì´ë¯¸ì§€ ì¶”ê°€: {src}")
                
                # ë¹„ë””ì˜¤ ìš”ì†Œ ì²˜ë¦¬
                elif tag_name == 'video':
                    src = await extract_attribute(element, "src")
                    autoplay = await extract_attribute(element, "autoplay")
                    muted = await extract_attribute(element, "muted")
                    
                    if src:
                        content.append({
                            "type": "video",
                            "order": order,
                            "data": {
                                "src": src,
                                "autoplay": autoplay == "true" or autoplay == "",
                                "muted": muted == "true" or muted == ""
                            }
                        })
                        order += 1
                        print(f"  ğŸ¥ ë¹„ë””ì˜¤ ì¶”ê°€: {src}")
            
            # ë§Œì•½ ìœ„ ë°©ë²•ìœ¼ë¡œ ì½˜í…ì¸ ë¥¼ ì°¾ì§€ ëª»í–ˆë‹¤ë©´ ì§ì ‘ p íƒœê·¸ë“¤ ê²€ìƒ‰
            if len(content) == 0:
                print("ğŸ”„ ëŒ€ì²´ ë°©ë²•ìœ¼ë¡œ ì½˜í…ì¸  ì¶”ì¶œ ì‹œë„...")
                p_elements = await container.query_selector_all("p")
                
                for element in p_elements:
                    text = await extract_text_content(element)
                    if text and text.strip():
                        content.append({
                            "type": "text",
                            "order": order,
                            "data": {"text": text.strip()}
                        })
                        order += 1
                        print(f"  ğŸ“ í…ìŠ¤íŠ¸ ì¶”ê°€ (ëŒ€ì²´): {text[:50]}...")
                
                # ì´ë¯¸ì§€ë„ ë³„ë„ë¡œ ê²€ìƒ‰
                img_elements = await container.query_selector_all("img")
                for element in img_elements:
                    src = await extract_attribute(element, "src")
                    alt = await extract_attribute(element, "alt")
                    
                    if src:
                        if src.startswith("//"):
                            src = "https:" + src
                        elif src.startswith("/"):
                            src = "https://www.fmkorea.com" + src
                            
                        content.append({
                            "type": "image",
                            "order": order,
                            "data": {
                                "src": src,
                                "alt": alt or f"image_{order}",
                                "width": "",
                                "height": ""
                            }
                        })
                        order += 1
                        print(f"  ğŸ–¼ï¸ ì´ë¯¸ì§€ ì¶”ê°€ (ëŒ€ì²´): {src}")
                    
        except Exception as e:
            print(f"ë³¸ë¬¸ ì½˜í…ì¸  ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            
        print(f"ğŸ“„ ì´ {len(content)}ê°œ ì½˜í…ì¸  ìš”ì†Œ ì¶”ì¶œ ì™„ë£Œ")
        return content
        
    async def extract_comments(self, page: Page) -> List[Dict]:
        """ëŒ“ê¸€ ì¶”ì¶œ (ê°œì„ ëœ ë²„ì „)"""
        comments = []
        selectors = self.selectors["comments"]
        
        try:
            # ëŒ“ê¸€ ì»¨í…Œì´ë„ˆë“¤ ì°¾ê¸° (ì‹¤ì œ í™•ì¸ëœ ì…€ë ‰í„° ì‚¬ìš©)
            comment_elements = await page.query_selector_all(selectors["container"])
            print(f"ğŸ” ëŒ“ê¸€ ìš”ì†Œ {len(comment_elements)}ê°œ ë°œê²¬")
            
            for i, element in enumerate(comment_elements):
                comment_data = {}
                
                # ëŒ“ê¸€ ID ì¶”ì¶œ (li ìš”ì†Œì˜ id ì†ì„±ì—ì„œ)
                comment_id_attr = await extract_attribute(element, "id")
                if comment_id_attr:
                    comment_data["comment_id"] = comment_id_attr
                else:
                    comment_data["comment_id"] = f"comment_{i}"
                    
                # ëŒ“ê¸€ ë‚´ìš© ì¶”ì¶œ (ì—¬ëŸ¬ ì…€ë ‰í„° ì‹œë„)
                content_text = ""
                content_selectors = [".xe_content", ".comment_content", "div"]
                
                for content_sel in content_selectors:
                    content_element = await element.query_selector(content_sel)
                    if content_element:
                        temp_text = await extract_text_content(content_element)
                        if temp_text and temp_text.strip():
                            content_text = temp_text
                            break
                
                # ë§Œì•½ ìœ„ ë°©ë²•ìœ¼ë¡œ ì°¾ì§€ ëª»í•˜ë©´ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ
                if not content_text:
                    full_text = await extract_text_content(element)
                    # ì‘ì„±ìëª…ê³¼ ë‚ ì§œë¥¼ ì œì™¸í•œ ì‹¤ì œ ëŒ“ê¸€ ë‚´ìš© ì¶”ì¶œ
                    lines = full_text.split('\n')
                    for line in lines:
                        line = line.strip()
                        if line and not any(keyword in line for keyword in ['ë¶„ ì „', 'ì‹œê°„ ì „', 'ì¼ ì „', 'ì›” ì „']):
                            # ì²« ë²ˆì§¸ ì˜ë¯¸ìˆëŠ” ë¼ì¸ì„ ëŒ“ê¸€ ë‚´ìš©ìœ¼ë¡œ ì‚¬ìš©
                            if len(line) > 2:  # ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ ì œì™¸
                                content_text = line
                                break
                
                comment_data["content"] = content_text
                    
                # ëŒ“ê¸€ ì‘ì„±ì ì¶”ì¶œ
                author_element = await element.query_selector(selectors["author"])
                if author_element:
                    comment_data["author"] = await extract_text_content(author_element)
                else:
                    # ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì‘ì„±ì ì¶”ì¶œ ì‹œë„
                    full_text = await extract_text_content(element)
                    lines = full_text.split('\n')
                    # ë³´í†µ ì²« ë²ˆì§¸ë‚˜ ë‘ ë²ˆì§¸ ë¼ì¸ì— ì‘ì„±ìëª…ì´ ìˆìŒ
                    for line in lines[:3]:
                        line = line.strip()
                        if line and len(line) < 20 and not any(char in line for char in ['ë¶„', 'ì‹œê°„', 'ì¼', 'ì›”']):
                            comment_data["author"] = line
                            break
                    if "author" not in comment_data:
                        comment_data["author"] = ""
                    
                # ëŒ“ê¸€ ì‘ì„±ì¼ ì¶”ì¶œ
                date_element = await element.query_selector(selectors["date"])
                if date_element:
                    date_text = await extract_text_content(date_element)
                    comment_data["date"] = parse_date(date_text)
                else:
                    # ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ë‚ ì§œ ì¶”ì¶œ ì‹œë„
                    full_text = await extract_text_content(element)
                    date_match = re.search(r'(\d+)\s*(ë¶„|ì‹œê°„|ì¼|ì›”)\s*ì „', full_text)
                    if date_match:
                        comment_data["date"] = datetime.now(pytz.timezone('Asia/Seoul')).isoformat()
                    else:
                        comment_data["date"] = datetime.now(pytz.timezone('Asia/Seoul')).isoformat()
                    
                # ëŒ“ê¸€ ë¯¸ë””ì–´ ì¶”ì¶œ
                media_elements = await element.query_selector_all(selectors["media"])
                media_list = []
                for j, media_element in enumerate(media_elements):
                    tag_name = await media_element.evaluate("element => element.tagName.toLowerCase()")
                    src = await extract_attribute(media_element, "src")
                    
                    if src:
                        media_type = "image" if tag_name == "img" else "video"
                        media_list.append({
                            "type": media_type,
                            "order": j,
                            "data": {"src": src}
                        })
                        
                comment_data["media"] = media_list
                
                # FMKoreaëŠ” ë‹¨ìˆœí•œ ëŒ“ê¸€ êµ¬ì¡°
                comment_data["level"] = 1
                comment_data["is_reply"] = False
                comment_data["parent_comment_id"] = ""
                
                # ëŒ“ê¸€ ì¶”ì²œ/ë¹„ì¶”ì²œìˆ˜ ì¶”ì¶œ
                up_element = await element.query_selector(selectors["up_count"])
                if up_element:
                    up_text = await extract_text_content(up_element)
                    comment_data["up_count"] = parse_number(up_text)
                else:
                    comment_data["up_count"] = 0
                    
                down_element = await element.query_selector(selectors["down_count"])
                if down_element:
                    down_text = await extract_text_content(down_element)
                    comment_data["down_count"] = parse_number(down_text)
                else:
                    comment_data["down_count"] = 0
                
                # ìœ íš¨í•œ ëŒ“ê¸€ë§Œ ì¶”ê°€ (ë‚´ìš©ì´ ìˆëŠ” ê²½ìš°)
                if comment_data["content"].strip():
                    comments.append(comment_data)
                    print(f"  âœ… ëŒ“ê¸€ {i+1}: {comment_data['author']} - {comment_data['content'][:30]}...")
                
        except Exception as e:
            print(f"ëŒ“ê¸€ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            
        print(f"ğŸ“ ì´ {len(comments)}ê°œ ëŒ“ê¸€ ì¶”ì¶œ ì™„ë£Œ")
        return comments
        
    async def scrape_post(self, url: str, config: Optional[Dict] = None) -> Dict:
        """ê²Œì‹œê¸€ ìŠ¤í¬ë˜í•‘"""
        if config is None:
            config = {}
            
        browser, page = await setup_browser(config)
        
        try:
            # í˜ì´ì§€ ì´ë™
            success = await navigate_to_page(page, url, config)
            if not success:
                raise Exception("í˜ì´ì§€ ì´ë™ ì‹¤íŒ¨")
                
            # ë°ì´í„° ì¶”ì¶œ
            post_id = parse_post_id(url)
            metadata = await self.extract_metadata(page)
            content = await self.extract_content(page)
            comments = await self.extract_comments(page)
            
            # ê²°ê³¼ êµ¬ì„±
            result = {
                "post_id": post_id,
                "post_url": url,
                "metadata": metadata,
                "content": content,
                "comments": comments,
                "scraped_at": datetime.now(pytz.timezone('Asia/Seoul')).isoformat()
            }
            
            # ë°ì´í„° ê²€ì¦
            if validate_data(result, self.schema):
                return result
            else:
                print("ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨, ê¸°ë³¸ êµ¬ì¡°ë¡œ ë°˜í™˜")
                return result
                
        except Exception as e:
            print(f"ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜: {e}")
            return {}
        finally:
            await close_browser(browser)

# MongoDB ì—°ë™ í•¨ìˆ˜ë“¤
async def connect_mongodb(connection_string: str, db_name: str, collection_name: str) -> Collection:
    """MongoDB ì—°ê²°"""
    try:
        client = MongoClient(connection_string)
        db = client[db_name]
        collection = db[collection_name]
        return collection
    except Exception as e:
        print(f"MongoDB ì—°ê²° ì‹¤íŒ¨: {e}")
        raise

async def save_to_mongodb(collection: Collection, data: Dict) -> str:
    """MongoDBì— ë°ì´í„° ì €ì¥"""
    try:
        result = collection.insert_one(data)
        return str(result.inserted_id)
    except Exception as e:
        print(f"MongoDB ì €ì¥ ì‹¤íŒ¨: {e}")
        raise

# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ë“¤
async def scrape_and_save_page_posts(community: str, page_url: str, config: Optional[Dict] = None, mongo_config: Optional[Dict] = None) -> List[Dict]:
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (íŠ¹ì • í˜ì´ì§€) - íŠ¹ì • í˜ì´ì§€ ê²Œì‹œê¸€ ëª©ë¡ ìŠ¤í¬ë˜í•‘, ì„ ë³„, ìƒì„¸ ìŠ¤í¬ë˜í•‘ ë° ì €ì¥"""
    if config is None:
        config = {}
        
    print(f"ğŸ” {community} í˜ì´ì§€ ìŠ¤í¬ë˜í•‘ ì‹œì‘: {page_url}")
    
    # 1. ê²Œì‹œê¸€ ëª©ë¡ ìŠ¤í¬ë˜í•‘ ë° ì„ ë³„
    selected_posts = await scrape_and_select_page_posts(community, page_url, config)
    print(f"ğŸ“‹ ì„ ë³„ëœ ê²Œì‹œê¸€ ìˆ˜: {len(selected_posts)}")
    
    if not selected_posts:
        print("âŒ ì„ ë³„ëœ ê²Œì‹œê¸€ì´ ì—†ìŠµë‹ˆë‹¤.")
        return []
    
    # 2. ê° ê²Œì‹œê¸€ ìƒì„¸ ìŠ¤í¬ë˜í•‘
    results = []
    scraper = FMKoreaScraper()
    
    for i, post in enumerate(selected_posts, 1):
        print(f"ğŸ“„ ê²Œì‹œê¸€ {i}/{len(selected_posts)} ìŠ¤í¬ë˜í•‘ ì¤‘: {post.get('title', 'N/A')}")
        
        try:
            scraped_data = await scraper.scrape_post(post["url"], config)
            if scraped_data:
                results.append(scraped_data)
                print(f"âœ… ì„±ê³µ: {scraped_data.get('metadata', {}).get('title', 'N/A')}")
            else:
                print(f"âŒ ì‹¤íŒ¨: {post.get('title', 'N/A')}")
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜: {e}")
            
        # ìš”ì²­ ê°„ ê°„ê²© (ì„œë²„ ë¶€í•˜ ë°©ì§€)
        if i < len(selected_posts):
            await asyncio.sleep(config.get('delay_between_requests', 2))
    
    # 3. ê²°ê³¼ ì €ì¥ (íŒŒì¼ + API)
    timestamp = int(time.time())
    file_name = f"fmkorea-page-scraping-{timestamp}.json"
    
    # íŒŒì¼ ì €ì¥
    with open(file_name, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ ê²°ê³¼ê°€ {file_name}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # APIë¡œ ë°ì´í„° ì „ì†¡
    api_url = config.get('api_url', 'http://localhost:3000/api/scraper-data')
    if results:
        print(f"ğŸŒ APIë¡œ ë°ì´í„° ì „ì†¡ ì¤‘: {api_url}")
        try:
            response = requests.post(api_url, json=results, timeout=10)
            if response.status_code == 200:
                print(f"âœ… API ì „ì†¡ ì„±ê³µ: {len(results)}ê°œ ê²Œì‹œê¸€")
            else:
                print(f"âŒ API ì „ì†¡ ì‹¤íŒ¨: {response.status_code}")
        except Exception as e:
            print(f"âŒ API ì „ì†¡ ì˜¤ë¥˜: {e}")
    
    print(f"ğŸ“Š ìµœì¢… ê²°ê³¼: {len(results)}/{len(selected_posts)} ì„±ê³µ")
    
    return results

# ê°„ë‹¨í•œ ì‹¤í–‰ í•¨ìˆ˜
async def scrape_fmkorea_politics_page(config: Optional[Dict] = None) -> List[Dict]:
    """FMKorea ì •ì¹˜ ê²Œì‹œíŒ ì²« í˜ì´ì§€ ìŠ¤í¬ë˜í•‘"""
    page_url = "https://www.fmkorea.com/politics"
    return await scrape_and_save_page_posts("fmkorea", page_url, config)

# API ì „ì†¡ í•¨ìˆ˜
async def send_to_api(data: Dict, api_url: str) -> bool:
    """APIë¥¼ í†µí•´ ë°ì´í„° ì „ì†¡"""
    try:
        response = requests.post(api_url, json=data)
        return response.status_code == 200
    except Exception as e:
        print(f"API ì „ì†¡ ì‹¤íŒ¨: {e}")
        return False 