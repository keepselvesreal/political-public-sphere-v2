"""
FMì½”ë¦¬ì•„ ìŠ¤í¬ë˜í¼ v2 (ì¶”ìƒ ê¸°ë³¸ í´ë˜ìŠ¤ ê¸°ë°˜)

ì£¼ìš” ê°œì„ ì‚¬í•­:
- BaseCommunityScaper ìƒì†ìœ¼ë¡œ ê³µí†µ ë¡œì§ ì¬ì‚¬ìš© (line 20-50)
- ê¸°ì¡´ HTML êµ¬ì¡° ë¶„ì„ ë¡œì§ ì™„ì „ í˜¸í™˜ (line 52-120)
- ìˆœì„œ ë³´ì¡´ ì½˜í…ì¸  ì¶”ì¶œ ë¡œì§ ìœ ì§€ (line 122-200)
- ê²Œì‹œíŒ ëª©ë¡ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì¶”ê°€ (line 202-280)
- CommunityPost ëª¨ë¸ ì¶œë ¥ í†µí•© (line 282-320)

ì‘ì„±ì: AI Assistant
ì‘ì„±ì¼: 2025-01-28
ëª©ì : í†µì¼ëœ êµ¬ì¡°ë¡œ FMì½”ë¦¬ì•„ ìŠ¤í¬ë˜í•‘ (ê¸°ì¡´ ê¸°ëŠ¥ ì™„ì „ í˜¸í™˜)
"""

from typing import List, Dict, Optional
import re
from loguru import logger
from urllib.parse import urljoin

from .base_scraper import BaseCommunityScaper, FMKoreaConfig


class FMKoreaScraper(BaseCommunityScaper):
    """FMì½”ë¦¬ì•„ ìŠ¤í¬ë˜í¼ v2 (í†µì¼ëœ êµ¬ì¡° + ê¸°ì¡´ ë¡œì§ ì™„ì „ í˜¸í™˜)"""
    
    def __init__(self):
        super().__init__(FMKoreaConfig())
    
    def get_site_name(self) -> str:
        return 'fmkorea'
    
    async def wait_for_board_elements(self):
        """ê²Œì‹œíŒ ìš”ì†Œ ë¡œë”© ëŒ€ê¸° (FMì½”ë¦¬ì•„ íŠ¹í™”)"""
        try:
            await self.page.wait_for_selector('table, .board_list, .list_table', 
                                            timeout=self.site_config.wait_timeout)
        except:
            logger.warning("âš ï¸ FMì½”ë¦¬ì•„ ê²Œì‹œíŒ ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    async def wait_for_post_elements(self):
        """ê²Œì‹œê¸€ ìš”ì†Œ ë¡œë”© ëŒ€ê¸° (FMì½”ë¦¬ì•„ íŠ¹í™”)"""
        try:
            await self.page.wait_for_selector('article, .xe_content, .rd_body', 
                                            timeout=self.site_config.wait_timeout)
        except:
            logger.warning("âš ï¸ FMì½”ë¦¬ì•„ ê²Œì‹œê¸€ ë³¸ë¬¸ ì˜ì—­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    async def extract_board_posts(self) -> List[Dict]:
        """
        ê²Œì‹œíŒì—ì„œ ê²Œì‹œê¸€ ëª©ë¡ ì¶”ì¶œ (FMì½”ë¦¬ì•„ íŠ¹í™” + ë©”íƒ€ë°ì´í„° í¬í•¨)
        
        ê¸°ì¡´ ë¡œì§ì„ í™œìš©í•˜ë˜ ë©”íƒ€ë°ì´í„°(ì¶”ì²œìˆ˜, ëŒ“ê¸€ìˆ˜, ì¡°íšŒìˆ˜) ì¶”ê°€ ì¶”ì¶œ
        """
        try:
            posts = []
            
            # FMì½”ë¦¬ì•„ ê²Œì‹œê¸€ í–‰ ì¶”ì¶œ (ë‹¤ì–‘í•œ ì…€ë ‰í„° ì‹œë„)
            row_selectors = [
                'table tbody tr',
                '.list_table tbody tr',
                'tbody tr:has(td)'
            ]
            
            post_rows = []
            for selector in row_selectors:
                try:
                    elements = await self.page.query_selector_all(selector)
                    if elements:
                        post_rows = elements
                        logger.info(f"âœ… FMì½”ë¦¬ì•„ ê²Œì‹œê¸€ í–‰ ë°œê²¬: {len(elements)}ê°œ ({selector})")
                        break
                except:
                    continue
            
            if not post_rows:
                logger.warning("âš ï¸ FMì½”ë¦¬ì•„ ê²Œì‹œê¸€ í–‰ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return []
            
            # ê° ê²Œì‹œê¸€ í–‰ì—ì„œ ì •ë³´ ì¶”ì¶œ
            for row in post_rows:
                try:
                    post_data = await self.extract_post_info_from_row(row)
                    if post_data and post_data.get('post_url'):
                        posts.append(post_data)
                        logger.debug(f"FMì½”ë¦¬ì•„ ê²Œì‹œê¸€ ì¶”ì¶œ: {post_data['title'][:30]}... (ì¶”ì²œ:{post_data.get('like_count', 0)}, ëŒ“ê¸€:{post_data.get('comment_count', 0)}, ì¡°íšŒ:{post_data.get('view_count', 0)})")
                except Exception as e:
                    logger.debug(f"ê²Œì‹œê¸€ í–‰ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    continue
            
            logger.info(f"âœ… FMì½”ë¦¬ì•„ ê²Œì‹œíŒ ëª©ë¡ ì¶”ì¶œ ì™„ë£Œ: {len(posts)}ê°œ")
            return posts
            
        except Exception as e:
            logger.error(f"ğŸ’¥ FMì½”ë¦¬ì•„ ê²Œì‹œíŒ ëª©ë¡ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    async def extract_post_info_from_row(self, row_element) -> Optional[Dict]:
        """
        ê²Œì‹œê¸€ í–‰ì—ì„œ ê¸°ë³¸ ì •ë³´ + ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (FMì½”ë¦¬ì•„ íŠ¹í™”)
        
        ê¸°ì¡´ extract_post_info ë¡œì§ì„ í™•ì¥í•˜ì—¬ ë©”íƒ€ë°ì´í„° ì¶”ê°€
        """
        try:
            post_info = {}
            
            # ì œëª©ê³¼ ë§í¬ ì¶”ì¶œ (ê¸°ì¡´ ë¡œì§ í™œìš©)
            title_selectors = [
                'td.title a',
                '.title a',
                'td a[href*="document_srl"]',
                'td a[href*="/"]'
            ]
            
            title_found = False
            for selector in title_selectors:
                try:
                    title_element = await row_element.query_selector(selector)
                    if title_element:
                        title_text = await title_element.inner_text()
                        href = await title_element.get_attribute('href')
                        
                        if title_text and href:
                            post_info['title'] = title_text.strip()
                            
                            # ì ˆëŒ€ URLë¡œ ë³€í™˜
                            if href.startswith('/'):
                                post_info['post_url'] = urljoin(self.site_config.base_url, href)
                            else:
                                post_info['post_url'] = href
                            
                            # ê²Œì‹œê¸€ ID ì¶”ì¶œ
                            post_info['post_id'] = self.parse_post_id_from_url(post_info['post_url'])
                            title_found = True
                            break
                except:
                    continue
            
            if not title_found:
                return None
            
            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (ì¶”ì²œìˆ˜, ëŒ“ê¸€ìˆ˜, ì¡°íšŒìˆ˜)
            cells = await row_element.query_selector_all('td')
            
            # ê¸°ë³¸ê°’ ì„¤ì •
            post_info['like_count'] = 0
            post_info['comment_count'] = 0
            post_info['view_count'] = 0
            
            for cell in cells:
                try:
                    cell_text = await cell.inner_text()
                    cell_text = cell_text.strip()
                    
                    # ìˆ«ì íŒ¨í„´ í™•ì¸
                    if cell_text.isdigit():
                        number = int(cell_text)
                        
                        # ì…€ì˜ í´ë˜ìŠ¤ë‚˜ ìœ„ì¹˜ë¡œ êµ¬ë¶„
                        cell_class = await cell.get_attribute('class') or ''
                        
                        if 'recomd' in cell_class or 'like' in cell_class or 'vote' in cell_class:
                            post_info['like_count'] = number
                        elif 'comment' in cell_class or 'reply' in cell_class:
                            post_info['comment_count'] = number
                        elif 'hit' in cell_class or 'view' in cell_class:
                            post_info['view_count'] = number
                        else:
                            # í´ë˜ìŠ¤ê°€ ì—†ëŠ” ê²½ìš° ìˆ«ì í¬ê¸°ë¡œ ì¶”ì •
                            if number > 1000:  # ì¡°íšŒìˆ˜ë¡œ ì¶”ì •
                                if post_info['view_count'] == 0:
                                    post_info['view_count'] = number
                            elif number > 0 and number < 1000:  # ì¶”ì²œìˆ˜ë‚˜ ëŒ“ê¸€ìˆ˜ë¡œ ì¶”ì •
                                if post_info['like_count'] == 0:
                                    post_info['like_count'] = number
                                elif post_info['comment_count'] == 0:
                                    post_info['comment_count'] = number
                    
                    # ëŒ“ê¸€ìˆ˜ íŠ¹ë³„ ì²˜ë¦¬ (ì œëª©ì— í¬í•¨ëœ ê²½ìš°)
                    elif '(' in cell_text and ')' in cell_text:
                        comment_match = re.search(r'\((\d+)\)', cell_text)
                        if comment_match:
                            post_info['comment_count'] = int(comment_match.group(1))
                
                except:
                    continue
            
            # ì‘ì„±ì ì¶”ì¶œ (ì„ íƒì‚¬í•­)
            try:
                author_selectors = ['.author', '.writer', '.member']
                for selector in author_selectors:
                    author_element = await row_element.query_selector(selector)
                    if author_element:
                        author_text = await author_element.inner_text()
                        if author_text and author_text.strip():
                            post_info['author'] = author_text.strip()
                            break
            except:
                pass
            
            # ê³µì§€ê¸€ í•„í„°ë§ (ê´€ë¦¬ì ê²Œì‹œê¸€ ì œì™¸)
            if self.is_notice_post(post_info['title'], post_info.get('author', '')):
                logger.debug(f"ê³µì§€ê¸€ ì œì™¸: {post_info['title'][:30]}... (ì‘ì„±ì: {post_info.get('author', 'Unknown')})")
                return None
            
            # ì‘ì„±ì‹œê°„ ì¶”ì¶œ (ì„ íƒì‚¬í•­)
            try:
                date_selectors = ['.date', '.time', '.regdate']
                for selector in date_selectors:
                    date_element = await row_element.query_selector(selector)
                    if date_element:
                        date_text = await date_element.inner_text()
                        if date_text and date_text.strip():
                            post_info['date'] = date_text.strip()
                            break
            except:
                pass
            
            return post_info
            
        except Exception as e:
            logger.debug(f"FMì½”ë¦¬ì•„ ê²Œì‹œê¸€ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    async def extract_post_metadata(self) -> Dict:
        """
        ê²Œì‹œê¸€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (FMì½”ë¦¬ì•„ íŠ¹í™”)
        
        ì‹¤ì œ HTML êµ¬ì¡°ì— ë§ê²Œ ê°œì„ ëœ ì…€ë ‰í„° ì‚¬ìš©
        """
        try:
            metadata = {}
            
            # ì œëª© ì¶”ì¶œ (ì‹¤ì œ HTML êµ¬ì¡°: h1.np_18px > span.np_18px_span)
            title_selectors = [
                'h1.np_18px span.np_18px_span',  # ì‹¤ì œ êµ¬ì¡°
                'h1.np_18px',
                '.np_18px_span',
                'h1 span',
                'h1'
            ]
            
            for selector in title_selectors:
                try:
                    title_element = await self.page.query_selector(selector)
                    if title_element:
                        title_text = await title_element.inner_text()
                        if title_text and title_text.strip():
                            metadata['title'] = title_text.strip()
                            logger.info(f"âœ… ì œëª© ì¶”ì¶œ ì„±ê³µ: {title_text.strip()}")
                            break
                except:
                    continue
            
            # ì‘ì„±ì ì¶”ì¶œ (ì‹¤ì œ HTML êµ¬ì¡°: .btm_area .side .member_plate)
            author_selectors = [
                '.btm_area .side .member_plate',  # ì‹¤ì œ êµ¬ì¡°
                '.member_plate',
                '.side .member_plate',
                '.btm_area .member_plate'
            ]
            
            for selector in author_selectors:
                try:
                    author_element = await self.page.query_selector(selector)
                    if author_element:
                        author_text = await author_element.inner_text()
                        if author_text and author_text.strip():
                            metadata['author'] = author_text.strip()
                            logger.info(f"âœ… ì‘ì„±ì ì¶”ì¶œ ì„±ê³µ: {author_text.strip()}")
                            break
                except:
                    continue
            
            # ì‘ì„± ì‹œê°„ ì¶”ì¶œ (ì‹¤ì œ HTML êµ¬ì¡°: .top_area .date.m_no)
            date_selectors = [
                '.top_area .date.m_no',  # ì‹¤ì œ êµ¬ì¡°
                '.date.m_no',
                '.top_area .date',
                '.date',
                'span.date'
            ]
            
            for selector in date_selectors:
                try:
                    date_element = await self.page.query_selector(selector)
                    if date_element:
                        date_text = await date_element.inner_text()
                        if date_text and date_text.strip():
                            metadata['date'] = date_text.strip()
                            logger.info(f"âœ… ì‘ì„±ì‹œê°„ ì¶”ì¶œ ì„±ê³µ: {date_text.strip()}")
                            break
                except:
                    continue
            
            # í†µê³„ ì •ë³´ ì¶”ì¶œ (ì‹¤ì œ HTML êµ¬ì¡°: .btm_area .side.fr span)
            view_count = 0
            like_count = 0
            comment_count = 0
            
            try:
                # ì¡°íšŒìˆ˜, ì¶”ì²œìˆ˜, ëŒ“ê¸€ìˆ˜ê°€ í¬í•¨ëœ ì˜ì—­
                stats_area = await self.page.query_selector('.btm_area .side.fr')
                if stats_area:
                    stats_text = await stats_area.inner_text()
                    logger.debug(f"í†µê³„ ì˜ì—­ í…ìŠ¤íŠ¸: {stats_text}")
                    
                    # ì¡°íšŒìˆ˜ ì¶”ì¶œ
                    view_match = re.search(r'ì¡°íšŒ\s*ìˆ˜?\s*(\d+)', stats_text)
                    if view_match:
                        view_count = int(view_match.group(1))
                    
                    # ì¶”ì²œìˆ˜ ì¶”ì¶œ
                    like_match = re.search(r'ì¶”ì²œ\s*ìˆ˜?\s*(\d+)', stats_text)
                    if like_match:
                        like_count = int(like_match.group(1))
                    
                    # ëŒ“ê¸€ìˆ˜ ì¶”ì¶œ
                    comment_match = re.search(r'ëŒ“ê¸€\s*(\d+)', stats_text)
                    if comment_match:
                        comment_count = int(comment_match.group(1))
                        
                    logger.info(f"âœ… í†µê³„ ì¶”ì¶œ ì„±ê³µ - ì¡°íšŒ:{view_count}, ì¶”ì²œ:{like_count}, ëŒ“ê¸€:{comment_count}")
            except Exception as e:
                logger.debug(f"í†µê³„ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            
            # ì¶”ê°€ë¡œ í¬í…/ë°©ì¶œ ë²„íŠ¼ì—ì„œ ì¶”ì²œìˆ˜ í™•ì¸
            try:
                vote_element = await self.page.query_selector('.fm_vote .new_voted_count')
                if vote_element:
                    vote_text = await vote_element.inner_text()
                    if vote_text and vote_text.strip().isdigit():
                        like_count = int(vote_text.strip())
                        logger.info(f"âœ… í¬í… ë²„íŠ¼ì—ì„œ ì¶”ì²œìˆ˜ ì¶”ì¶œ: {like_count}")
            except:
                pass
            
            metadata['view_count'] = view_count
            metadata['like_count'] = like_count
            metadata['dislike_count'] = 0  # ì—í¨ì½”ë¦¬ì•„ëŠ” ë¹„ì¶”ì²œ ìˆ˜ê°€ ë³„ë„ë¡œ í‘œì‹œë˜ì§€ ì•ŠìŒ
            metadata['comment_count'] = comment_count
            
            logger.info(f"âœ… FMì½”ë¦¬ì•„ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ: {metadata}")
            return metadata
            
        except Exception as e:
            logger.error(f"ğŸ’¥ FMì½”ë¦¬ì•„ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {}
    
    async def extract_content_in_order(self) -> List[Dict]:
        """
        ê²Œì‹œê¸€ ë³¸ë¬¸ ë‚´ìš© ìˆœì„œëŒ€ë¡œ ì¶”ì¶œ (FMì½”ë¦¬ì•„ íŠ¹í™”)
        
        ì‹¤ì œ HTML êµ¬ì¡°: article .xe_content
        """
        try:
            content_list: List[Dict] = []
            order = 0
            
            # ë³¸ë¬¸ ì»¨í…Œì´ë„ˆ ì°¾ê¸° (ì‹¤ì œ HTML êµ¬ì¡°)
            article_selectors = [
                'article .xe_content',  # ì‹¤ì œ êµ¬ì¡°
                '.xe_content',
                'article',
                '.rd_body .xe_content',
                '.document_content'
            ]
            
            article_element = None
            for selector in article_selectors:
                try:
                    element = await self.page.query_selector(selector)
                    if element:
                        article_element = element
                        logger.info(f"âœ… FMì½”ë¦¬ì•„ ë³¸ë¬¸ ì»¨í…Œì´ë„ˆ ë°œê²¬: {selector}")
                        break
                except:
                    continue
            
            if not article_element:
                logger.warning("âš ï¸ FMì½”ë¦¬ì•„ ê²Œì‹œê¸€ ë³¸ë¬¸ ì»¨í…Œì´ë„ˆë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return []
            
            # ë³¸ë¬¸ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°
            try:
                content_preview = await article_element.inner_text()
                logger.info(f"ğŸ“„ ë³¸ë¬¸ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {content_preview[:100]}...")
            except:
                pass
            
            # ê¸°ì¡´ extract_elements_improved ë¡œì§ ì‚¬ìš©
            order = await self.extract_elements_improved(article_element, content_list, order)
            
            logger.info(f"âœ… FMì½”ë¦¬ì•„ ì½˜í…ì¸  ì¶”ì¶œ ì™„ë£Œ: {len(content_list)}ê°œ ìš”ì†Œ")
            return content_list
            
        except Exception as e:
            logger.error(f"ğŸ’¥ FMì½”ë¦¬ì•„ ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    async def extract_elements_improved(self, parent_element, content_list: List[Dict], order_start: int) -> int:
        """
        ê°œì„ ëœ ìš”ì†Œ ì¶”ì¶œ (ì—í¨ì½”ë¦¬ì•„ íŠ¹í™”)
        
        ë¹ˆ ìš”ì†Œì™€ í…ìŠ¤íŠ¸ ë…¸ë“œë¥¼ í¬í•¨í•œ ëª¨ë“  ì½˜í…ì¸ ë¥¼ ìˆœì„œëŒ€ë¡œ ì¶”ì¶œ
        """
        try:
            order = order_start
            
            # ì§ì ‘ ìì‹ ìš”ì†Œë“¤ì„ ìˆœì„œëŒ€ë¡œ ì²˜ë¦¬ (ì¤‘ì²©ëœ ìì‹ ì œì™¸)
            direct_children = await parent_element.query_selector_all(':scope > *')
            
            processed_images = set()  # ì¤‘ë³µ ì´ë¯¸ì§€ ë°©ì§€
            
            logger.info(f"ğŸ” ì—í¨ì½”ë¦¬ì•„ ì§ì ‘ ìì‹ ìš”ì†Œ ê°œìˆ˜: {len(direct_children)}")
            
            for i, element in enumerate(direct_children):
                try:
                    tag_name = await element.evaluate('el => el.tagName.toLowerCase()')
                    element_text = await element.inner_text()
                    element_html = await element.inner_html()
                    
                    logger.debug(f"ìš”ì†Œ {i+1}: <{tag_name}> - í…ìŠ¤íŠ¸: '{element_text}' - HTML: '{element_html[:100]}...'")
                    
                    # ì´ë¯¸ì§€ ì²˜ë¦¬
                    if tag_name == 'img':
                        img_src = await element.get_attribute('src')
                        if img_src and img_src not in processed_images:
                            processed_images.add(img_src)
                            
                            # ì´ë¯¸ì§€ ë§í¬ ì°¾ê¸°
                            parent_link = await element.evaluate('''
                                el => {
                                    let parent = el.parentElement;
                                    while (parent && parent.tagName.toLowerCase() !== 'a') {
                                        parent = parent.parentElement;
                                    }
                                    return parent ? parent.href : null;
                                }
                            ''')
                            
                            image_data = await self.extract_image_data(element, parent_link, order)
                            if image_data:
                                content_list.append(image_data)
                                order += 1
                                logger.debug(f"âœ… ì´ë¯¸ì§€ ì¶”ê°€: {img_src}")
                    
                    # ì´ë¯¸ì§€ë¥¼ í¬í•¨í•œ ìš”ì†Œ ì²˜ë¦¬
                    elif tag_name in ['div', 'p', 'span']:
                        # ë‚´ë¶€ì— ì´ë¯¸ì§€ê°€ ìˆëŠ”ì§€ í™•ì¸
                        inner_images = await element.query_selector_all('img')
                        
                        if inner_images:
                            # ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²½ìš° ì´ë¯¸ì§€ë“¤ì„ ì¶”ì¶œ
                            for img in inner_images:
                                img_src = await img.get_attribute('src')
                                if img_src and img_src not in processed_images:
                                    processed_images.add(img_src)
                                    
                                    # ì´ë¯¸ì§€ ë§í¬ ì°¾ê¸°
                                    parent_link = await img.evaluate('''
                                        el => {
                                            let parent = el.parentElement;
                                            while (parent && parent.tagName.toLowerCase() !== 'a') {
                                                parent = parent.parentElement;
                                            }
                                            return parent ? parent.href : null;
                                        }
                                    ''')
                                    
                                    image_data = await self.extract_image_data(img, parent_link, order)
                                    if image_data:
                                        content_list.append(image_data)
                                        order += 1
                                        logger.debug(f"âœ… ë‚´ë¶€ ì´ë¯¸ì§€ ì¶”ê°€: {img_src}")
                        
                        # í…ìŠ¤íŠ¸ ë‚´ìš©ì´ ìˆëŠ” ê²½ìš° í…ìŠ¤íŠ¸ë„ ì¶”ì¶œ
                        if element_text and element_text.strip():
                            text_data = await self.extract_text_data(element, order)
                            if text_data:
                                content_list.append(text_data)
                                order += 1
                                logger.debug(f"âœ… í…ìŠ¤íŠ¸ ì¶”ê°€: {element_text[:50]}...")
                        
                        # í…ìŠ¤íŠ¸ê°€ ì—†ì–´ë„ ì˜ë¯¸ìˆëŠ” HTMLì´ ìˆìœ¼ë©´ ì¶”ê°€ (ì˜ˆ: <br>, ë¹ˆ div ë“±)
                        elif element_html and element_html.strip():
                            # br íƒœê·¸ë‚˜ ë¹ˆ divë„ ë ˆì´ì•„ì›ƒìƒ ì˜ë¯¸ê°€ ìˆì„ ìˆ˜ ìˆìŒ
                            if tag_name in ['br'] or 'br' in element_html.lower():
                                text_data = {
                                    'type': 'text',
                                    'order': order,
                                    'data': {
                                        'tag': tag_name,
                                        'text': '\n',  # ì¤„ë°”ê¿ˆìœ¼ë¡œ ì²˜ë¦¬
                                        'id': await element.get_attribute('id') or '',
                                        'class': await element.get_attribute('class') or '',
                                        'style': await element.get_attribute('style') or '',
                                        'innerHTML': element_html
                                    }
                                }
                                content_list.append(text_data)
                                order += 1
                                logger.debug(f"âœ… ì¤„ë°”ê¿ˆ ìš”ì†Œ ì¶”ê°€: <{tag_name}>")
                    
                    # ë¹„ë””ì˜¤ ì²˜ë¦¬
                    elif tag_name in ['video', 'iframe']:
                        video_data = await self.extract_video_data(element, order)
                        if video_data:
                            content_list.append(video_data)
                            order += 1
                            logger.debug(f"âœ… ë¹„ë””ì˜¤ ì¶”ê°€: <{tag_name}>")
                    
                    # ê¸°íƒ€ í…ìŠ¤íŠ¸ ìš”ì†Œ ì²˜ë¦¬
                    elif tag_name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'strong', 'em', 'b', 'i']:
                        if element_text and element_text.strip():
                            text_data = await self.extract_text_data(element, order)
                            if text_data:
                                content_list.append(text_data)
                                order += 1
                                logger.debug(f"âœ… ì œëª©/ê°•ì¡° í…ìŠ¤íŠ¸ ì¶”ê°€: {element_text[:50]}...")
                
                except Exception as e:
                    logger.debug(f"ìš”ì†Œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œë¨): {e}")
                    continue
            
            # ì¶”ê°€ë¡œ í…ìŠ¤íŠ¸ ë…¸ë“œë„ í™•ì¸ (JavaScriptë¡œ)
            try:
                text_nodes = await parent_element.evaluate('''
                    el => {
                        const walker = document.createTreeWalker(
                            el,
                            NodeFilter.SHOW_TEXT,
                            null,
                            false
                        );
                        
                        const textNodes = [];
                        let node;
                        while (node = walker.nextNode()) {
                            const text = node.textContent.trim();
                            if (text && text.length > 0) {
                                textNodes.push(text);
                            }
                        }
                        return textNodes;
                    }
                ''')
                
                logger.debug(f"ğŸ” ì¶”ê°€ í…ìŠ¤íŠ¸ ë…¸ë“œë“¤: {text_nodes}")
                
                # ì•„ì§ ì¶”ì¶œë˜ì§€ ì•Šì€ í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ì¶”ê°€
                for text_node in text_nodes:
                    # ì´ë¯¸ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ì¸ì§€ í™•ì¸
                    already_extracted = any(
                        item.get('data', {}).get('text', '') == text_node 
                        for item in content_list 
                        if item.get('type') == 'text'
                    )
                    
                    if not already_extracted and len(text_node) > 2:  # ì˜ë¯¸ìˆëŠ” í…ìŠ¤íŠ¸ë§Œ
                        text_data = {
                            'type': 'text',
                            'order': order,
                            'data': {
                                'tag': 'text_node',
                                'text': text_node,
                                'id': '',
                                'class': '',
                                'style': '',
                                'innerHTML': text_node
                            }
                        }
                        content_list.append(text_data)
                        order += 1
                        logger.debug(f"âœ… ì¶”ê°€ í…ìŠ¤íŠ¸ ë…¸ë“œ: {text_node[:50]}...")
                        
            except Exception as e:
                logger.debug(f"í…ìŠ¤íŠ¸ ë…¸ë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            
            logger.info(f"âœ… ì—í¨ì½”ë¦¬ì•„ ìš”ì†Œ ì¶”ì¶œ ì™„ë£Œ: {order - order_start}ê°œ ì¶”ê°€")
            return order
            
        except Exception as e:
            logger.error(f"ğŸ’¥ ì—í¨ì½”ë¦¬ì•„ ìš”ì†Œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return order_start
    
    async def extract_image_data(self, link_element, img_element, order: int) -> Optional[Dict]:
        """ì´ë¯¸ì§€ ë°ì´í„° ì¶”ì¶œ (ê¸°ì¡´ ë¡œì§ ì™„ì „ í™œìš©)"""
        try:
            # ì´ë¯¸ì§€ ì†ŒìŠ¤ ìš°ì„ ìˆœìœ„: data-original > src
            src = await img_element.get_attribute('data-original')
            if not src:
                src = await img_element.get_attribute('src')
            
            if not src:
                return None
            
            # ì ˆëŒ€ URLë¡œ ë³€í™˜
            src = self.make_absolute_url(src)
            
            # ì›ë³¸ srcë„ ë³´ì¡´
            original_src = await img_element.get_attribute('src')
            if original_src:
                original_src = self.make_absolute_url(original_src)
            
            img_data = {
                'type': 'image',
                'order': order,
                'data': {
                    'src': src,
                    'original_src': original_src if original_src != src else '',
                    'width': await img_element.get_attribute('width'),
                    'height': await img_element.get_attribute('height'),
                    'style': await img_element.get_attribute('style') or '',
                    'alt': await img_element.get_attribute('alt') or '',
                    'class': await img_element.get_attribute('class') or '',
                    'title': await img_element.get_attribute('title') or '',
                    'data_original': await img_element.get_attribute('data-original') or '',
                    'data_file_srl': await img_element.get_attribute('data-file-srl') or ''
                }
            }
            
            # ë§í¬ ì •ë³´ ì¶”ê°€
            if link_element:
                href = await link_element.get_attribute('href')
                if href:
                    href = self.make_absolute_url(href)
                    img_data['data']['href'] = href
                    img_data['data']['link_class'] = await link_element.get_attribute('class') or ''
                    img_data['data']['link_rel'] = await link_element.get_attribute('rel') or ''
            
            return img_data
            
        except Exception as e:
            logger.warning(f"âš ï¸ FMì½”ë¦¬ì•„ ì´ë¯¸ì§€ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    async def extract_video_data(self, video_element, order: int) -> Optional[Dict]:
        """ë™ì˜ìƒ ë°ì´í„° ì¶”ì¶œ (ê¸°ì¡´ ë¡œì§ ì™„ì „ í™œìš©)"""
        try:
            src = await video_element.get_attribute('src')
            if not src:
                source = await video_element.query_selector('source')
                if source:
                    src = await source.get_attribute('src')
            
            if not src:
                return None
            
            src = self.make_absolute_url(src)
            
            # ìë™ì¬ìƒ ì†ì„± ê°ì§€
            autoplay = await video_element.get_attribute('autoplay') is not None
            muted = await video_element.get_attribute('muted') is not None
            
            # ìë™ì¬ìƒì´ë©´ ìŒì†Œê±° ì²˜ë¦¬ (ë¸Œë¼ìš°ì € ì •ì±…)
            if autoplay and not muted:
                muted = True
            
            video_data = {
                'type': 'video',
                'order': order,
                'data': {
                    'src': src,
                    'poster': await video_element.get_attribute('poster') or '',
                    'width': await video_element.get_attribute('width'),
                    'height': await video_element.get_attribute('height'),
                    'autoplay': autoplay,
                    'loop': await video_element.get_attribute('loop') is not None,
                    'muted': muted,
                    'controls': await video_element.get_attribute('controls') is not None,
                    'class': await video_element.get_attribute('class') or '',
                    'preload': await video_element.get_attribute('preload') or 'metadata'
                }
            }
            
            return video_data
            
        except Exception as e:
            logger.warning(f"âš ï¸ FMì½”ë¦¬ì•„ ë™ì˜ìƒ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    async def extract_text_data(self, text_element, order: int) -> Optional[Dict]:
        """í…ìŠ¤íŠ¸ ë°ì´í„° ì¶”ì¶œ (ê¸°ì¡´ ë¡œì§ ì™„ì „ í™œìš©)"""
        try:
            text_content = await text_element.evaluate('el => el.textContent?.trim()')
            if not text_content or len(text_content.strip()) == 0:
                return None
            
            tag_name = await text_element.evaluate('el => el.tagName.toLowerCase()')
            
            text_data = {
                'type': 'text',
                'order': order,
                'data': {
                    'tag': tag_name,
                    'text': text_content,
                    'id': await text_element.get_attribute('id') or '',
                    'class': await text_element.get_attribute('class') or '',
                    'style': await text_element.get_attribute('style') or '',
                    'innerHTML': await text_element.inner_html()
                }
            }
            
            return text_data
            
        except Exception as e:
            logger.warning(f"âš ï¸ FMì½”ë¦¬ì•„ í…ìŠ¤íŠ¸ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    async def extract_comments_data(self) -> List[Dict]:
        """
        ëŒ“ê¸€ ë°ì´í„° ì¶”ì¶œ (FMì½”ë¦¬ì•„ íŠ¹í™”, ì‹¤ì œ HTML êµ¬ì¡° ê¸°ë°˜)
        
        ì‹¤ì œ HTML êµ¬ì¡°: .fdb_lst_ul > .fdb_itm
        """
        try:
            comments = []
            
            # ëŒ“ê¸€ ì»¨í…Œì´ë„ˆ í™•ì¸ (ì‹¤ì œ HTML êµ¬ì¡°)
            comment_container_selectors = [
                '.fdb_lst_ul',  # ì‹¤ì œ êµ¬ì¡°
                '.fdb_lst',
                '#cmtPosition .fdb_lst_ul',
                '.comment_list'
            ]
            
            comment_wrapper = None
            for selector in comment_container_selectors:
                try:
                    element = await self.page.query_selector(selector)
                    if element:
                        comment_wrapper = element
                        logger.info(f"âœ… FMì½”ë¦¬ì•„ ëŒ“ê¸€ ì»¨í…Œì´ë„ˆ ë°œê²¬: {selector}")
                        break
                except:
                    continue
            
            if not comment_wrapper:
                logger.info("ğŸ“ FMì½”ë¦¬ì•„ ëŒ“ê¸€ì´ ì—†ìŠµë‹ˆë‹¤.")
                return []
            
            # ëŒ“ê¸€ ìš”ì†Œë“¤ ì¶”ì¶œ (ì‹¤ì œ HTML êµ¬ì¡°: .fdb_itm)
            comment_elements = await comment_wrapper.query_selector_all('.fdb_itm')
            
            if not comment_elements:
                logger.warning("âš ï¸ FMì½”ë¦¬ì•„ ëŒ“ê¸€ ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return []
            
            logger.info(f"âœ… FMì½”ë¦¬ì•„ ëŒ“ê¸€ ìš”ì†Œ ë°œê²¬: {len(comment_elements)}ê°œ")
            
            # ê° ëŒ“ê¸€ ìš”ì†Œ ì²˜ë¦¬
            for i, comment_element in enumerate(comment_elements):
                try:
                    comment_data = await self.extract_single_comment_improved(comment_element, i)
                    if comment_data:
                        comments.append(comment_data)
                        logger.debug(f"ëŒ“ê¸€ ì¶”ì¶œ: {comment_data['author']} - {comment_data['content'][:50]}...")
                except Exception as e:
                    logger.warning(f"âš ï¸ FMì½”ë¦¬ì•„ ëŒ“ê¸€ {i+1} ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                    continue
            
            logger.info(f"âœ… FMì½”ë¦¬ì•„ ëŒ“ê¸€ ì¶”ì¶œ ì™„ë£Œ: {len(comments)}ê°œ")
            return comments
            
        except Exception as e:
            logger.error(f"ğŸ’¥ FMì½”ë¦¬ì•„ ëŒ“ê¸€ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    async def extract_single_comment_improved(self, comment_element, index: int) -> Optional[Dict]:
        """
        ê°œë³„ ëŒ“ê¸€ ì¶”ì¶œ (ì‹¤ì œ HTML êµ¬ì¡° ê¸°ë°˜)
        
        ì‹¤ì œ HTML êµ¬ì¡°:
        - ëŒ“ê¸€ ID: li#comment_8468493522
        - ì‘ì„±ì: .meta .member_plate
        - ë‚´ìš©: .comment-content .xe_content
        - ì‹œê°„: .meta .date
        - ì¶”ì²œìˆ˜: .vote .voted_count
        """
        try:
            # ëŒ“ê¸€ ID ì¶”ì¶œ (ì‹¤ì œ êµ¬ì¡°: li#comment_8468493522)
            comment_id = await comment_element.get_attribute('id')
            if not comment_id:
                comment_id = f'comment_{index}'
            
            # ëŒ€ëŒ“ê¸€ ì—¬ë¶€ í™•ì¸ (ì‹¤ì œ êµ¬ì¡°: .re í´ë˜ìŠ¤ì™€ margin-left ìŠ¤íƒ€ì¼)
            is_reply = False
            try:
                class_attr = await comment_element.get_attribute('class') or ''
                style_attr = await comment_element.get_attribute('style') or ''
                if 're' in class_attr or 'margin-left' in style_attr:
                    is_reply = True
            except:
                pass
            
            # ì‘ì„±ì ì¶”ì¶œ (ì‹¤ì œ êµ¬ì¡°: .meta .member_plate)
            author = 'ìµëª…'
            try:
                author_element = await comment_element.query_selector('.meta .member_plate')
                if author_element:
                    author_text = await author_element.inner_text()
                    if author_text and author_text.strip():
                        author = author_text.strip()
                        logger.debug(f"ì‘ì„±ì ì¶”ì¶œ: {author}")
            except:
                pass
            
            # ëŒ“ê¸€ ë‚´ìš© ì¶”ì¶œ (ì‹¤ì œ êµ¬ì¡°: .comment-content .xe_content)
            content = ''
            try:
                content_selectors = [
                    '.comment-content .xe_content',  # ì‹¤ì œ êµ¬ì¡°
                    '.xe_content',
                    '.comment-content'
                ]
                
                for selector in content_selectors:
                    content_element = await comment_element.query_selector(selector)
                    if content_element:
                        content_text = await content_element.inner_text()
                        if content_text and content_text.strip():
                            content = content_text.strip()
                            logger.debug(f"ëŒ“ê¸€ ë‚´ìš© ì¶”ì¶œ: {content[:50]}...")
                            break
            except:
                pass
            
            # ì‘ì„± ì‹œê°„ ì¶”ì¶œ (ì‹¤ì œ êµ¬ì¡°: .meta .date)
            date = ''
            try:
                date_element = await comment_element.query_selector('.meta .date')
                if date_element:
                    date_text = await date_element.inner_text()
                    if date_text and date_text.strip():
                        date = date_text.strip()
                        logger.debug(f"ì‘ì„±ì‹œê°„ ì¶”ì¶œ: {date}")
            except:
                pass
            
            # ì¶”ì²œ/ë¹„ì¶”ì²œ ìˆ˜ ì¶”ì¶œ (ì‹¤ì œ êµ¬ì¡°: .vote .voted_count, .vote .blamed_count)
            like_count = 0
            dislike_count = 0
            
            try:
                # ì¶”ì²œìˆ˜ (ì‹¤ì œ êµ¬ì¡°: .vote .voted_count)
                voted_element = await comment_element.query_selector('.vote .voted_count')
                if voted_element:
                    voted_text = await voted_element.inner_text()
                    if voted_text and voted_text.strip().isdigit():
                        like_count = int(voted_text.strip())
                        logger.debug(f"ì¶”ì²œìˆ˜ ì¶”ì¶œ: {like_count}")
                
                # ë¹„ì¶”ì²œìˆ˜ (ì‹¤ì œ êµ¬ì¡°: .vote .blamed_count)
                blamed_element = await comment_element.query_selector('.vote .blamed_count')
                if blamed_element:
                    blamed_text = await blamed_element.inner_text()
                    if blamed_text and blamed_text.strip().isdigit():
                        dislike_count = int(blamed_text.strip())
                        logger.debug(f"ë¹„ì¶”ì²œìˆ˜ ì¶”ì¶œ: {dislike_count}")
            except:
                pass
            
            # ë‚´ìš©ì´ ì—†ëŠ” ëŒ“ê¸€ì€ ì œì™¸
            if not content:
                logger.debug(f"ë‚´ìš©ì´ ì—†ëŠ” ëŒ“ê¸€ ì œì™¸: {comment_id}")
                return None
            
            comment_data = {
                'id': comment_id,
                'author': author,
                'content': content,
                'date': date,
                'like_count': like_count,
                'dislike_count': dislike_count,
                'is_reply': is_reply,
                'is_best': False   # ë² ìŠ¤íŠ¸ ëŒ“ê¸€ êµ¬ë¶„ ë¡œì§ ì¶”ê°€ ê°€ëŠ¥
            }
            
            return comment_data
            
        except Exception as e:
            logger.warning(f"âš ï¸ FMì½”ë¦¬ì•„ ê°œë³„ ëŒ“ê¸€ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    def parse_post_id_from_url(self, url: str) -> str:
        """URLì—ì„œ ê²Œì‹œê¸€ ID ì¶”ì¶œ (ê¸°ì¡´ ë¡œì§ ì™„ì „ í™œìš©)"""
        try:
            if '/index.php' in url:
                from urllib.parse import parse_qs, urlparse
                parsed = urlparse(url)
                params = parse_qs(parsed.query)
                if 'document_srl' in params:
                    return params['document_srl'][0]
            else:
                match = re.search(r'/(\d+)/?$', url)
                if match:
                    return match.group(1)
            
            numbers = re.findall(r'\d+', url)
            if numbers:
                return max(numbers, key=len)
            
            return 'unknown'
            
        except Exception as e:
            logger.error(f"ğŸ’¥ FMì½”ë¦¬ì•„ ê²Œì‹œê¸€ ID ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return 'unknown'


# í¸ì˜ í•¨ìˆ˜ë“¤
async def scrape_fmkorea_board_v2(board_url: str, criteria_count: int = 3) -> Dict[str, List[Dict]]:
    """FMì½”ë¦¬ì•„ ê²Œì‹œíŒ ì§€í‘œë³„ ì„ ë³„ ìŠ¤í¬ë˜í•‘ í¸ì˜ í•¨ìˆ˜"""
    async with FMKoreaScraper() as scraper:
        return await scraper.scrape_board_with_selection(board_url, criteria_count)


async def scrape_fmkorea_post_v2(post_url: str) -> Dict:
    """FMì½”ë¦¬ì•„ ê²Œì‹œê¸€ ìƒì„¸ ìŠ¤í¬ë˜í•‘ í¸ì˜ í•¨ìˆ˜"""
    async with FMKoreaScraper() as scraper:
        return await scraper.scrape_post_detail(post_url) 