"""
ì—í¨ì½”ë¦¬ì•„ ìŠ¤í¬ë˜í¼ ëª¨ë“ˆ

ì£¼ìš” ê¸°ëŠ¥:
- FMKoreaScraper: ë©”ì¸ ìŠ¤í¬ë˜í¼ í´ë˜ìŠ¤ (line 30-80)
- extract_post_list: ê²Œì‹œê¸€ ëª©ë¡ ì¶”ì¶œ (line 82-150)
- filter_admin_posts: ê´€ë¦¬ì ê²Œì‹œê¸€ í•„í„°ë§ (line 152-180)
- select_top_posts: ìƒìœ„ ê²Œì‹œê¸€ ì„ ë³„ (line 182-220)
- scrape_post_detail: ê°œë³„ ê²Œì‹œê¸€ ìŠ¤í¬ë˜í•‘ (line 222-280)
- extract_comments: ëŒ“ê¸€ ì¶”ì¶œ (line 282-350)
- extract_content_in_order: ë³¸ë¬¸ ìˆœì„œ ë³´ì¡´ ì¶”ì¶œ (line 352-420)

ì‘ì„±ì: AI Assistant
ì‘ì„±ì¼: 2025-06-02 16:45 KST
ëª©ì : TDD ê¸°ë°˜ ì—í¨ì½”ë¦¬ì•„ ìŠ¤í¬ë˜í¼ êµ¬í˜„
"""

import asyncio
import re
import json
from datetime import datetime
from typing import List, Dict, Optional, Any
from urllib.parse import urljoin, urlparse
import pytz
from bs4 import BeautifulSoup

from playwright.async_api import async_playwright, Page, Browser
from playwright_stealth import stealth_async
from loguru import logger


class FMKoreaScraper:
    """
    ì—í¨ì½”ë¦¬ì•„ ìŠ¤í¬ë˜í¼ í´ë˜ìŠ¤
    
    TDD ë°©ì‹ìœ¼ë¡œ ê°œë°œëœ ìŠ¤í¬ë˜í¼ë¡œ ë‹¤ìŒ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤:
    1. ê²Œì‹œê¸€ ëª©ë¡ ì¶”ì¶œ (ê³µì§€ê¸€ ì œì™¸)
    2. ì¶”ì²œìˆ˜, ëŒ“ê¸€ìˆ˜, ì¡°íšŒìˆ˜ ìƒìœ„ 3ê°œì”© ì„ ë³„
    3. ê°œë³„ ê²Œì‹œê¸€ ìƒì„¸ ìŠ¤í¬ë˜í•‘
    4. ëŒ“ê¸€ ê³„ì¸µêµ¬ì¡° ë³´ì¡´
    5. ë³¸ë¬¸ ë‚´ìš© ìˆœì„œ ë³´ì¡´
    """
    
    def __init__(self):
        self.base_url = 'https://www.fmkorea.com'
        self.browser: Optional[Browser] = None
        self.page: Optional[Page] = None
        
        # í•œêµ­ ì‹œê°„ëŒ€ ì„¤ì •
        self.kst = pytz.timezone('Asia/Seoul')
        
        # ìŠ¤í¬ë˜í•‘ ì„¤ì •
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        ]
        
        # ëŒ€ê¸° ì‹œê°„ ì„¤ì •
        self.wait_timeout = 15000  # 15ì´ˆ
        self.navigation_timeout = 30000  # 30ì´ˆ
    
    async def __aenter__(self):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§„ì…"""
        await self.setup_browser()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ"""
        await self.close_browser()
    
    async def setup_browser(self):
        """ë¸Œë¼ìš°ì € ì„¤ì • ë° ì´ˆê¸°í™”"""
        try:
            playwright = await async_playwright().start()
            
            # ë¸Œë¼ìš°ì € ì˜µì…˜ ì„¤ì •
            self.browser = await playwright.chromium.launch(
                headless=False,  # ë´‡ ì°¨ë‹¨ ìš°íšŒë¥¼ ìœ„í•´ í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ í•´ì œ
                args=[
                    '--no-sandbox',
                    '--disable-dev-shm-usage',
                    '--disable-blink-features=AutomationControlled',
                    '--window-size=1920,1080'
                ]
            )
            
            # ìƒˆ í˜ì´ì§€ ìƒì„±
            self.page = await self.browser.new_page()
            
            # Stealth ëª¨ë“œ ì ìš©
            await stealth_async(self.page)
            
            # ì¶”ê°€ ì„¤ì •
            await self.page.set_viewport_size({"width": 1920, "height": 1080})
            await self.page.set_extra_http_headers({
                'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8'
            })
            
            logger.info("âœ… ë¸Œë¼ìš°ì € ì„¤ì • ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ğŸ’¥ ë¸Œë¼ìš°ì € ì„¤ì • ì‹¤íŒ¨: {e}")
            raise
    
    async def close_browser(self):
        """ë¸Œë¼ìš°ì € ì¢…ë£Œ"""
        try:
            if self.page:
                await self.page.close()
                self.page = None
                
            if self.browser:
                contexts = self.browser.contexts
                for context in contexts:
                    try:
                        await context.close()
                    except:
                        pass
                
                await self.browser.close()
                self.browser = None
                
            await asyncio.sleep(0.5)
            logger.info("âœ… ë¸Œë¼ìš°ì € ì¢…ë£Œ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âš ï¸ ë¸Œë¼ìš°ì € ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: {e}")
            try:
                if self.browser:
                    await self.browser.close()
            except:
                pass
    
    def extract_post_list(self, html_content: str) -> List[Dict]:
        """
        ê²Œì‹œê¸€ ëª©ë¡ ì¶”ì¶œ
        
        Args:
            html_content: ê²Œì‹œê¸€ ëª©ë¡ HTML
            
        Returns:
            List[Dict]: ê²Œì‹œê¸€ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            posts = []
            
            # tbody ë‚´ì˜ tr ìš”ì†Œë“¤ì„ ì°¾ìŒ
            rows = soup.find_all('tr')
            
            for row in rows:
                # ê³µì§€ê¸€ì€ ê±´ë„ˆë›°ê¸° (notice í´ë˜ìŠ¤ê°€ ìˆëŠ” ê²½ìš°)
                if 'notice' in row.get('class', []):
                    continue
                
                # ê²Œì‹œê¸€ ì •ë³´ ì¶”ì¶œ
                post_data = self._extract_post_from_row(row)
                if post_data:
                    posts.append(post_data)
            
            logger.info(f"ğŸ“‹ ê²Œì‹œê¸€ ëª©ë¡ ì¶”ì¶œ ì™„ë£Œ: {len(posts)}ê°œ")
            return posts
            
        except Exception as e:
            logger.error(f"ğŸ’¥ ê²Œì‹œê¸€ ëª©ë¡ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    def _extract_post_from_row(self, row) -> Optional[Dict]:
        """
        ê°œë³„ ê²Œì‹œê¸€ í–‰ì—ì„œ ì •ë³´ ì¶”ì¶œ
        
        Args:
            row: BeautifulSoup í–‰ ìš”ì†Œ
            
        Returns:
            Optional[Dict]: ê²Œì‹œê¸€ ì •ë³´ ë˜ëŠ” None
        """
        try:
            # ì œëª©ê³¼ URL ì¶”ì¶œ
            title_cell = row.find('td', class_='title')
            if not title_cell:
                return None
            
            title_link = title_cell.find('a')
            if not title_link:
                return None
            
            title = title_link.get_text(strip=True)
            url = title_link.get('href', '')
            
            # ì‘ì„±ì ì¶”ì¶œ
            author_cell = row.find('td', class_='author')
            author = ''
            is_admin = False
            
            if author_cell:
                author_link = author_cell.find('a')
                if author_link:
                    author = author_link.get_text(strip=True)
                    # ê´€ë¦¬ì ì—¬ë¶€ í™•ì¸ (admin.png ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²½ìš°)
                    admin_img = author_link.find('img', src=lambda x: x and 'admin.png' in x)
                    is_admin = admin_img is not None
            
            # ì¡°íšŒìˆ˜ ì¶”ì¶œ
            view_count = 0
            view_cells = row.find_all('td', class_='m_no')
            if len(view_cells) >= 1:
                view_text = view_cells[0].get_text(strip=True)
                try:
                    view_count = int(view_text) if view_text.isdigit() else 0
                except:
                    view_count = 0
            
            # ì¶”ì²œìˆ˜ ì¶”ì¶œ (ë‘ ë²ˆì§¸ m_no ì…€)
            like_count = 0
            if len(view_cells) >= 2:
                like_text = view_cells[1].get_text(strip=True)
                try:
                    like_count = int(like_text) if like_text.isdigit() else 0
                except:
                    like_count = 0
            
            # ëŒ“ê¸€ìˆ˜ ì¶”ì¶œ
            comment_count = 0
            comment_link = title_cell.find('a', class_='replyNum')
            if comment_link:
                comment_text = comment_link.get_text(strip=True)
                try:
                    comment_count = int(comment_text) if comment_text.isdigit() else 0
                except:
                    comment_count = 0
            
            # ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
            category_cell = row.find('td', class_='cate')
            category = ''
            if category_cell:
                category_link = category_cell.find('a')
                if category_link:
                    category = category_link.get_text(strip=True)
            
            return {
                'title': title,
                'url': url,
                'author': author,
                'is_admin': is_admin,
                'view_count': view_count,
                'like_count': like_count,
                'comment_count': comment_count,
                'category': category
            }
            
        except Exception as e:
            logger.error(f"ğŸ’¥ ê²Œì‹œê¸€ í–‰ íŒŒì‹± ì‹¤íŒ¨: {e}")
            return None
    
    def filter_admin_posts(self, posts: List[Dict]) -> List[Dict]:
        """
        ê´€ë¦¬ì ê²Œì‹œê¸€ í•„í„°ë§ (ê³µì§€ê¸€ ë“± ì œì™¸)
        
        Args:
            posts: ê²Œì‹œê¸€ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            List[Dict]: í•„í„°ë§ëœ ê²Œì‹œê¸€ ë¦¬ìŠ¤íŠ¸
        """
        try:
            # ê´€ë¦¬ìê°€ ì‘ì„±í•œ ê²Œì‹œê¸€ê³¼ ê³µì§€ ì¹´í…Œê³ ë¦¬ ì œì™¸
            filtered_posts = [
                post for post in posts 
                if not post.get('is_admin', False) and post.get('category', '') != 'ê³µì§€'
            ]
            
            logger.info(f"ğŸ” ê´€ë¦¬ì ê²Œì‹œê¸€ í•„í„°ë§: {len(posts)}ê°œ â†’ {len(filtered_posts)}ê°œ")
            return filtered_posts
            
        except Exception as e:
            logger.error(f"ğŸ’¥ ê´€ë¦¬ì ê²Œì‹œê¸€ í•„í„°ë§ ì‹¤íŒ¨: {e}")
            return posts
    
    def select_top_posts(self, posts: List[Dict]) -> Dict[str, List[Dict]]:
        """
        ìƒìœ„ ê²Œì‹œê¸€ ì„ ë³„ (ì¶”ì²œìˆ˜, ëŒ“ê¸€ìˆ˜, ì¡°íšŒìˆ˜ ê°ê° ìƒìœ„ 3ê°œ)
        
        Args:
            posts: ê²Œì‹œê¸€ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            Dict[str, List[Dict]]: ë©”íŠ¸ë¦­ë³„ ìƒìœ„ ê²Œì‹œê¸€
        """
        try:
            # ê° ë©”íŠ¸ë¦­ë³„ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ 3ê°œ ì„ ë³„
            top_by_likes = sorted(posts, key=lambda x: x.get('like_count', 0), reverse=True)[:3]
            top_by_comments = sorted(posts, key=lambda x: x.get('comment_count', 0), reverse=True)[:3]
            top_by_views = sorted(posts, key=lambda x: x.get('view_count', 0), reverse=True)[:3]
            
            result = {
                'like_count': top_by_likes,
                'comment_count': top_by_comments,
                'view_count': top_by_views
            }
            
            # ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•´ ëª¨ë“  ì„ ë³„ëœ ê²Œì‹œê¸€ì„ í•˜ë‚˜ì˜ ì„¸íŠ¸ë¡œ í•©ì¹¨
            all_selected = set()
            for posts_list in result.values():
                for post in posts_list:
                    all_selected.add(post['url'])
            
            logger.info(f"ğŸ† ìƒìœ„ ê²Œì‹œê¸€ ì„ ë³„ ì™„ë£Œ: ì´ {len(all_selected)}ê°œ (ì¤‘ë³µ ì œê±°)")
            return result
            
        except Exception as e:
            logger.error(f"ğŸ’¥ ìƒìœ„ ê²Œì‹œê¸€ ì„ ë³„ ì‹¤íŒ¨: {e}")
            return {'like_count': [], 'comment_count': [], 'view_count': []}
    
    async def scrape_post_detail(self, post_url: str) -> Dict:
        """
        ê°œë³„ ê²Œì‹œê¸€ ìƒì„¸ ìŠ¤í¬ë˜í•‘
        
        Args:
            post_url: ê²Œì‹œê¸€ URL
            
        Returns:
            Dict: ê²Œì‹œê¸€ ìƒì„¸ ë°ì´í„°
        """
        try:
            logger.info(f"ğŸ“„ ê²Œì‹œê¸€ ìƒì„¸ ìŠ¤í¬ë˜í•‘ ì‹œì‘: {post_url}")
            
            if self.page is None:
                logger.error("ğŸ’¥ ë¸Œë¼ìš°ì €ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return {}
            
            # í˜ì´ì§€ ì´ë™
            await self.page.goto(post_url, wait_until="networkidle", timeout=self.navigation_timeout)
            
            # ê²Œì‹œê¸€ ë³¸ë¬¸ ì˜ì—­ ëŒ€ê¸°
            try:
                await self.page.wait_for_selector('article, .xe_content, .rd_body', timeout=self.wait_timeout)
            except:
                logger.warning("âš ï¸ ê²Œì‹œê¸€ ë³¸ë¬¸ ì˜ì—­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            # HTML ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
            html_content = await self.page.content()
            
            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            metadata = await self._extract_post_metadata(html_content)
            
            # ë³¸ë¬¸ ë‚´ìš©ì„ ìˆœì„œëŒ€ë¡œ ì¶”ì¶œ
            content_list = await self._extract_content_in_order(html_content)
            
            # ëŒ“ê¸€ ë°ì´í„° ì¶”ì¶œ
            comments = await self._extract_comments(html_content)
            
            post_data = {
                'url': post_url,
                'scraped_at': datetime.now(self.kst).isoformat(),
                'metadata': metadata,
                'content': content_list,
                'comments': comments
            }
            
            logger.info(f"âœ… ê²Œì‹œê¸€ ìƒì„¸ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: {len(content_list)}ê°œ ì½˜í…ì¸ , {len(comments)}ê°œ ëŒ“ê¸€")
            return post_data
            
        except Exception as e:
            logger.error(f"ğŸ’¥ ê²Œì‹œê¸€ ìƒì„¸ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
            return {'url': post_url, 'error': str(e)}
    
    async def _extract_post_metadata(self, html_content: str) -> Dict:
        """ê²Œì‹œê¸€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            metadata = {}
            
            # ì œëª© ì¶”ì¶œ
            title_elem = soup.find('h1') or soup.find('.np_18px_span') or soup.find('span', class_='np_18px_span')
            if title_elem:
                metadata['title'] = title_elem.get_text(strip=True)
            
            # ì‘ì„±ì ì¶”ì¶œ
            author_elem = soup.find('.member_plate') or soup.find('a', class_='member_plate')
            if author_elem:
                # ì´ë¯¸ì§€ íƒœê·¸ ì œê±°í•˜ê³  í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
                for img in author_elem.find_all('img'):
                    img.decompose()
                metadata['author'] = author_elem.get_text(strip=True)
            
            # ì¡°íšŒìˆ˜, ì¶”ì²œìˆ˜, ëŒ“ê¸€ìˆ˜ ì¶”ì¶œ
            stats_area = soup.find('div', class_='side fr') or soup.find('.btm_area')
            if stats_area:
                # ëª¨ë“  span ìš”ì†Œì—ì„œ í†µê³„ ì •ë³´ ì¶”ì¶œ
                spans = stats_area.find_all('span')
                for span in spans:
                    span_text = span.get_text(strip=True)
                    
                    # ì¡°íšŒìˆ˜
                    if 'ì¡°íšŒ ìˆ˜' in span_text:
                        b_tag = span.find('b')
                        if b_tag:
                            try:
                                metadata['view_count'] = int(b_tag.get_text(strip=True))
                            except:
                                metadata['view_count'] = 0
                    
                    # ì¶”ì²œìˆ˜
                    elif 'ì¶”ì²œ ìˆ˜' in span_text:
                        b_tag = span.find('b')
                        if b_tag:
                            try:
                                metadata['like_count'] = int(b_tag.get_text(strip=True))
                            except:
                                metadata['like_count'] = 0
                    
                    # ëŒ“ê¸€ìˆ˜
                    elif 'ëŒ“ê¸€' in span_text:
                        b_tag = span.find('b')
                        if b_tag:
                            try:
                                metadata['comment_count'] = int(b_tag.get_text(strip=True))
                            except:
                                metadata['comment_count'] = 0
            
            return metadata
            
        except Exception as e:
            logger.error(f"ğŸ’¥ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {}
    
    async def _extract_content_in_order(self, html_content: str) -> List[Dict]:
        """ë³¸ë¬¸ ë‚´ìš©ì„ ìˆœì„œëŒ€ë¡œ ì¶”ì¶œ"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            content_list = []
            order = 0
            
            # ë³¸ë¬¸ ì˜ì—­ ì°¾ê¸°
            content_area = soup.find('article') or soup.find('.xe_content') or soup.find('.rd_body')
            if not content_area:
                return content_list
            
            # ëª¨ë“  í•˜ìœ„ ìš”ì†Œë¥¼ ìˆœì„œëŒ€ë¡œ ì²˜ë¦¬
            for element in content_area.find_all(['img', 'video', 'p', 'div', 'span']):
                content_item = None
                
                # ì´ë¯¸ì§€ ì²˜ë¦¬
                if element.name == 'img' and element.get('src'):
                    content_item = {
                        'type': 'image',
                        'order': order,
                        'src': element.get('src'),
                        'alt': element.get('alt', ''),
                        'width': element.get('width'),
                        'height': element.get('height')
                    }
                
                # ë¹„ë””ì˜¤ ì²˜ë¦¬
                elif element.name == 'video' and element.get('src'):
                    content_item = {
                        'type': 'video',
                        'order': order,
                        'src': element.get('src'),
                        'controls': element.has_attr('controls'),
                        'poster': element.get('poster')
                    }
                
                # í…ìŠ¤íŠ¸ ì²˜ë¦¬
                elif element.name in ['p', 'div', 'span']:
                    text_content = element.get_text(strip=True)
                    if text_content and len(text_content) > 0:
                        content_item = {
                            'type': 'text',
                            'order': order,
                            'content': text_content,
                            'html': str(element)
                        }
                
                if content_item:
                    content_list.append(content_item)
                    order += 1
            
            return content_list
            
        except Exception as e:
            logger.error(f"ğŸ’¥ ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    async def _extract_comments(self, html_content: str) -> List[Dict]:
        """ëŒ“ê¸€ ì¶”ì¶œ (ê³„ì¸µêµ¬ì¡° ë³´ì¡´)"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            comments = []
            
            # ëŒ“ê¸€ ì˜ì—­ ì°¾ê¸°
            comment_area = soup.find('.fdb_lst_ul') or soup.find('ul', class_='fdb_lst_ul')
            if not comment_area:
                return comments
            
            # ëª¨ë“  ëŒ“ê¸€ li ìš”ì†Œ ì°¾ê¸°
            comment_items = comment_area.find_all('li', class_='fdb_itm')
            
            for item in comment_items:
                comment_data = self._extract_single_comment(item)
                if comment_data:
                    comments.append(comment_data)
            
            return comments
            
        except Exception as e:
            logger.error(f"ğŸ’¥ ëŒ“ê¸€ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    def _extract_single_comment(self, comment_element) -> Optional[Dict]:
        """ê°œë³„ ëŒ“ê¸€ ë°ì´í„° ì¶”ì¶œ"""
        try:
            # ëŒ“ê¸€ ID ì¶”ì¶œ
            comment_id = comment_element.get('id', '')
            
            # ëŒ€ëŒ“ê¸€ ì—¬ë¶€ í™•ì¸ (margin-left ìŠ¤íƒ€ì¼ì´ë‚˜ re í´ë˜ìŠ¤ë¡œ íŒë‹¨)
            is_reply = 're' in comment_element.get('class', [])
            depth = 0
            
            if is_reply:
                style = comment_element.get('style', '')
                if 'margin-left:2%' in style:
                    depth = 1
                elif 'margin-left:4%' in style:
                    depth = 2
            
            # ì‘ì„±ì ì¶”ì¶œ
            author_elem = comment_element.find('a', class_='member_plate')
            author = ''
            if author_elem:
                # ì´ë¯¸ì§€ íƒœê·¸ ì œê±°í•˜ê³  í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
                author_copy = BeautifulSoup(str(author_elem), 'html.parser')
                for img in author_copy.find_all('img'):
                    img.decompose()
                author = author_copy.get_text(strip=True)
            
            # ì‘ì„±ì‹œê°„ ì¶”ì¶œ
            date_elem = comment_element.find('span', class_='date')
            date = date_elem.get_text(strip=True) if date_elem else ''
            
            # ëŒ“ê¸€ ë‚´ìš© ì¶”ì¶œ
            content_elem = comment_element.find('div', class_='comment-content')
            content = ''
            if content_elem:
                xe_content = content_elem.find('div', class_='xe_content')
                if xe_content:
                    content = xe_content.get_text(strip=True)
                else:
                    content = content_elem.get_text(strip=True)
            
            # ì¶”ì²œìˆ˜ ì¶”ì¶œ
            like_count = 0
            vote_elem = comment_element.find('span', class_='voted_count')
            if vote_elem:
                like_text = vote_elem.get_text(strip=True)
                try:
                    like_count = int(like_text) if like_text.isdigit() else 0
                except:
                    like_count = 0
            
            # ë² ìŠ¤íŠ¸ ëŒ“ê¸€ ì—¬ë¶€ í™•ì¸ (ì¶”ì²œìˆ˜ê°€ ë†’ê±°ë‚˜ íŠ¹ë³„í•œ í´ë˜ìŠ¤ê°€ ìˆëŠ” ê²½ìš°)
            is_best = like_count >= 10  # ì„ê³„ê°’ ì„¤ì •
            
            # ë¶€ëª¨ ëŒ“ê¸€ ID ì¶”ì¶œ (ëŒ€ëŒ“ê¸€ì¸ ê²½ìš°)
            parent_id = ''
            if is_reply:
                parent_link = comment_element.find('a', class_='findParent')
                if parent_link:
                    onclick = parent_link.get('onclick', '')
                    match = re.search(r'findComment\((\d+)\)', onclick)
                    if match:
                        parent_id = match.group(1)
            
            return {
                'id': comment_id,
                'author': author,
                'content': content,
                'date': date,
                'like_count': like_count,
                'is_reply': is_reply,
                'depth': depth,
                'parent_id': parent_id,
                'is_best': is_best
            }
            
        except Exception as e:
            logger.error(f"ğŸ’¥ ê°œë³„ ëŒ“ê¸€ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    def identify_best_comments(self, comments: List[Dict]) -> List[Dict]:
        """ë² ìŠ¤íŠ¸ ëŒ“ê¸€ ì‹ë³„ ë° í‘œì‹œ"""
        try:
            # ì´ë¯¸ is_best í•„ë“œê°€ ì„¤ì •ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ê·¸ëŒ€ë¡œ ë°˜í™˜
            # í•„ìš”ì‹œ ì¶”ê°€ ë¡œì§ êµ¬í˜„ ê°€ëŠ¥
            return comments
            
        except Exception as e:
            logger.error(f"ğŸ’¥ ë² ìŠ¤íŠ¸ ëŒ“ê¸€ ì‹ë³„ ì‹¤íŒ¨: {e}")
            return comments


async def scrape_fmkorea_posts() -> Dict:
    """
    ì—í¨ì½”ë¦¬ì•„ ê²Œì‹œê¸€ ìŠ¤í¬ë˜í•‘ ë©”ì¸ í•¨ìˆ˜
    
    Returns:
        Dict: ìŠ¤í¬ë˜í•‘ ê²°ê³¼
    """
    try:
        async with FMKoreaScraper() as scraper:
            # ì •ì¹˜ ê²Œì‹œíŒ URL
            politics_url = "https://www.fmkorea.com/index.php?mid=politics"
            
            logger.info("ğŸš€ ì—í¨ì½”ë¦¬ì•„ ìŠ¤í¬ë˜í•‘ ì‹œì‘")
            
            # ê²Œì‹œê¸€ ëª©ë¡ í˜ì´ì§€ ì´ë™
            await scraper.page.goto(politics_url, wait_until="networkidle", timeout=scraper.navigation_timeout)
            
            # ê²Œì‹œê¸€ ëª©ë¡ HTML ê°€ì ¸ì˜¤ê¸°
            html_content = await scraper.page.content()
            
            # ê²Œì‹œê¸€ ëª©ë¡ ì¶”ì¶œ
            all_posts = scraper.extract_post_list(html_content)
            
            # ê´€ë¦¬ì ê²Œì‹œê¸€ í•„í„°ë§
            filtered_posts = scraper.filter_admin_posts(all_posts)
            
            # ìƒìœ„ ê²Œì‹œê¸€ ì„ ë³„
            top_posts = scraper.select_top_posts(filtered_posts)
            
            # ì„ ë³„ëœ ê²Œì‹œê¸€ë“¤ì˜ ìƒì„¸ ì •ë³´ ìŠ¤í¬ë˜í•‘
            detailed_posts = {}
            
            for metric, posts in top_posts.items():
                detailed_posts[metric] = []
                
                for post in posts:
                    # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
                    if post['url'].startswith('/'):
                        post_url = scraper.base_url + post['url']
                    else:
                        post_url = post['url']
                    
                    # ê²Œì‹œê¸€ ìƒì„¸ ìŠ¤í¬ë˜í•‘
                    detail_data = await scraper.scrape_post_detail(post_url)
                    
                    # ê¸°ë³¸ ì •ë³´ì™€ ìƒì„¸ ì •ë³´ ë³‘í•©
                    combined_data = {**post, **detail_data}
                    detailed_posts[metric].append(combined_data)
            
            result = {
                'scraped_at': datetime.now(scraper.kst).isoformat(),
                'total_posts': len(all_posts),
                'filtered_posts': len(filtered_posts),
                'top_posts': detailed_posts,
                'success': True
            }
            
            logger.info("âœ… ì—í¨ì½”ë¦¬ì•„ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ")
            return result
            
    except Exception as e:
        logger.error(f"ğŸ’¥ ì—í¨ì½”ë¦¬ì•„ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
        return {
            'scraped_at': datetime.now().isoformat(),
            'error': str(e),
            'success': False
        }


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    result = asyncio.run(scrape_fmkorea_posts())
    print(json.dumps(result, ensure_ascii=False, indent=2)) 