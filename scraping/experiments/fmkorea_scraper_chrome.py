# ëª©ì°¨
# 1. ëª¨ë“ˆ ì„í¬íŠ¸ ë° ì„¤ì • (1-25)
# 2. Chrome ë¸Œë¼ìš°ì € ì„¤ì • (26-80)
# 3. ìŠ¤í‚¤ë§ˆ ì •ì˜ (81-200)
# 4. ì…€ë ‰í„° ì •ì˜ (201-270)
# 5. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ (271-370)
# 6. Chrome ë¸Œë¼ìš°ì € ê´€ë¦¬ (371-450)
# 7. ë°ì´í„° ì¶”ì¶œ í•¨ìˆ˜ (451-650)
# 8. ê²Œì‹œê¸€ ëª©ë¡ ê´€ë ¨ í•¨ìˆ˜ (651-750)
# 9. ë©”ì¸ ìŠ¤í¬ë˜í¼ í´ë˜ìŠ¤ (751-900)
# 10. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ë“¤ (901-1000)

import asyncio
import json
import re
import time
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Any
from urllib.parse import urljoin, urlparse
import requests
import os

import pytz
from playwright.async_api import Browser, Page, ElementHandle, async_playwright
import jsonschema

# Chrome ë¸Œë¼ìš°ì € ì„¤ì • í´ë˜ìŠ¤
class ChromeBrowserConfig:
    """Chrome ë¸Œë¼ìš°ì € ì „ìš© ì„¤ì •"""
    
    def __init__(self):
        # ì‹¤ì œ Chrome User-Agentë“¤
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        ]
        
        # Chrome ë¸Œë¼ìš°ì € ì‹¤í–‰ ì˜µì…˜
        self.chrome_args = [
            '--no-sandbox',
            '--disable-setuid-sandbox',
            '--disable-dev-shm-usage',
            '--disable-accelerated-2d-canvas',
            '--no-first-run',
            '--no-zygote',
            '--disable-gpu',
            '--disable-background-timer-throttling',
            '--disable-backgrounding-occluded-windows',
            '--disable-renderer-backgrounding',
            '--disable-features=TranslateUI',
            '--disable-ipc-flooding-protection',
            '--disable-web-security',
            '--disable-features=VizDisplayCompositor',
            '--disable-extensions',
            '--disable-plugins',
            '--disable-images',  # ì´ë¯¸ì§€ ë¡œë”© ë¹„í™œì„±í™”ë¡œ ì†ë„ í–¥ìƒ
            '--disable-javascript',  # í•„ìš”ì‹œ í™œì„±í™”
            '--window-size=1920,1080',
            '--start-maximized'
        ]
        
        # Chrome ì±„ë„ ì˜µì…˜ (Playwrightì—ì„œ ì§€ì›í•˜ëŠ” Chrome ì±„ë„)
        self.chrome_channels = ['chrome', 'chrome-beta', 'chrome-dev', 'chrome-canary', 'msedge', 'msedge-beta', 'msedge-dev']
        
    def get_chrome_config(self, headless: bool = True) -> Dict:
        """Chrome ë¸Œë¼ìš°ì € ì„¤ì • ë°˜í™˜"""
        return {
            'headless': headless,
            'args': self.chrome_args,
            'channel': 'chrome',  # ì‹¤ì œ Chrome ì‚¬ìš©
            'ignore_default_args': ['--enable-automation'],
            'slow_mo': 1000,  # 1ì´ˆ ì§€ì—°ìœ¼ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ ë™ì‘
        }

# JSON ìŠ¤í‚¤ë§ˆ ì •ì˜ (ê¸°ì¡´ê³¼ ë™ì¼)
POST_SCHEMA = {
    "type": "object",
    "properties": {
        "post_id": {"type": "string"},
        "post_url": {"type": "string"},
        "metadata": {
            "type": "object",
            "properties": {
                "title": {"type": "string"},
                "author": {"type": "string"},
                "date": {"type": "string"},
                "view_count": {"type": "integer"},
                "up_count": {"type": "integer"},
                "down_count": {"type": "integer"},
                "comment_count": {"type": "integer"},
                "category": {"type": "string"}
            },
            "required": ["title", "author", "date", "view_count", "up_count", "down_count", "comment_count"]
        },
        "content": {
            "type": "array",
            "items": {
                "type": "object",
                "properties": {
                    "type": {"enum": ["text", "image", "video"]},
                    "order": {"type": "integer"},
                    "data": {
                        "type": "object",
                        "properties": {
                            "text": {"type": "string"},
                            "src": {"type": "string"},
                            "href": {"type": "string"},
                            "alt": {"type": "string"},
                            "width": {"type": "string"},
                            "height": {"type": "string"},
                            "autoplay": {"type": "boolean"},
                            "muted": {"type": "boolean"}
                        }
                    }
                },
                "required": ["type", "order", "data"]
            }
        },
        "comments": {
            "type": "array",
            "items": {
                "type": "object",
                "properties": {
                    "comment_id": {"type": "string"},
                    "content": {"type": "string"},
                    "author": {"type": "string"},
                    "date": {"type": "string"},
                    "media": {"type": "array"},
                    "level": {"type": "integer"},
                    "is_reply": {"type": "boolean"},
                    "parent_comment_id": {"type": "string"},
                    "up_count": {"type": "integer"},
                    "down_count": {"type": "integer"}
                },
                "required": ["comment_id", "content", "author", "date", "media", "level", "is_reply", "parent_comment_id", "up_count", "down_count"]
            }
        },
        "scraped_at": {"type": "string"}
    },
    "required": ["post_id", "post_url", "metadata", "content", "comments", "scraped_at"]
}

# FMKorea ì…€ë ‰í„° ì •ì˜ (ê¸°ì¡´ê³¼ ë™ì¼)
FMKOREA_SELECTORS = {
    "metadata": {
        "title": "h1.np_18px .np_18px_span",
        "author": ".btm_area .side a.member_plate",
        "date": ".top_area .date",
        "view_count": ".btm_area .side.fr span:nth-child(1) b",
        "up_count": ".btm_area .side.fr span:nth-child(2) b",
        "down_count": None,
        "comment_count": ".btm_area .side.fr span:nth-child(3) b",
        "category": ".cate a"
    },
    "content": {
        "container": ".xe_content",
        "text": "p:not(:has(img)):not(:has(a.highslide))",
        "image": "img, a.highslide img",
        "video": "video"
    },
    "comments": {
        "container": "ul.fdb_lst_ul li.fdb_itm",
        "comment_id": "[id^='comment_']",
        "content": ".comment-content .xe_content",
        "author": ".meta a.member_plate",
        "date": ".meta .date",
        "media": ".comment-content img, .comment-content video",
        "level": None,
        "is_reply": None,
        "parent_comment_id": None,
        "up_count": ".vote .voted_count",
        "down_count": ".vote .blamed_count"
    },
    "post_list": {
        "container": "table.bd_lst tbody tr",
        "title_link": "td.title a",
        "author": "td.author a.member_plate",
        "date": "td.time",
        "view_count": "td.m_no:nth-last-child(2)",
        "up_count": "td.m_no:last-child",
        "comment_count_in_title": "td.title .comment_count"
    }
}

# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ (ê¸°ì¡´ê³¼ ë™ì¼)
def parse_post_id(url: str) -> str:
    """URLì—ì„œ ê²Œì‹œê¸€ ID ì¶”ì¶œ"""
    parsed = urlparse(url)
    post_id = parsed.path.strip('/')
    return post_id

def validate_data(data: Dict, schema: Dict) -> bool:
    """JSON Schemaë¡œ ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬"""
    try:
        jsonschema.validate(data, schema)
        return True
    except jsonschema.ValidationError as e:
        print(f"ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨: {e}")
        return False

def clean_text(text: str) -> str:
    """í…ìŠ¤íŠ¸ ì •ë¦¬ (ê³µë°±, íŠ¹ìˆ˜ë¬¸ì ì œê±°)"""
    if not text:
        return ""
    return re.sub(r'\s+', ' ', text.strip())

def parse_number(text: str) -> int:
    """ë¬¸ìì—´ì—ì„œ ìˆ«ì ì¶”ì¶œ"""
    if not text:
        return 0
    numbers = re.findall(r'\d+', text.replace(',', ''))
    return int(numbers[0]) if numbers else 0

def parse_date(date_str: str) -> str:
    """ë‚ ì§œ ë¬¸ìì—´ íŒŒì‹± ë° ISO í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    if not date_str:
        return datetime.now(pytz.timezone('Asia/Seoul')).isoformat()
    
    date_str = clean_text(date_str)
    
    try:
        if ':' in date_str and '.' not in date_str:
            today = datetime.now(pytz.timezone('Asia/Seoul')).date()
            time_str = date_str
            datetime_str = f"{today.strftime('%Y.%m.%d')} {time_str}"
        else:
            datetime_str = date_str
            
        dt = datetime.strptime(datetime_str, '%Y.%m.%d %H:%M')
        dt = pytz.timezone('Asia/Seoul').localize(dt)
        return dt.isoformat()
    except ValueError:
        return datetime.now(pytz.timezone('Asia/Seoul')).isoformat()

# Chrome ë¸Œë¼ìš°ì € ê´€ë¦¬ í•¨ìˆ˜ë“¤
async def setup_chrome_browser(config: Optional[Dict] = None) -> Tuple[Browser, Page]:
    """Chrome ë¸Œë¼ìš°ì € ì´ˆê¸°í™” (ì‹¤ì œ Chrome ì‚¬ìš©)"""
    if config is None:
        config = {}
    
    chrome_config = ChromeBrowserConfig()
    playwright = await async_playwright().start()
    
    print("ğŸ”§ Chrome ë¸Œë¼ìš°ì € ì„¤ì • ì¤‘...")
    
    # Chrome ì±„ë„ ì‹œë„
    browser = None
    for channel in chrome_config.chrome_channels:
        try:
            print(f"  ğŸŒ {channel} ì±„ë„ ì‹œë„ ì¤‘...")
            browser_config = chrome_config.get_chrome_config(config.get('headless', False))
            browser_config['channel'] = channel
            
            browser = await playwright.chromium.launch(**browser_config)
            print(f"  âœ… {channel} ë¸Œë¼ìš°ì € ì‹¤í–‰ ì„±ê³µ!")
            break
        except Exception as e:
            print(f"  âŒ {channel} ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            continue
    
    # ëª¨ë“  ì±„ë„ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ Chromium ì‚¬ìš© (Chrome ì„¤ì • ì ìš©)
    if not browser:
        print("  ğŸ”„ ê¸°ë³¸ Chromiumìœ¼ë¡œ Chrome ëª¨ë°© ì‹œë„...")
        try:
            browser_config = chrome_config.get_chrome_config(config.get('headless', False))
            del browser_config['channel']  # ì±„ë„ ì œê±°
            browser = await playwright.chromium.launch(**browser_config)
            print("  âœ… Chromium (Chrome ëª¨ë°©) ì‹¤í–‰ ì„±ê³µ!")
        except Exception as e:
            print(f"  âŒ Chromium ì‹¤í–‰ë„ ì‹¤íŒ¨: {e}")
            raise
    
    # ìƒˆ í˜ì´ì§€ ìƒì„±
    page = await browser.new_page()
    
    # Chromeê³¼ ë™ì¼í•œ User-Agent ë° í—¤ë” ì„¤ì •
    import random
    user_agent = random.choice(chrome_config.user_agents)
    
    await page.set_extra_http_headers({
        'User-Agent': user_agent,
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
        'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'DNT': '1',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Sec-Fetch-Dest': 'document',
        'Sec-Fetch-Mode': 'navigate',
        'Sec-Fetch-Site': 'none',
        'Sec-Fetch-User': '?1',
        'sec-ch-ua': '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
        'sec-ch-ua-mobile': '?0',
        'sec-ch-ua-platform': '"Windows"'
    })
    
    # ë·°í¬íŠ¸ ì„¤ì •
    await page.set_viewport_size({"width": 1920, "height": 1080})
    
    print(f"  ğŸ‘¤ User-Agent: {user_agent}")
    print("  ğŸ¯ Chrome ë¸Œë¼ìš°ì € ì„¤ì • ì™„ë£Œ!")
    
    return browser, page

async def close_chrome_browser(browser: Browser):
    """Chrome ë¸Œë¼ìš°ì € ì¢…ë£Œ"""
    await browser.close()

async def navigate_to_page_chrome(page: Page, url: str, config: Optional[Dict] = None) -> bool:
    """Chrome ë¸Œë¼ìš°ì €ë¡œ í˜ì´ì§€ ì´ë™"""
    if config is None:
        config = {}
        
    try:
        print(f"ğŸŒ í˜ì´ì§€ ì´ë™ ì¤‘: {url}")
        
        # ì‹¤ì œ ì‚¬ìš©ìì²˜ëŸ¼ ìì—°ìŠ¤ëŸ¬ìš´ ì´ë™
        await page.goto(url, 
                       wait_until='networkidle', 
                       timeout=config.get('timeout', 60000))
        
        # ì¶”ê°€ ëŒ€ê¸° ì‹œê°„ (ìì—°ìŠ¤ëŸ¬ìš´ ë™ì‘)
        await asyncio.sleep(config.get('wait_time', 3))
        
        # í˜ì´ì§€ ë¡œë”© í™•ì¸
        title = await page.title()
        print(f"  ğŸ“„ í˜ì´ì§€ ì œëª©: {title}")
        
        return True
    except Exception as e:
        print(f"âŒ í˜ì´ì§€ ì´ë™ ì‹¤íŒ¨: {e}")
        return False

# ë°ì´í„° ì¶”ì¶œ í•¨ìˆ˜ë“¤ (ê¸°ì¡´ê³¼ ë™ì¼í•˜ì§€ë§Œ Chrome ìµœì í™”)
async def extract_text_content_chrome(element: ElementHandle) -> str:
    """Chrome ë¸Œë¼ìš°ì €ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    try:
        html_content = await element.evaluate("""
            element => {
                const clone = element.cloneNode(true);
                
                const brElements = clone.querySelectorAll('br');
                brElements.forEach(br => {
                    br.replaceWith('\\n');
                });
                
                const blockElements = clone.querySelectorAll('p, div');
                blockElements.forEach(block => {
                    if (block.nextSibling) {
                        block.insertAdjacentText('afterend', '\\n');
                    }
                });
                
                return clone.textContent || clone.innerText || '';
            }
        """)
        
        if html_content:
            text = re.sub(r'\n{3,}', '\n\n', html_content)
            text = text.strip()
            return text
        else:
            return ""
    except Exception as e:
        print(f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        try:
            return await element.evaluate("element => element.textContent || ''")
        except:
            return ""

async def extract_attribute_chrome(element: ElementHandle, attr: str) -> str:
    """Chrome ë¸Œë¼ìš°ì €ì—ì„œ ì†ì„±ê°’ ì¶”ì¶œ"""
    try:
        value = await element.get_attribute(attr)
        return value if value else ""
    except Exception:
        return ""

# ê²Œì‹œê¸€ ëª©ë¡ ê´€ë ¨ í•¨ìˆ˜ë“¤ (Chrome ìµœì í™”)
async def extract_post_list_chrome(page: Page, selectors: Dict) -> List[Dict]:
    """Chrome ë¸Œë¼ìš°ì €ë¡œ ê²Œì‹œê¸€ ëª©ë¡ ì¶”ì¶œ"""
    posts = []
    post_selectors = selectors.get("post_list", {})
    
    try:
        print("ğŸ“‹ ê²Œì‹œê¸€ ëª©ë¡ ì¶”ì¶œ ì¤‘...")
        
        # ê²Œì‹œê¸€ ëª©ë¡ ì»¨í…Œì´ë„ˆë“¤ ì°¾ê¸°
        post_elements = await page.query_selector_all(post_selectors.get("container", ""))
        print(f"  ğŸ“ ë°œê²¬ëœ ê²Œì‹œê¸€ ìˆ˜: {len(post_elements)}")
        
        for i, element in enumerate(post_elements):
            if i >= 10:  # ìµœëŒ€ 10ê°œë§Œ ì²˜ë¦¬
                break
                
            post_data = {}
            
            # ì œëª© ë° URL ì¶”ì¶œ
            title_link_element = await element.query_selector(post_selectors.get("title_link", ""))
            if title_link_element:
                title = await extract_text_content_chrome(title_link_element)
                url = await extract_attribute_chrome(title_link_element, "href")
                
                if url and url.startswith('/'):
                    url = "https://www.fmkorea.com" + url
                    
                post_data["title"] = clean_text(title)
                post_data["url"] = url
                post_data["post_id"] = parse_post_id(url) if url else ""
            else:
                continue
                
            # ì‘ì„±ì ì¶”ì¶œ
            author_element = await element.query_selector(post_selectors.get("author", ""))
            if author_element:
                post_data["author"] = await extract_text_content_chrome(author_element)
            else:
                post_data["author"] = ""
                
            # ì‘ì„±ì¼ ì¶”ì¶œ
            date_element = await element.query_selector(post_selectors.get("date", ""))
            if date_element:
                date_text = await extract_text_content_chrome(date_element)
                post_data["date"] = parse_date(date_text)
            else:
                post_data["date"] = datetime.now(pytz.timezone('Asia/Seoul')).isoformat()
                
            # ì¡°íšŒìˆ˜ ì¶”ì¶œ
            view_element = await element.query_selector(post_selectors.get("view_count", ""))
            if view_element:
                view_text = await extract_text_content_chrome(view_element)
                post_data["view_count"] = parse_number(view_text)
            else:
                post_data["view_count"] = 0
                
            # ì¶”ì²œìˆ˜ ì¶”ì¶œ
            up_element = await element.query_selector(post_selectors.get("up_count", ""))
            if up_element:
                up_text = await extract_text_content_chrome(up_element)
                post_data["up_count"] = parse_number(up_text)
            else:
                post_data["up_count"] = 0
                
            post_data["down_count"] = 0
            
            # ëŒ“ê¸€ìˆ˜ ì¶”ì¶œ
            comment_element = await element.query_selector(post_selectors.get("comment_count_in_title", ""))
            if comment_element:
                comment_text = await extract_text_content_chrome(comment_element)
                post_data["comment_count"] = parse_number(comment_text)
            else:
                title_text = post_data.get("title", "")
                comment_match = re.search(r'\[(\d+)\]', title_text)
                post_data["comment_count"] = int(comment_match.group(1)) if comment_match else 0
                
            posts.append(post_data)
            print(f"  âœ… ê²Œì‹œê¸€ {i+1}: {post_data['title'][:30]}...")
            
    except Exception as e:
        print(f"ê²Œì‹œê¸€ ëª©ë¡ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        
    return posts

class FMKoreaChromeScraper:
    """FMKorea Chrome ìŠ¤í¬ë˜í¼ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.selectors = FMKOREA_SELECTORS
        self.schema = POST_SCHEMA
        
    async def extract_metadata_chrome(self, page: Page) -> Dict:
        """Chrome ë¸Œë¼ìš°ì €ë¡œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        metadata = {}
        selectors = self.selectors["metadata"]
        
        try:
            print("ğŸ“Š ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì¤‘...")
            
            # ì œëª© ì¶”ì¶œ
            title_element = await page.query_selector(selectors["title"])
            if title_element:
                metadata["title"] = await extract_text_content_chrome(title_element)
            else:
                metadata["title"] = ""
                
            # ì‘ì„±ì ì¶”ì¶œ
            author_element = await page.query_selector(selectors["author"])
            if author_element:
                metadata["author"] = await extract_text_content_chrome(author_element)
            else:
                metadata["author"] = ""
                
            # ì‘ì„±ì¼ ì¶”ì¶œ
            date_element = await page.query_selector(selectors["date"])
            if date_element:
                date_text = await extract_text_content_chrome(date_element)
                metadata["date"] = parse_date(date_text)
            else:
                metadata["date"] = datetime.now(pytz.timezone('Asia/Seoul')).isoformat()
                
            # ì¡°íšŒìˆ˜, ì¶”ì²œìˆ˜, ëŒ“ê¸€ìˆ˜ë¥¼ í…ìŠ¤íŠ¸ë¡œ ì°¾ê¸°
            side_fr_element = await page.query_selector(".btm_area .side.fr")
            if side_fr_element:
                side_text = await extract_text_content_chrome(side_fr_element)
                
                view_match = re.search(r'ì¡°íšŒ\s*ìˆ˜\s*(\d+)', side_text)
                metadata["view_count"] = int(view_match.group(1)) if view_match else 0
                
                up_match = re.search(r'ì¶”ì²œ\s*ìˆ˜\s*(\d+)', side_text)
                metadata["up_count"] = int(up_match.group(1)) if up_match else 0
                
                comment_match = re.search(r'ëŒ“ê¸€\s*(\d+)', side_text)
                metadata["comment_count"] = int(comment_match.group(1)) if comment_match else 0
            else:
                metadata["view_count"] = 0
                metadata["up_count"] = 0
                metadata["comment_count"] = 0
                
            metadata["down_count"] = 0
                
            # ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
            category_element = await page.query_selector(selectors["category"])
            if category_element:
                metadata["category"] = await extract_text_content_chrome(category_element)
            else:
                metadata["category"] = ""
                
            print(f"  ğŸ“„ ì œëª©: {metadata['title'][:50]}...")
            print(f"  ğŸ‘¤ ì‘ì„±ì: {metadata['author']}")
            print(f"  ğŸ‘€ ì¡°íšŒìˆ˜: {metadata['view_count']}")
            print(f"  ğŸ‘ ì¶”ì²œìˆ˜: {metadata['up_count']}")
                
        except Exception as e:
            print(f"ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            metadata = {
                "title": "",
                "author": "",
                "date": datetime.now(pytz.timezone('Asia/Seoul')).isoformat(),
                "view_count": 0,
                "up_count": 0,
                "down_count": 0,
                "comment_count": 0,
                "category": ""
            }
            
        return metadata
        
    async def scrape_post_chrome(self, url: str, config: Optional[Dict] = None) -> Dict:
        """Chrome ë¸Œë¼ìš°ì €ë¡œ ê²Œì‹œê¸€ ìŠ¤í¬ë˜í•‘"""
        if config is None:
            config = {}
            
        browser, page = await setup_chrome_browser(config)
        
        try:
            # í˜ì´ì§€ ì´ë™
            success = await navigate_to_page_chrome(page, url, config)
            if not success:
                raise Exception("í˜ì´ì§€ ì´ë™ ì‹¤íŒ¨")
                
            # ë°ì´í„° ì¶”ì¶œ
            post_id = parse_post_id(url)
            metadata = await self.extract_metadata_chrome(page)
            
            # ê°„ë‹¨í•œ ì½˜í…ì¸  ì¶”ì¶œ (ì œëª©ë§Œ)
            content = [{
                "type": "text",
                "order": 0,
                "data": {"text": f"ì œëª©: {metadata.get('title', '')}\n\n(Chrome ë¸Œë¼ìš°ì €ë¡œ ìŠ¤í¬ë˜í•‘ë¨)"}
            }]
            
            # ê²°ê³¼ êµ¬ì„±
            result = {
                "post_id": post_id,
                "post_url": url,
                "metadata": metadata,
                "content": content,
                "comments": [],  # ê°„ë‹¨ ë²„ì „ì—ì„œëŠ” ëŒ“ê¸€ ì œì™¸
                "scraped_at": datetime.now(pytz.timezone('Asia/Seoul')).isoformat()
            }
            
            return result
                
        except Exception as e:
            print(f"ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜: {e}")
            return {}
        finally:
            await close_chrome_browser(browser)

# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ë“¤
async def scrape_fmkorea_chrome_test(config: Optional[Dict] = None) -> List[Dict]:
    """FMKorea Chrome ë¸Œë¼ìš°ì € í…ŒìŠ¤íŠ¸"""
    if config is None:
        config = {'headless': False}  # Chrome í…ŒìŠ¤íŠ¸ëŠ” headless=Falseë¡œ
        
    print("ğŸš€ FMKorea Chrome ë¸Œë¼ìš°ì € í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    browser, page = await setup_chrome_browser(config)
    results = []
    
    try:
        # 1. ì—í¨ì½”ë¦¬ì•„ ì •ì¹˜ ê²Œì‹œíŒ ì ‘ì† í…ŒìŠ¤íŠ¸
        page_url = "https://www.fmkorea.com/politics"
        success = await navigate_to_page_chrome(page, page_url, config)
        
        if not success:
            print("âŒ ì—í¨ì½”ë¦¬ì•„ ì ‘ì† ì‹¤íŒ¨")
            return []
        
        print("âœ… ì—í¨ì½”ë¦¬ì•„ ì ‘ì† ì„±ê³µ!")
        
        # 2. ê²Œì‹œê¸€ ëª©ë¡ ì¶”ì¶œ
        posts = await extract_post_list_chrome(page, FMKOREA_SELECTORS)
        
        if not posts:
            print("âŒ ê²Œì‹œê¸€ ëª©ë¡ ì¶”ì¶œ ì‹¤íŒ¨")
            return []
        
        print(f"ğŸ“‹ ê²Œì‹œê¸€ ëª©ë¡ ì¶”ì¶œ ì„±ê³µ: {len(posts)}ê°œ")
        
        # 3. ìƒìœ„ 3ê°œ ê²Œì‹œê¸€ ìƒì„¸ ìŠ¤í¬ë˜í•‘
        scraper = FMKoreaChromeScraper()
        
        for i, post in enumerate(posts[:3]):
            print(f"\nğŸ“„ ê²Œì‹œê¸€ {i+1}/{min(3, len(posts))} ìŠ¤í¬ë˜í•‘ ì¤‘...")
            print(f"  ì œëª©: {post.get('title', 'N/A')[:50]}...")
            
            try:
                scraped_data = await scraper.scrape_post_chrome(post["url"], config)
                if scraped_data:
                    results.append(scraped_data)
                    print(f"  âœ… ì„±ê³µ!")
                else:
                    print(f"  âŒ ì‹¤íŒ¨")
            except Exception as e:
                print(f"  âŒ ì˜¤ë¥˜: {e}")
                
            # ìš”ì²­ ê°„ ê°„ê²©
            if i < min(2, len(posts) - 1):
                await asyncio.sleep(3)
        
        # 4. ê²°ê³¼ ì €ì¥
        if results:
            timestamp = int(time.time())
            file_name = f"fmkorea-chrome-test-{timestamp}.json"
            
            data_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), "data")
            os.makedirs(data_dir, exist_ok=True)
            filepath = os.path.join(data_dir, file_name)
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            print(f"\nğŸ’¾ ê²°ê³¼ê°€ {filepath}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            # API ì „ì†¡ ì‹œë„
            api_url = config.get('api_url', 'http://localhost:3000/api/scraping-data')
            try:
                response = requests.post(api_url, json=results, timeout=10)
                if response.status_code == 200:
                    print(f"âœ… API ì „ì†¡ ì„±ê³µ: {len(results)}ê°œ ê²Œì‹œê¸€")
                else:
                    print(f"âš ï¸ API ì „ì†¡ ì‹¤íŒ¨: {response.status_code}")
            except Exception as e:
                print(f"âš ï¸ API ì „ì†¡ ì˜¤ë¥˜: {e}")
        
        print(f"\nğŸ‰ Chrome ë¸Œë¼ìš°ì € í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print(f"ğŸ“Š ìµœì¢… ê²°ê³¼: {len(results)}ê°œ ê²Œì‹œê¸€ ì„±ê³µ")
        
        return results
        
    except Exception as e:
        print(f"ğŸ’¥ Chrome í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        return []
        
    finally:
        await close_chrome_browser(browser)

# ë©”ì¸ ì‹¤í–‰
async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸŒŸ FMKorea Chrome ìŠ¤í¬ë˜í¼ ì‹œì‘")
    
    config = {
        'headless': False,  # Chrome ë¸Œë¼ìš°ì € ì°½ í‘œì‹œ
        'timeout': 60000,
        'wait_time': 3,
        'api_url': 'http://localhost:3000/api/scraping-data'
    }
    
    result = await scrape_fmkorea_chrome_test(config)
    
    if result:
        print("âœ… Chrome ìŠ¤í¬ë˜í•‘ í…ŒìŠ¤íŠ¸ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ")
    else:
        print("âŒ Chrome ìŠ¤í¬ë˜í•‘ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
    
    return result

if __name__ == "__main__":
    asyncio.run(main()) 