#!/usr/bin/env python3
"""
ìŠ¤í¬ë˜í•‘ ì‹¤í—˜ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ (v3 ìŠ¤í¬ë˜í¼ í™œìš©)

ì£¼ìš” ê¸°ëŠ¥:
- fmkorea_scraper_v3.pyì™€ ruliweb_scraper_v3.py í™œìš© (line 20-50)
- ë³‘ë ¬ ìŠ¤í¬ë˜í•‘ ì‹¤í–‰ ë° ê²°ê³¼ í†µí•© (line 52-100)
- ì—ëŸ¬ ì²˜ë¦¬ ë° ì¬ì‹œë„ ë¡œì§ ê°•í™” (line 102-150)
- API ì „ì†¡ ë° íŒŒì¼ ì €ì¥ (line 152-200)

ì‘ì„±ì: AI Assistant
ì‘ì„±ì¼: 2025ë…„ 6ì›” 4ì¼ 22:47 (KST)
ëª©ì : v3 ìŠ¤í¬ë˜í¼ë¥¼ í™œìš©í•œ ì•ˆì •ì ì¸ ìŠ¤í¬ë˜í•‘ ì‹¤í—˜ ì‹¤í–‰
"""

import asyncio
import json
import os
import sys
import time
from datetime import datetime
from typing import List, Dict, Optional, Any
import pytz
from loguru import logger

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(project_root)

# v3 ìŠ¤í¬ë˜í¼ ì„í¬íŠ¸
try:
    from fmkorea_scraper_v3 import scrape_fmkorea_politics_page
    from ruliweb_scraper_v3 import scrape_ruliweb_politics_page
    logger.info("âœ… v3 ìŠ¤í¬ë˜í¼ ëª¨ë“ˆ ì„í¬íŠ¸ ì„±ê³µ")
except ImportError as e:
    logger.error(f"âŒ v3 ìŠ¤í¬ë˜í¼ ëª¨ë“ˆ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
    sys.exit(1)

# í•œêµ­ ì‹œê°„ëŒ€ ì„¤ì •
KST = pytz.timezone('Asia/Seoul')

class ScrapingExperimentRunner:
    """
    ìŠ¤í¬ë˜í•‘ ì‹¤í—˜ ì‹¤í–‰ í´ë˜ìŠ¤ (v3 ìŠ¤í¬ë˜í¼ í™œìš©)
    """
    
    def __init__(self, config: Optional[Dict] = None):
        self.config = config or self.get_default_config()
        self.results = []
        self.errors = []
        
        # ë¡œê¹… ì„¤ì •
        logger.remove()
        logger.add(
            sys.stdout,
            format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>",
            level="INFO"
        )
        
        # íŒŒì¼ ë¡œê¹… ì¶”ê°€
        log_dir = os.path.join(project_root, "logs")
        os.makedirs(log_dir, exist_ok=True)
        
        timestamp = datetime.now(KST).strftime('%Y%m%d_%H%M%S')
        log_file = os.path.join(log_dir, f"scraping_experiment_{timestamp}.log")
        
        logger.add(
            log_file,
            format="{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {name}:{function}:{line} - {message}",
            level="DEBUG"
        )
        
        logger.info(f"ğŸ“ ë¡œê·¸ íŒŒì¼: {log_file}")
    
    def get_default_config(self) -> Dict:
        """ê¸°ë³¸ ì„¤ì • ë°˜í™˜"""
        return {
            'headless': True,
            'slow_mo': 1000,
            'wait_time': 2,
            'delay_between_requests': 3,
            'timeout': 45000,
            'post_limit': 10,
            'api_url': 'http://localhost:3000/api/scraping-data',
            'retry_count': 3,
            'retry_delay': 5,
            'parallel_execution': True,
            'save_individual_files': True,
            'save_combined_file': True
        }
    
    async def run_fmkorea_scraping(self) -> List[Dict]:
        """FMì½”ë¦¬ì•„ ìŠ¤í¬ë˜í•‘ ì‹¤í–‰"""
        logger.info("ğŸ® FMì½”ë¦¬ì•„ ìŠ¤í¬ë˜í•‘ ì‹œì‘")
        
        try:
            results = await scrape_fmkorea_politics_page(self.config)
            
            if results:
                logger.info(f"âœ… FMì½”ë¦¬ì•„ ìŠ¤í¬ë˜í•‘ ì„±ê³µ: {len(results)}ê°œ ê²Œì‹œê¸€")
                return results
            else:
                logger.warning("âš ï¸ FMì½”ë¦¬ì•„ ìŠ¤í¬ë˜í•‘ ê²°ê³¼ ì—†ìŒ")
                return []
                
        except Exception as e:
            error_msg = f"FMì½”ë¦¬ì•„ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}"
            logger.error(f"âŒ {error_msg}")
            self.errors.append({
                'site': 'fmkorea',
                'error': error_msg,
                'timestamp': datetime.now(KST).isoformat()
            })
            return []
    
    async def run_ruliweb_scraping(self) -> List[Dict]:
        """ë£¨ë¦¬ì›¹ ìŠ¤í¬ë˜í•‘ ì‹¤í–‰"""
        logger.info("ğŸ¯ ë£¨ë¦¬ì›¹ ìŠ¤í¬ë˜í•‘ ì‹œì‘")
        
        try:
            results = await scrape_ruliweb_politics_page(self.config)
            
            if results:
                logger.info(f"âœ… ë£¨ë¦¬ì›¹ ìŠ¤í¬ë˜í•‘ ì„±ê³µ: {len(results)}ê°œ ê²Œì‹œê¸€")
                return results
            else:
                logger.warning("âš ï¸ ë£¨ë¦¬ì›¹ ìŠ¤í¬ë˜í•‘ ê²°ê³¼ ì—†ìŒ")
                return []
                
        except Exception as e:
            error_msg = f"ë£¨ë¦¬ì›¹ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}"
            logger.error(f"âŒ {error_msg}")
            self.errors.append({
                'site': 'ruliweb',
                'error': error_msg,
                'timestamp': datetime.now(KST).isoformat()
            })
            return []
    
    async def run_parallel_scraping(self) -> Dict[str, List[Dict]]:
        """ë³‘ë ¬ ìŠ¤í¬ë˜í•‘ ì‹¤í–‰"""
        logger.info("ğŸš€ ë³‘ë ¬ ìŠ¤í¬ë˜í•‘ ì‹¤í–‰ ì‹œì‘")
        
        if self.config.get('parallel_execution', True):
            # ë³‘ë ¬ ì‹¤í–‰
            tasks = [
                self.run_fmkorea_scraping(),
                self.run_ruliweb_scraping()
            ]
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            fmkorea_results = results[0] if not isinstance(results[0], Exception) else []
            ruliweb_results = results[1] if not isinstance(results[1], Exception) else []
            
            # ì˜ˆì™¸ ì²˜ë¦¬
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    site = 'fmkorea' if i == 0 else 'ruliweb'
                    error_msg = f"{site} ë³‘ë ¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {result}"
                    logger.error(f"âŒ {error_msg}")
                    self.errors.append({
                        'site': site,
                        'error': error_msg,
                        'timestamp': datetime.now(KST).isoformat()
                    })
        else:
            # ìˆœì°¨ ì‹¤í–‰
            logger.info("ğŸ”„ ìˆœì°¨ ì‹¤í–‰ ëª¨ë“œ")
            fmkorea_results = await self.run_fmkorea_scraping()
            
            # ì‚¬ì´íŠ¸ ê°„ ì§€ì—°
            await asyncio.sleep(self.config.get('delay_between_sites', 10))
            
            ruliweb_results = await self.run_ruliweb_scraping()
        
        return {
            'fmkorea': fmkorea_results,
            'ruliweb': ruliweb_results
        }
    
    def save_results(self, results: Dict[str, List[Dict]]) -> Dict[str, str]:
        """ê²°ê³¼ ì €ì¥"""
        logger.info("ğŸ’¾ ê²°ê³¼ ì €ì¥ ì‹œì‘")
        
        saved_files = {}
        timestamp = datetime.now(KST).strftime('%Y%m%d_%H%M%S')
        
        # ë°ì´í„° í´ë” ìƒì„±
        data_dir = os.path.join(project_root, "data")
        os.makedirs(data_dir, exist_ok=True)
        
        try:
            # ê°œë³„ íŒŒì¼ ì €ì¥
            if self.config.get('save_individual_files', True):
                for site, site_results in results.items():
                    if site_results:
                        filename = f"{site}_politics_experiment_{timestamp}.json"
                        filepath = os.path.join(data_dir, filename)
                        
                        with open(filepath, 'w', encoding='utf-8') as f:
                            json.dump(site_results, f, ensure_ascii=False, indent=2)
                        
                        saved_files[site] = filepath
                        logger.info(f"âœ… {site} ê²°ê³¼ ì €ì¥: {filepath}")
            
            # í†µí•© íŒŒì¼ ì €ì¥
            if self.config.get('save_combined_file', True):
                all_results = []
                for site_results in results.values():
                    all_results.extend(site_results)
                
                if all_results:
                    combined_filename = f"combined_politics_experiment_{timestamp}.json"
                    combined_filepath = os.path.join(data_dir, combined_filename)
                    
                    with open(combined_filepath, 'w', encoding='utf-8') as f:
                        json.dump(all_results, f, ensure_ascii=False, indent=2)
                    
                    saved_files['combined'] = combined_filepath
                    logger.info(f"âœ… í†µí•© ê²°ê³¼ ì €ì¥: {combined_filepath}")
            
            return saved_files
            
        except Exception as e:
            logger.error(f"âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            return {}
    
    async def send_to_api(self, results: Dict[str, List[Dict]]) -> bool:
        """APIë¡œ ê²°ê³¼ ì „ì†¡"""
        api_url = self.config.get('api_url')
        if not api_url:
            logger.info("âš ï¸ API URLì´ ì„¤ì •ë˜ì§€ ì•ŠìŒ, API ì „ì†¡ ê±´ë„ˆëœ€")
            return False
        
        logger.info(f"ğŸŒ APIë¡œ ê²°ê³¼ ì „ì†¡: {api_url}")
        
        try:
            import requests
            
            # ëª¨ë“  ê²°ê³¼ í†µí•©
            all_results = []
            for site_results in results.values():
                all_results.extend(site_results)
            
            if not all_results:
                logger.warning("âš ï¸ ì „ì†¡í•  ê²°ê³¼ê°€ ì—†ìŒ")
                return False
            
            response = requests.post(
                api_url, 
                json=all_results, 
                timeout=30,
                headers={'Content-Type': 'application/json'}
            )
            
            if response.status_code == 200:
                logger.info(f"âœ… API ì „ì†¡ ì„±ê³µ: {len(all_results)}ê°œ ê²Œì‹œê¸€")
                return True
            else:
                logger.error(f"âŒ API ì „ì†¡ ì‹¤íŒ¨: {response.status_code} - {response.text}")
                return False
                
        except Exception as e:
            logger.error(f"âŒ API ì „ì†¡ ì˜¤ë¥˜: {e}")
            return False
    
    def generate_summary(self, results: Dict[str, List[Dict]]) -> Dict:
        """ì‹¤í—˜ ê²°ê³¼ ìš”ì•½ ìƒì„±"""
        total_posts = sum(len(site_results) for site_results in results.values())
        
        summary = {
            'experiment_info': {
                'timestamp': datetime.now(KST).isoformat(),
                'config': self.config,
                'total_sites': len(results),
                'total_posts': total_posts,
                'errors_count': len(self.errors)
            },
            'site_results': {},
            'errors': self.errors
        }
        
        for site, site_results in results.items():
            summary['site_results'][site] = {
                'posts_count': len(site_results),
                'success': len(site_results) > 0,
                'sample_titles': [
                    post.get('metadata', {}).get('title', 'N/A')[:50] + '...'
                    for post in site_results[:3]
                ]
            }
        
        return summary
    
    async def run_experiment(self) -> Dict:
        """ì „ì²´ ì‹¤í—˜ ì‹¤í–‰"""
        logger.info("ğŸ§ª ìŠ¤í¬ë˜í•‘ ì‹¤í—˜ ì‹œì‘")
        start_time = time.time()
        
        try:
            # ë³‘ë ¬ ìŠ¤í¬ë˜í•‘ ì‹¤í–‰
            results = await self.run_parallel_scraping()
            
            # ê²°ê³¼ ì €ì¥
            saved_files = self.save_results(results)
            
            # API ì „ì†¡
            api_success = await self.send_to_api(results)
            
            # ìš”ì•½ ìƒì„±
            summary = self.generate_summary(results)
            summary['saved_files'] = saved_files
            summary['api_success'] = api_success
            summary['execution_time'] = time.time() - start_time
            
            # ìµœì¢… ê²°ê³¼ ì¶œë ¥
            total_posts = sum(len(site_results) for site_results in results.values())
            logger.info(f"ğŸ‰ ì‹¤í—˜ ì™„ë£Œ!")
            logger.info(f"ğŸ“Š ì´ {total_posts}ê°œ ê²Œì‹œê¸€ ìˆ˜ì§‘")
            logger.info(f"â±ï¸ ì‹¤í–‰ ì‹œê°„: {summary['execution_time']:.2f}ì´ˆ")
            logger.info(f"ğŸ’¾ ì €ì¥ëœ íŒŒì¼: {len(saved_files)}ê°œ")
            logger.info(f"ğŸŒ API ì „ì†¡: {'ì„±ê³µ' if api_success else 'ì‹¤íŒ¨'}")
            
            if self.errors:
                logger.warning(f"âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {len(self.errors)}ê°œ")
                for error in self.errors:
                    logger.warning(f"  - {error['site']}: {error['error']}")
            
            return summary
            
        except Exception as e:
            logger.error(f"ğŸ’¥ ì‹¤í—˜ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {
                'success': False,
                'error': str(e),
                'execution_time': time.time() - start_time
            }


# í¸ì˜ í•¨ìˆ˜
async def run_scraping_experiment(config: Optional[Dict] = None) -> Dict:
    """ìŠ¤í¬ë˜í•‘ ì‹¤í—˜ ì‹¤í–‰ í¸ì˜ í•¨ìˆ˜"""
    runner = ScrapingExperimentRunner(config)
    return await runner.run_experiment()


# ë©”ì¸ ì‹¤í–‰
async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("ğŸš€ ìŠ¤í¬ë˜í•‘ ì‹¤í—˜ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ ì‹œì‘")
    
    # ì„¤ì •
    config = {
        'headless': True,
        'slow_mo': 1000,
        'wait_time': 3,
        'delay_between_requests': 4,
        'timeout': 60000,
        'post_limit': 10,
        'api_url': 'http://localhost:3000/api/scraping-data',
        'retry_count': 3,
        'parallel_execution': True,
        'save_individual_files': True,
        'save_combined_file': True
    }
    
    # ì‹¤í—˜ ì‹¤í–‰
    result = await run_scraping_experiment(config)
    
    # ê²°ê³¼ ì¶œë ¥
    if result.get('success', True):
        logger.info("âœ… ìŠ¤í¬ë˜í•‘ ì‹¤í—˜ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ")
    else:
        logger.error("âŒ ìŠ¤í¬ë˜í•‘ ì‹¤í—˜ ì‹¤íŒ¨")
    
    return result


if __name__ == "__main__":
    asyncio.run(main()) 