#!/usr/bin/env python3
"""
ë„¤íŠ¸ì›Œí¬ ì°¨ë‹¨ ìš°íšŒ ìŠ¤í¬ë˜í¼ (ì—í¨ì½”ë¦¬ì•„ + ë£¨ë¦¬ì›¹)

ëª©ì°¨:
1. ëª¨ë“ˆ ì„í¬íŠ¸ ë° ì„¤ì • (1-50)
2. ë„¤íŠ¸ì›Œí¬ ìš°íšŒ ì„¤ì • í´ë˜ìŠ¤ (51-150)
3. ë¸Œë¼ìš°ì € ì„¤ì • ìµœì í™” (151-250)
4. ì—í¨ì½”ë¦¬ì•„ ìŠ¤í¬ë˜í¼ (251-400)
5. ë£¨ë¦¬ì›¹ ìŠ¤í¬ë˜í¼ (401-550)
6. í†µí•© ì‹¤í–‰ í•¨ìˆ˜ (551-650)

ì‘ì„±ì: AI Assistant
ì‘ì„±ì¼: 2025ë…„ 6ì›” 5ì¼ 11:25 (KST)
ëª©ì : ê³µê³µ ë„ì„œê´€ ë“± ì œí•œëœ ë„¤íŠ¸ì›Œí¬ í™˜ê²½ì—ì„œ ìŠ¤í¬ë˜í•‘ ê°€ëŠ¥í•˜ë„ë¡ ìµœì í™”
"""

import asyncio
import json
import re
import time
import random
from datetime import datetime
from typing import List, Dict, Optional, Any
from urllib.parse import urljoin, urlparse
import pytz
import requests
import os
import sys

from playwright.async_api import async_playwright, Page, Browser
from playwright_stealth import stealth_async
from loguru import logger

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.append(project_root)

class NetworkBypassConfig:
    """ë„¤íŠ¸ì›Œí¬ ìš°íšŒ ì„¤ì • í´ë˜ìŠ¤"""
    
    def __init__(self):
        # ì‹¤ì œ ë¸Œë¼ìš°ì €ì™€ ë™ì¼í•œ User-Agentë“¤
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0'
        ]
        
        # ë‹¤ì–‘í•œ ì ‘ê·¼ URL (HTTP ìš°ì„ )
        self.fmkorea_urls = [
            'http://www.fmkorea.com',
            'http://m.fmkorea.com',
            'https://www.fmkorea.com',
            'https://m.fmkorea.com'
        ]
        
        self.ruliweb_urls = [
            'http://bbs.ruliweb.com',
            'https://bbs.ruliweb.com'
        ]
        
        # ë¸Œë¼ìš°ì € ì„¤ì •ë“¤
        self.browser_configs = [
            {
                'name': 'bypass_security',
                'headless': False,  # í•­ìƒ headless=False
                'args': [
                    '--no-sandbox',
                    '--disable-dev-shm-usage',
                    '--disable-blink-features=AutomationControlled',
                    '--disable-web-security',
                    '--ignore-certificate-errors',
                    '--ignore-ssl-errors',
                    '--ignore-certificate-errors-spki-list',
                    '--allow-running-insecure-content',
                    '--disable-extensions',
                    '--disable-plugins',
                    '--disable-images',  # ì´ë¯¸ì§€ ë¡œë”© ë¹„í™œì„±í™”ë¡œ ì†ë„ í–¥ìƒ
                    '--disable-javascript',  # í•„ìš”ì‹œ JavaScript ë¹„í™œì„±í™”
                    '--window-size=1920,1080',
                    '--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
                ]
            },
            {
                'name': 'minimal_security',
                'headless': False,
                'args': [
                    '--no-sandbox',
                    '--disable-dev-shm-usage',
                    '--disable-web-security',
                    '--ignore-certificate-errors',
                    '--allow-running-insecure-content',
                    '--window-size=1920,1080'
                ]
            },
            {
                'name': 'basic_bypass',
                'headless': False,
                'args': [
                    '--no-sandbox',
                    '--disable-dev-shm-usage',
                    '--window-size=1920,1080'
                ]
            }
        ]

class NetworkBypassScraper:
    """ë„¤íŠ¸ì›Œí¬ ì°¨ë‹¨ ìš°íšŒ ìŠ¤í¬ë˜í¼"""
    
    def __init__(self):
        self.config = NetworkBypassConfig()
        self.browser: Optional[Browser] = None
        self.page: Optional[Page] = None
        self.playwright = None
        self.kst = pytz.timezone('Asia/Seoul')
        
        # ì„±ê³µí•œ ì„¤ì • ê¸°ë¡
        self.successful_config = None
        self.successful_fmkorea_url = None
        self.successful_ruliweb_url = None
        
        # ë¡œê¹… ì„¤ì •
        logger.remove()
        logger.add(
            sys.stdout,
            format="<green>{time:HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{function}</cyan> - <level>{message}</level>",
            level="INFO"
        )
    
    async def setup_browser(self, config_name: str = None) -> bool:
        """ë¸Œë¼ìš°ì € ì„¤ì • (íŠ¹ì • ì„¤ì • ë˜ëŠ” ìˆœì°¨ ì‹œë„)"""
        configs_to_try = [config_name] if config_name else [c['name'] for c in self.config.browser_configs]
        
        for config_name in configs_to_try:
            config = next((c for c in self.config.browser_configs if c['name'] == config_name), None)
            if not config:
                continue
                
            try:
                logger.info(f"ğŸ”§ ë¸Œë¼ìš°ì € ì„¤ì • ì‹œë„: {config['name']}")
                
                # ì´ì „ ë¸Œë¼ìš°ì € ì •ë¦¬
                await self.close_browser()
                
                self.playwright = await async_playwright().start()
                
                # ë¸Œë¼ìš°ì € ì‹¤í–‰
                self.browser = await self.playwright.chromium.launch(
                    headless=config['headless'],
                    args=config['args']
                )
                
                # ìƒˆ í˜ì´ì§€ ìƒì„±
                self.page = await self.browser.new_page()
                
                # Stealth ëª¨ë“œ ì ìš©
                await stealth_async(self.page)
                
                # ë·°í¬íŠ¸ ì„¤ì •
                await self.page.set_viewport_size({"width": 1920, "height": 1080})
                
                # User-Agent ì„¤ì •
                user_agent = random.choice(self.config.user_agents)
                await self.page.set_extra_http_headers({
                    'User-Agent': user_agent,
                    'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                    'Accept-Encoding': 'gzip, deflate, br',
                    'DNT': '1',
                    'Connection': 'keep-alive',
                    'Upgrade-Insecure-Requests': '1'
                })
                
                self.successful_config = config
                logger.info(f"âœ… ë¸Œë¼ìš°ì € ì„¤ì • ì„±ê³µ: {config['name']}")
                return True
                
            except Exception as e:
                logger.warning(f"âš ï¸ ë¸Œë¼ìš°ì € ì„¤ì • {config['name']} ì‹¤íŒ¨: {e}")
                await self.close_browser()
                continue
        
        logger.error("âŒ ëª¨ë“  ë¸Œë¼ìš°ì € ì„¤ì • ì‹¤íŒ¨")
        return False
    
    async def test_fmkorea_connection(self) -> str:
        """ì—í¨ì½”ë¦¬ì•„ ì—°ê²° í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸ” ì—í¨ì½”ë¦¬ì•„ ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        for url in self.config.fmkorea_urls:
            try:
                test_url = f"{url}/politics"
                logger.info(f"ğŸ§ª í…ŒìŠ¤íŠ¸ URL: {test_url}")
                
                response = await self.page.goto(
                    test_url,
                    wait_until="domcontentloaded",
                    timeout=30000
                )
                
                if response and response.status == 200:
                    title = await self.page.title()
                    logger.info(f"ğŸ“„ í˜ì´ì§€ ì œëª©: {title}")
                    
                    if title and ('fmkorea' in title.lower() or 'ì •ì¹˜' in title or 'fmì½”ë¦¬ì•„' in title):
                        self.successful_fmkorea_url = url
                        logger.info(f"âœ… ì—í¨ì½”ë¦¬ì•„ ì—°ê²° ì„±ê³µ: {url}")
                        return url
                
                await asyncio.sleep(2)
                
            except Exception as e:
                logger.warning(f"âš ï¸ {url} ì—°ê²° ì‹¤íŒ¨: {str(e)[:100]}...")
                continue
        
        logger.error("âŒ ì—í¨ì½”ë¦¬ì•„ ëª¨ë“  URL ì—°ê²° ì‹¤íŒ¨")
        return ""
    
    async def test_ruliweb_connection(self) -> str:
        """ë£¨ë¦¬ì›¹ ì—°ê²° í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸ” ë£¨ë¦¬ì›¹ ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        for url in self.config.ruliweb_urls:
            try:
                test_url = f"{url}/community/board/300148"
                logger.info(f"ğŸ§ª í…ŒìŠ¤íŠ¸ URL: {test_url}")
                
                response = await self.page.goto(
                    test_url,
                    wait_until="domcontentloaded",
                    timeout=30000
                )
                
                if response and response.status == 200:
                    title = await self.page.title()
                    logger.info(f"ğŸ“„ í˜ì´ì§€ ì œëª©: {title}")
                    
                    if title and ('ruliweb' in title.lower() or 'ë£¨ë¦¬ì›¹' in title or 'ì •ì¹˜' in title):
                        self.successful_ruliweb_url = url
                        logger.info(f"âœ… ë£¨ë¦¬ì›¹ ì—°ê²° ì„±ê³µ: {url}")
                        return url
                
                await asyncio.sleep(2)
                
            except Exception as e:
                logger.warning(f"âš ï¸ {url} ì—°ê²° ì‹¤íŒ¨: {str(e)[:100]}...")
                continue
        
        logger.error("âŒ ë£¨ë¦¬ì›¹ ëª¨ë“  URL ì—°ê²° ì‹¤íŒ¨")
        return ""
    
    async def scrape_fmkorea_simple(self, limit: int = 5) -> List[Dict]:
        """ì—í¨ì½”ë¦¬ì•„ ê°„ë‹¨ ìŠ¤í¬ë˜í•‘"""
        if not self.successful_fmkorea_url:
            logger.error("âŒ ì—í¨ì½”ë¦¬ì•„ ì—°ê²° URLì´ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        try:
            url = f"{self.successful_fmkorea_url}/politics"
            logger.info(f"ğŸ“‹ ì—í¨ì½”ë¦¬ì•„ ê²Œì‹œê¸€ ëª©ë¡ ìŠ¤í¬ë˜í•‘: {url}")
            
            await self.page.goto(url, wait_until="networkidle", timeout=45000)
            await self.page.wait_for_timeout(3000)
            
            # ê²Œì‹œê¸€ ëª©ë¡ ì¶”ì¶œ (ê°„ë‹¨í•œ ë²„ì „)
            posts = []
            
            # í…Œì´ë¸” í–‰ë“¤ ì°¾ê¸°
            rows = await self.page.query_selector_all("table.bd_lst tbody tr")
            logger.info(f"ğŸ“ ë°œê²¬ëœ í–‰ ìˆ˜: {len(rows)}")
            
            for i, row in enumerate(rows[:limit]):
                try:
                    # ì œëª© ì¶”ì¶œ
                    title_element = await row.query_selector("td.title a")
                    if not title_element:
                        continue
                    
                    title = await title_element.inner_text()
                    href = await title_element.get_attribute('href')
                    
                    if not title or not href:
                        continue
                    
                    # ì ˆëŒ€ URLë¡œ ë³€í™˜
                    if href.startswith('/'):
                        post_url = urljoin(self.successful_fmkorea_url, href)
                    else:
                        post_url = href
                    
                    # ê²Œì‹œê¸€ ID ì¶”ì¶œ
                    post_id = self.parse_post_id_from_url(post_url)
                    
                    # ì‘ì„±ì ì¶”ì¶œ
                    author_element = await row.query_selector("td.author")
                    author = await author_element.inner_text() if author_element else ""
                    
                    # ì¡°íšŒìˆ˜ ì¶”ì¶œ
                    view_element = await row.query_selector("td.m_no")
                    view_count = 0
                    if view_element:
                        view_text = await view_element.inner_text()
                        view_count = int(view_text) if view_text.isdigit() else 0
                    
                    post_data = {
                        "post_id": post_id,
                        "post_url": post_url,
                        "scraped_at": datetime.now(self.kst).isoformat(),
                        "metadata": {
                            "title": title.strip(),
                            "category": "ì •ì¹˜",
                            "author": author.strip(),
                            "date": datetime.now(self.kst).isoformat(),
                            "view_count": view_count,
                            "up_count": 0,
                            "down_count": 0,
                            "comment_count": 0
                        },
                        "content": [
                            {
                                "type": "text",
                                "order": 0,
                                "data": {"text": f"ì œëª©: {title}\n\n(ë³¸ë¬¸ ë‚´ìš©ì€ ê°œë³„ ìŠ¤í¬ë˜í•‘ì´ í•„ìš”í•©ë‹ˆë‹¤)"}
                            }
                        ],
                        "comments": []
                    }
                    
                    posts.append(post_data)
                    logger.info(f"âœ… ê²Œì‹œê¸€ {i+1}: {title[:30]}...")
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ ê²Œì‹œê¸€ {i+1} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    continue
            
            logger.info(f"ğŸ“Š ì—í¨ì½”ë¦¬ì•„ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: {len(posts)}ê°œ")
            return posts
            
        except Exception as e:
            logger.error(f"ğŸ’¥ ì—í¨ì½”ë¦¬ì•„ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
            return []
    
    async def scrape_ruliweb_simple(self, limit: int = 5) -> List[Dict]:
        """ë£¨ë¦¬ì›¹ ê°„ë‹¨ ìŠ¤í¬ë˜í•‘"""
        if not self.successful_ruliweb_url:
            logger.error("âŒ ë£¨ë¦¬ì›¹ ì—°ê²° URLì´ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        try:
            url = f"{self.successful_ruliweb_url}/community/board/300148"
            logger.info(f"ğŸ“‹ ë£¨ë¦¬ì›¹ ê²Œì‹œê¸€ ëª©ë¡ ìŠ¤í¬ë˜í•‘: {url}")
            
            await self.page.goto(url, wait_until="networkidle", timeout=45000)
            await self.page.wait_for_timeout(3000)
            
            # ê²Œì‹œê¸€ ëª©ë¡ ì¶”ì¶œ (ê°„ë‹¨í•œ ë²„ì „)
            posts = []
            
            # í…Œì´ë¸” í–‰ë“¤ ì°¾ê¸°
            rows = await self.page.query_selector_all("table.board_list_table tbody tr.table_body")
            logger.info(f"ğŸ“ ë°œê²¬ëœ í–‰ ìˆ˜: {len(rows)}")
            
            for i, row in enumerate(rows[:limit]):
                try:
                    # ì œëª© ì¶”ì¶œ
                    title_element = await row.query_selector("td.subject a.subject_link")
                    if not title_element:
                        continue
                    
                    title = await title_element.inner_text()
                    href = await title_element.get_attribute('href')
                    
                    if not title or not href:
                        continue
                    
                    # ì ˆëŒ€ URLë¡œ ë³€í™˜
                    if href.startswith('/'):
                        post_url = urljoin(self.successful_ruliweb_url, href)
                    else:
                        post_url = href
                    
                    # ê²Œì‹œê¸€ ID ì¶”ì¶œ
                    post_id = self.parse_post_id_from_url(post_url)
                    
                    # ì‘ì„±ì ì¶”ì¶œ
                    author_element = await row.query_selector("td.writer a")
                    author = await author_element.inner_text() if author_element else ""
                    
                    # ì¡°íšŒìˆ˜ ì¶”ì¶œ
                    hit_element = await row.query_selector("td.hit")
                    hit_count = 0
                    if hit_element:
                        hit_text = await hit_element.inner_text()
                        hit_count = int(hit_text) if hit_text.isdigit() else 0
                    
                    # ì¶”ì²œìˆ˜ ì¶”ì¶œ
                    recommend_element = await row.query_selector("td.recomd")
                    recommend_count = 0
                    if recommend_element:
                        recommend_text = await recommend_element.inner_text()
                        recommend_count = int(recommend_text) if recommend_text.isdigit() else 0
                    
                    post_data = {
                        "post_id": post_id,
                        "post_url": post_url,
                        "scraped_at": datetime.now(self.kst).isoformat(),
                        "metadata": {
                            "title": title.strip(),
                            "category": "ì •ì¹˜ìœ ë¨¸",
                            "author": author.strip(),
                            "date": datetime.now(self.kst).isoformat(),
                            "view_count": hit_count,
                            "up_count": recommend_count,
                            "down_count": 0,
                            "comment_count": 0
                        },
                        "content": [
                            {
                                "type": "text",
                                "order": 0,
                                "data": {"text": f"ì œëª©: {title}\n\n(ë³¸ë¬¸ ë‚´ìš©ì€ ê°œë³„ ìŠ¤í¬ë˜í•‘ì´ í•„ìš”í•©ë‹ˆë‹¤)"}
                            }
                        ],
                        "comments": []
                    }
                    
                    posts.append(post_data)
                    logger.info(f"âœ… ê²Œì‹œê¸€ {i+1}: {title[:30]}...")
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ ê²Œì‹œê¸€ {i+1} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    continue
            
            logger.info(f"ğŸ“Š ë£¨ë¦¬ì›¹ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: {len(posts)}ê°œ")
            return posts
            
        except Exception as e:
            logger.error(f"ğŸ’¥ ë£¨ë¦¬ì›¹ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
            return []
    
    def parse_post_id_from_url(self, url: str) -> str:
        """URLì—ì„œ ê²Œì‹œê¸€ ID ì¶”ì¶œ"""
        try:
            # ë£¨ë¦¬ì›¹ íŒ¨í„´
            if '/read/' in url:
                match = re.search(r'/read/(\d+)', url)
                if match:
                    return match.group(1)
            
            # ì—í¨ì½”ë¦¬ì•„ íŒ¨í„´
            if 'document_srl' in url:
                from urllib.parse import parse_qs, urlparse
                parsed = urlparse(url)
                params = parse_qs(parsed.query)
                if 'document_srl' in params:
                    return params['document_srl'][0]
            
            # ì¼ë°˜ì ì¸ ìˆ«ì íŒ¨í„´
            numbers = re.findall(r'\d+', url)
            if numbers:
                return max(numbers, key=len)
            
            return 'unknown'
            
        except Exception as e:
            logger.error(f"ğŸ’¥ ê²Œì‹œê¸€ ID ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return 'unknown'
    
    async def close_browser(self):
        """ë¸Œë¼ìš°ì € ì¢…ë£Œ"""
        try:
            if self.page:
                await self.page.close()
                self.page = None
                
            if self.browser:
                await self.browser.close()
                self.browser = None
                
            if self.playwright:
                await self.playwright.stop()
                self.playwright = None
                
        except Exception as e:
            logger.warning(f"âš ï¸ ë¸Œë¼ìš°ì € ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: {e}")

# í†µí•© ì‹¤í–‰ í•¨ìˆ˜
async def run_network_bypass_experiment(config: Optional[Dict] = None) -> Dict:
    """ë„¤íŠ¸ì›Œí¬ ì°¨ë‹¨ ìš°íšŒ ìŠ¤í¬ë˜í•‘ ì‹¤í—˜"""
    if config is None:
        config = {}
    
    logger.info("ğŸš€ ë„¤íŠ¸ì›Œí¬ ì°¨ë‹¨ ìš°íšŒ ìŠ¤í¬ë˜í•‘ ì‹¤í—˜ ì‹œì‘")
    start_time = time.time()
    
    scraper = NetworkBypassScraper()
    results = {
        'fmkorea': [],
        'ruliweb': [],
        'success': False,
        'errors': [],
        'execution_time': 0
    }
    
    try:
        # ë¸Œë¼ìš°ì € ì„¤ì •
        browser_success = await scraper.setup_browser()
        if not browser_success:
            results['errors'].append("ë¸Œë¼ìš°ì € ì„¤ì • ì‹¤íŒ¨")
            return results
        
        # ì—í¨ì½”ë¦¬ì•„ ì—°ê²° í…ŒìŠ¤íŠ¸
        fmkorea_url = await scraper.test_fmkorea_connection()
        if fmkorea_url:
            logger.info("ğŸ® ì—í¨ì½”ë¦¬ì•„ ìŠ¤í¬ë˜í•‘ ì‹œì‘")
            fmkorea_posts = await scraper.scrape_fmkorea_simple(limit=config.get('post_limit', 5))
            results['fmkorea'] = fmkorea_posts
        else:
            results['errors'].append("ì—í¨ì½”ë¦¬ì•„ ì—°ê²° ì‹¤íŒ¨")
        
        # ë£¨ë¦¬ì›¹ ì—°ê²° í…ŒìŠ¤íŠ¸
        ruliweb_url = await scraper.test_ruliweb_connection()
        if ruliweb_url:
            logger.info("ğŸ¯ ë£¨ë¦¬ì›¹ ìŠ¤í¬ë˜í•‘ ì‹œì‘")
            ruliweb_posts = await scraper.scrape_ruliweb_simple(limit=config.get('post_limit', 5))
            results['ruliweb'] = ruliweb_posts
        else:
            results['errors'].append("ë£¨ë¦¬ì›¹ ì—°ê²° ì‹¤íŒ¨")
        
        # ê²°ê³¼ ì €ì¥
        all_posts = results['fmkorea'] + results['ruliweb']
        if all_posts:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            
            # ë°ì´í„° í´ë” ìƒì„±
            data_dir = os.path.join(project_root, "scraping", "data")
            os.makedirs(data_dir, exist_ok=True)
            
            # í†µí•© íŒŒì¼ ì €ì¥
            combined_filename = f"network_bypass_experiment_{timestamp}.json"
            combined_filepath = os.path.join(data_dir, combined_filename)
            
            with open(combined_filepath, 'w', encoding='utf-8') as f:
                json.dump(all_posts, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {combined_filepath}")
            
            # API ì „ì†¡ ì‹œë„
            api_url = config.get('api_url', 'http://localhost:3000/api/scraping-data')
            if api_url:
                try:
                    response = requests.post(api_url, json=all_posts, timeout=30)
                    if response.status_code == 200:
                        logger.info(f"âœ… API ì „ì†¡ ì„±ê³µ: {len(all_posts)}ê°œ ê²Œì‹œê¸€")
                    else:
                        logger.warning(f"âš ï¸ API ì „ì†¡ ì‹¤íŒ¨: {response.status_code}")
                except Exception as e:
                    logger.warning(f"âš ï¸ API ì „ì†¡ ì˜¤ë¥˜: {e}")
        
        results['success'] = len(all_posts) > 0
        results['execution_time'] = time.time() - start_time
        
        # ìµœì¢… ê²°ê³¼ ì¶œë ¥
        total_posts = len(results['fmkorea']) + len(results['ruliweb'])
        logger.info(f"ğŸ‰ ì‹¤í—˜ ì™„ë£Œ!")
        logger.info(f"ğŸ“Š ì´ {total_posts}ê°œ ê²Œì‹œê¸€ ìˆ˜ì§‘")
        logger.info(f"  - ì—í¨ì½”ë¦¬ì•„: {len(results['fmkorea'])}ê°œ")
        logger.info(f"  - ë£¨ë¦¬ì›¹: {len(results['ruliweb'])}ê°œ")
        logger.info(f"â±ï¸ ì‹¤í–‰ ì‹œê°„: {results['execution_time']:.2f}ì´ˆ")
        
        if results['errors']:
            logger.warning(f"âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {len(results['errors'])}ê°œ")
            for error in results['errors']:
                logger.warning(f"  - {error}")
        
        return results
        
    except Exception as e:
        logger.error(f"ğŸ’¥ ì‹¤í—˜ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        results['errors'].append(str(e))
        results['execution_time'] = time.time() - start_time
        return results
        
    finally:
        await scraper.close_browser()

# ë©”ì¸ ì‹¤í–‰
async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("ğŸš€ ë„¤íŠ¸ì›Œí¬ ì°¨ë‹¨ ìš°íšŒ ìŠ¤í¬ë˜í¼ ì‹œì‘")
    
    config = {
        'post_limit': 5,
        'api_url': 'http://localhost:3000/api/scraping-data'
    }
    
    result = await run_network_bypass_experiment(config)
    
    if result['success']:
        logger.info("âœ… ìŠ¤í¬ë˜í•‘ ì‹¤í—˜ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ")
    else:
        logger.error("âŒ ìŠ¤í¬ë˜í•‘ ì‹¤í—˜ ì‹¤íŒ¨")
    
    return result

if __name__ == "__main__":
    asyncio.run(main()) 