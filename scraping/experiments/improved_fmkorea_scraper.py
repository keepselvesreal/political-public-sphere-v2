#!/usr/bin/env python3
"""
ê°œì„ ëœ FMì½”ë¦¬ì•„ ìŠ¤í¬ë˜í¼ ëª¨ë“ˆ

ëª©ì°¨:
1. ëª¨ë“ˆ ì„í¬íŠ¸ ë° ì„¤ì • (1-30)
2. ImprovedFMKoreaScraper í´ë˜ìŠ¤ (31-120)
3. ë¸Œë¼ìš°ì € ì„¤ì • ë° ê´€ë¦¬ (121-200)
4. ê²Œì‹œê¸€ ëª©ë¡ ìŠ¤í¬ë˜í•‘ (201-300)
5. ê°œë³„ ê²Œì‹œê¸€ ìŠ¤í¬ë˜í•‘ (301-450)
6. ë°ì´í„° ì¶”ì¶œ í•¨ìˆ˜ë“¤ (451-600)
7. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ (601-700)
8. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (701-800)

ì‘ì„±ì: AI Assistant
ì‘ì„±ì¼: 2025ë…„ 6ì›” 5ì¼ 10:52 (KST)
ëª©ì : ì œê³µëœ HTML êµ¬ì¡°ì™€ Stealth ì„¤ì •ì„ ë°˜ì˜í•œ ê°œì„ ëœ ìŠ¤í¬ë˜í¼
"""

import asyncio
import json
import re
import time
from datetime import datetime
from typing import List, Dict, Optional, Any
from urllib.parse import urljoin, urlparse
import pytz

from playwright.async_api import async_playwright, Page, Browser
from playwright_stealth import stealth_async
from loguru import logger

class ImprovedFMKoreaScraper:
    """
    ê°œì„ ëœ FMì½”ë¦¬ì•„ ìŠ¤í¬ë˜í¼ í´ë˜ìŠ¤
    
    ì£¼ìš” ê°œì„ ì‚¬í•­:
    - playwright-stealth ì ìš©
    - ì œê³µëœ HTML êµ¬ì¡° ê¸°ë°˜ ì…€ë ‰í„° ì‚¬ìš©
    - í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ í•´ì œë¡œ ë´‡ ì°¨ë‹¨ ìš°íšŒ
    - íƒ€ì„ì•„ì›ƒ ì¦ê°€ ë° ì•ˆì •ì„± ê°•í™”
    """
    
    def __init__(self):
        self.base_url = 'https://www.fmkorea.com'
        self.browser: Optional[Browser] = None
        self.page: Optional[Page] = None
        self.playwright = None
        
        # í•œêµ­ ì‹œê°„ëŒ€ ì„¤ì •
        self.kst = pytz.timezone('Asia/Seoul')
        
        # ê°œì„ ëœ ì„¤ì • (ì„±ê³µí•œ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë˜í¼ ê¸°ë°˜)
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        ]
        
        # íƒ€ì„ì•„ì›ƒ ì„¤ì • (ì¦ê°€)
        self.wait_timeout = 30000  # 30ì´ˆ
        self.navigation_timeout = 60000  # 60ì´ˆ
    
    async def __aenter__(self):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§„ì…"""
        await self.setup_browser()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ"""
        await self.close_browser()
    
    async def setup_browser(self):
        """ë¸Œë¼ìš°ì € ì„¤ì • ë° ì´ˆê¸°í™” (Stealth ëª¨ë“œ ì ìš©)"""
        try:
            self.playwright = await async_playwright().start()
            
            # ë¸Œë¼ìš°ì € ì˜µì…˜ ì„¤ì • (í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ í•´ì œ)
            self.browser = await self.playwright.chromium.launch(
                headless=False,  # í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ í•´ì œí•˜ì—¬ ë´‡ ì°¨ë‹¨ ìš°íšŒ
                args=[
                    '--no-sandbox',
                    '--disable-dev-shm-usage',
                    '--disable-blink-features=AutomationControlled',
                    '--window-size=1920,1080',
                    '--disable-web-security',
                    '--disable-features=VizDisplayCompositor'
                ]
            )
            
            # ìƒˆ í˜ì´ì§€ ìƒì„±
            self.page = await self.browser.new_page()
            
            # Stealth ëª¨ë“œ ì ìš© (ì¤‘ìš”!)
            await stealth_async(self.page)
            
            # ì¶”ê°€ ì„¤ì •
            await self.page.set_viewport_size({"width": 1920, "height": 1080})
            await self.page.set_extra_http_headers({
                'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
            })
            
            logger.info("âœ… ê°œì„ ëœ ë¸Œë¼ìš°ì € ì„¤ì • ì™„ë£Œ (Stealth ëª¨ë“œ ì ìš©)")
            
        except Exception as e:
            logger.error(f"ğŸ’¥ ë¸Œë¼ìš°ì € ì„¤ì • ì‹¤íŒ¨: {e}")
            raise
    
    async def close_browser(self):
        """ë¸Œë¼ìš°ì € ì¢…ë£Œ"""
        try:
            if self.page:
                await self.page.close()
                self.page = None
                
            if self.browser:
                await self.browser.close()
                self.browser = None
                
            if self.playwright:
                await self.playwright.stop()
                self.playwright = None
                
            logger.info("âœ… ë¸Œë¼ìš°ì € ì¢…ë£Œ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âš ï¸ ë¸Œë¼ìš°ì € ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: {e}")
    
    async def scrape_politics_page_list(self, limit: int = 20) -> List[Dict]:
        """
        FMì½”ë¦¬ì•„ ì •ì¹˜ ê²Œì‹œíŒ ëª©ë¡ ìŠ¤í¬ë˜í•‘ (ì œê³µëœ HTML êµ¬ì¡° ê¸°ë°˜)
        
        Args:
            limit: ìŠ¤í¬ë˜í•‘í•  ê²Œì‹œê¸€ ìˆ˜
        
        Returns:
            List[Dict]: ê²Œì‹œê¸€ ëª©ë¡
        """
        try:
            url = 'https://www.fmkorea.com/politics'
            logger.info(f"ğŸ” ì •ì¹˜ ê²Œì‹œíŒ ëª©ë¡ ìŠ¤í¬ë˜í•‘ ì‹œì‘: {url}")
            
            # í˜ì´ì§€ ì´ë™ (íƒ€ì„ì•„ì›ƒ ì¦ê°€)
            await self.page.goto(url, wait_until="networkidle", timeout=self.navigation_timeout)
            
            # í˜ì´ì§€ ë¡œë“œ í™•ì¸
            page_title = await self.page.title()
            logger.info(f"ğŸ“„ í˜ì´ì§€ ì œëª©: {page_title}")
            
            # ê²Œì‹œê¸€ ëª©ë¡ í…Œì´ë¸” í™•ì¸ (ì œê³µëœ HTML êµ¬ì¡°)
            table_selector = 'table.bd_lst.bd_tb_lst.bd_tb'
            table_element = await self.page.query_selector(table_selector)
            
            if not table_element:
                logger.error("âŒ ê²Œì‹œê¸€ ëª©ë¡ í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return []
            
            logger.info("âœ… ê²Œì‹œê¸€ ëª©ë¡ í…Œì´ë¸” ë°œê²¬")
            
            # ê²Œì‹œê¸€ í–‰ë“¤ ì¶”ì¶œ (ê³µì§€ì‚¬í•­ ì œì™¸)
            post_rows = await self.page.query_selector_all('table.bd_lst tbody tr:not(.notice)')
            
            if not post_rows:
                logger.error("âŒ ê²Œì‹œê¸€ í–‰ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return []
            
            logger.info(f"âœ… ê²Œì‹œê¸€ í–‰ ë°œê²¬: {len(post_rows)}ê°œ")
            
            # ê²Œì‹œê¸€ ì •ë³´ ì¶”ì¶œ
            posts = []
            for i, row in enumerate(post_rows[:limit]):
                try:
                    post_info = await self.extract_post_list_info(row)
                    if post_info:
                        posts.append(post_info)
                        logger.info(f"ğŸ“ ê²Œì‹œê¸€ {i+1}: {post_info.get('title', 'N/A')[:30]}...")
                except Exception as e:
                    logger.warning(f"âš ï¸ ê²Œì‹œê¸€ {i+1} ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                    continue
            
            logger.info(f"âœ… ì´ {len(posts)}ê°œ ê²Œì‹œê¸€ ëª©ë¡ ìˆ˜ì§‘ ì™„ë£Œ")
            return posts
            
        except Exception as e:
            logger.error(f"ğŸ’¥ ê²Œì‹œê¸€ ëª©ë¡ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
            return []
    
    async def extract_post_list_info(self, row_element) -> Optional[Dict]:
        """ê²Œì‹œê¸€ ëª©ë¡ ì •ë³´ ì¶”ì¶œ (ì œê³µëœ HTML êµ¬ì¡° ê¸°ë°˜)"""
        try:
            post_info = {}
            
            # ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
            try:
                cate_element = await row_element.query_selector('td.cate a')
                if cate_element:
                    category = await cate_element.inner_text()
                    post_info['category'] = category.strip()
                else:
                    post_info['category'] = ''
            except:
                post_info['category'] = ''
            
            # ì œëª© ë° URL ì¶”ì¶œ
            try:
                title_element = await row_element.query_selector('td.title a')
                if title_element:
                    title = await title_element.inner_text()
                    href = await title_element.get_attribute('href')
                    
                    post_info['title'] = title.strip()
                    
                    # ì ˆëŒ€ URLë¡œ ë³€í™˜
                    if href:
                        if href.startswith('/'):
                            post_info['url'] = urljoin(self.base_url, href)
                        else:
                            post_info['url'] = href
                        
                        # ê²Œì‹œê¸€ ID ì¶”ì¶œ
                        post_info['post_id'] = self.parse_post_id_from_url(post_info['url'])
                else:
                    return None
            except:
                return None
            
            # ëŒ“ê¸€ ìˆ˜ ì¶”ì¶œ (ì œëª© ì˜†ì˜ replyNum)
            try:
                reply_element = await row_element.query_selector('td.title a.replyNum')
                if reply_element:
                    reply_text = await reply_element.inner_text()
                    post_info['comment_count'] = int(reply_text) if reply_text.isdigit() else 0
                else:
                    post_info['comment_count'] = 0
            except:
                post_info['comment_count'] = 0
            
            # ì‘ì„±ì ì¶”ì¶œ
            try:
                author_element = await row_element.query_selector('td.author a.member_plate')
                if author_element:
                    author = await author_element.inner_text()
                    post_info['author'] = author.strip()
                else:
                    post_info['author'] = ''
            except:
                post_info['author'] = ''
            
            # ì‘ì„±ì‹œê°„ ì¶”ì¶œ
            try:
                time_element = await row_element.query_selector('td.time')
                if time_element:
                    time_text = await time_element.inner_text()
                    post_info['date'] = time_text.strip()
                else:
                    post_info['date'] = ''
            except:
                post_info['date'] = ''
            
            # ì¡°íšŒìˆ˜ ì¶”ì¶œ (ì²« ë²ˆì§¸ td.m_no)
            try:
                view_elements = await row_element.query_selector_all('td.m_no')
                if len(view_elements) >= 1:
                    view_text = await view_elements[0].inner_text()
                    post_info['view_count'] = int(view_text.strip()) if view_text.strip().isdigit() else 0
                else:
                    post_info['view_count'] = 0
            except:
                post_info['view_count'] = 0
            
            # ì¶”ì²œìˆ˜ ì¶”ì¶œ (ë§ˆì§€ë§‰ td.m_no)
            try:
                vote_elements = await row_element.query_selector_all('td.m_no')
                if len(vote_elements) >= 2:
                    vote_text = await vote_elements[-1].inner_text()
                    vote_text = vote_text.strip()
                    
                    # ë¹„ì¶”ì²œìˆ˜ ì²˜ë¦¬ (ìŒìˆ˜)
                    if vote_text.startswith('-'):
                        post_info['up_count'] = 0
                        post_info['down_count'] = abs(int(vote_text)) if vote_text[1:].isdigit() else 0
                    elif vote_text.isdigit():
                        post_info['up_count'] = int(vote_text)
                        post_info['down_count'] = 0
                    else:
                        post_info['up_count'] = 0
                        post_info['down_count'] = 0
                else:
                    post_info['up_count'] = 0
                    post_info['down_count'] = 0
            except:
                post_info['up_count'] = 0
                post_info['down_count'] = 0
            
            return post_info
            
        except Exception as e:
            logger.warning(f"âš ï¸ ê²Œì‹œê¸€ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    async def scrape_post_detail(self, post_url: str) -> Dict:
        """
        ê°œë³„ ê²Œì‹œê¸€ ìƒì„¸ ìŠ¤í¬ë˜í•‘ (ì‹¤í—˜ìš© ìŠ¤í¬ë˜í¼ ê¸°ë°˜)
        
        Args:
            post_url: ê²Œì‹œê¸€ URL
        
        Returns:
            Dict: ê²Œì‹œê¸€ ìƒì„¸ ë°ì´í„°
        """
        try:
            logger.info(f"ğŸ§ª ê²Œì‹œê¸€ ìƒì„¸ ìŠ¤í¬ë˜í•‘ ì‹œì‘: {post_url}")
            
            # í˜ì´ì§€ ì´ë™
            await self.page.goto(post_url, wait_until="networkidle", timeout=self.navigation_timeout)
            
            # ê²Œì‹œê¸€ ë³¸ë¬¸ ì˜ì—­ì´ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°
            try:
                await self.page.wait_for_selector('article, .xe_content, .rd_body', timeout=self.wait_timeout)
            except:
                logger.warning("âš ï¸ ê²Œì‹œê¸€ ë³¸ë¬¸ ì˜ì—­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
            
            # í˜ì´ì§€ ë¡œë“œ í™•ì¸
            page_title = await self.page.title()
            logger.info(f"ğŸ“„ í˜ì´ì§€ ì œëª©: {page_title}")
            
            # ê²Œì‹œê¸€ ID ì¶”ì¶œ
            post_id = self.parse_post_id_from_url(post_url)
            
            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            metadata = await self.extract_post_metadata()
            
            # ë³¸ë¬¸ ë‚´ìš©ì„ ìˆœì„œëŒ€ë¡œ ì¶”ì¶œ
            content_list = await self.extract_content_in_order()
            
            # ëŒ“ê¸€ ë°ì´í„° ì¶”ì¶œ
            comments = await self.extract_comments_data()
            
            # ì‹¤ì œ ì¶”ì¶œëœ ëŒ“ê¸€ ìˆ˜ë¡œ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            metadata['comment_count'] = len(comments)
            
            post_data = {
                'post_id': post_id,
                'post_url': post_url,
                'scraped_at': datetime.now(self.kst).isoformat(),
                'metadata': metadata,
                'content': content_list,
                'comments': comments,
                'scraper_version': 'improved_v1',
                'page_title': page_title
            }
            
            logger.info(f"âœ… ê²Œì‹œê¸€ ìƒì„¸ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: {len(content_list)}ê°œ ì½˜í…ì¸ , {len(comments)}ê°œ ëŒ“ê¸€")
            return post_data
            
        except Exception as e:
            logger.error(f"ğŸ’¥ ê²Œì‹œê¸€ ìƒì„¸ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
            return {
                'post_id': self.parse_post_id_from_url(post_url),
                'post_url': post_url,
                'scraped_at': datetime.now(self.kst).isoformat(),
                'error': str(e),
                'scraper_version': 'improved_v1'
            }
    
    async def extract_content_in_order(self) -> List[Dict]:
        """ê²Œì‹œê¸€ ë³¸ë¬¸ ë‚´ìš©ì„ DOM ìˆœì„œëŒ€ë¡œ ì¶”ì¶œ"""
        try:
            content_list: List[Dict] = []
            order = 1
            
            # FMì½”ë¦¬ì•„ ê²Œì‹œê¸€ ë³¸ë¬¸ ì»¨í…Œì´ë„ˆ ì°¾ê¸°
            article_selectors = [
                'article .xe_content',
                'article div[class*="document_"]',
                'div.rd_body article',
                'article'
            ]
            
            article_element = None
            for selector in article_selectors:
                try:
                    element = await self.page.query_selector(selector)
                    if element:
                        article_element = element
                        logger.info(f"âœ… ë³¸ë¬¸ ì»¨í…Œì´ë„ˆ ë°œê²¬: {selector}")
                        break
                except:
                    continue
            
            if not article_element:
                logger.warning("âš ï¸ ê²Œì‹œê¸€ ë³¸ë¬¸ ì»¨í…Œì´ë„ˆë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return []
            
            # ëª¨ë“  í•˜ìœ„ ìš”ì†Œë¥¼ ìˆœíšŒí•˜ì—¬ ì½˜í…ì¸  ì¶”ì¶œ
            child_elements = await article_element.query_selector_all('*')
            processed_elements = set()
            
            for element in child_elements:
                try:
                    # ì´ë¯¸ ì²˜ë¦¬ëœ ìš”ì†ŒëŠ” ê±´ë„ˆë›°ê¸°
                    element_id = id(element)
                    if element_id in processed_elements:
                        continue
                    processed_elements.add(element_id)
                    
                    tag_name = await element.evaluate('el => el.tagName.toLowerCase()')
                    
                    # ì´ë¯¸ì§€ ì²˜ë¦¬
                    if tag_name == 'img':
                        img_data = await self.extract_image_data(element, order)
                        if img_data:
                            content_list.append(img_data)
                            order += 1
                    
                    # ë™ì˜ìƒ ì²˜ë¦¬
                    elif tag_name == 'video':
                        video_data = await self.extract_video_data(element, order)
                        if video_data:
                            content_list.append(video_data)
                            order += 1
                    
                    # í…ìŠ¤íŠ¸ ìš”ì†Œ ì²˜ë¦¬
                    elif tag_name in ['p', 'div', 'span', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
                        # í•˜ìœ„ì— imgë‚˜ videoê°€ ì—†ëŠ” ê²½ìš°ì—ë§Œ í…ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬
                        has_media = await element.query_selector('img, video')
                        if not has_media:
                            text_content = await element.evaluate('el => el.textContent?.trim()')
                            if text_content and len(text_content.strip()) > 0:
                                text_data = await self.extract_text_data(element, order)
                                if text_data:
                                    content_list.append(text_data)
                                    order += 1
                
                except Exception as e:
                    logger.warning(f"âš ï¸ ìš”ì†Œ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    continue
            
            logger.info(f"âœ… ì´ {len(content_list)}ê°œ ì½˜í…ì¸  ìš”ì†Œ ìˆœì„œëŒ€ë¡œ ì¶”ì¶œ ì™„ë£Œ")
            return content_list
            
        except Exception as e:
            logger.error(f"ğŸ’¥ ìˆœì„œëŒ€ë¡œ ì½˜í…ì¸  ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    async def extract_image_data(self, img_element, order: int) -> Optional[Dict]:
        """ì´ë¯¸ì§€ ë°ì´í„° ì¶”ì¶œ"""
        try:
            # ì´ë¯¸ì§€ ì†ŒìŠ¤ ìš°ì„ ìˆœìœ„: data-original > src
            src = await img_element.get_attribute('data-original')
            if not src:
                src = await img_element.get_attribute('src')
            
            if not src:
                return None
            
            # ì ˆëŒ€ URLë¡œ ë³€í™˜
            if src.startswith('//'):
                src = 'https:' + src
            elif src.startswith('/'):
                src = urljoin(self.base_url, src)
            
            img_data = {
                'type': 'image',
                'order': order,
                'data': {
                    'src': src,
                    'width': await img_element.get_attribute('width'),
                    'height': await img_element.get_attribute('height'),
                    'alt': await img_element.get_attribute('alt') or '',
                    'class': await img_element.get_attribute('class') or ''
                }
            }
            
            return img_data
            
        except Exception as e:
            logger.warning(f"âš ï¸ ì´ë¯¸ì§€ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    async def extract_video_data(self, video_element, order: int) -> Optional[Dict]:
        """ë™ì˜ìƒ ë°ì´í„° ì¶”ì¶œ"""
        try:
            src = await video_element.get_attribute('src')
            if not src:
                source = await video_element.query_selector('source')
                if source:
                    src = await source.get_attribute('src')
            
            if not src:
                return None
            
            # ì ˆëŒ€ URLë¡œ ë³€í™˜
            if src.startswith('//'):
                src = 'https:' + src
            elif src.startswith('/'):
                src = urljoin(self.base_url, src)
            
            video_data = {
                'type': 'video',
                'order': order,
                'data': {
                    'src': src,
                    'poster': await video_element.get_attribute('poster') or '',
                    'width': await video_element.get_attribute('width'),
                    'height': await video_element.get_attribute('height'),
                    'autoplay': await video_element.get_attribute('autoplay') is not None,
                    'muted': await video_element.get_attribute('muted') is not None,
                    'controls': await video_element.get_attribute('controls') is not None
                }
            }
            
            return video_data
            
        except Exception as e:
            logger.warning(f"âš ï¸ ë™ì˜ìƒ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    async def extract_text_data(self, text_element, order: int) -> Optional[Dict]:
        """í…ìŠ¤íŠ¸ ë°ì´í„° ì¶”ì¶œ"""
        try:
            text_content = await text_element.evaluate('el => el.textContent?.trim()')
            if not text_content or len(text_content.strip()) == 0:
                return None
            
            tag_name = await text_element.evaluate('el => el.tagName.toLowerCase()')
            
            text_data = {
                'type': 'text',
                'order': order,
                'data': {
                    'tag': tag_name,
                    'text': text_content,
                    'class': await text_element.get_attribute('class') or ''
                }
            }
            
            return text_data
            
        except Exception as e:
            logger.warning(f"âš ï¸ í…ìŠ¤íŠ¸ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    async def extract_post_metadata(self) -> Dict:
        """ê²Œì‹œê¸€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        try:
            metadata = {}
            
            # ì œëª© ì¶”ì¶œ
            title_selectors = [
                'h1.np_18px span.np_18px_span',
                'h1.np_18px',
                'h1.title',
                'h1'
            ]
            
            for selector in title_selectors:
                try:
                    title_element = await self.page.query_selector(selector)
                    if title_element:
                        title_text = await title_element.inner_text()
                        if title_text and title_text.strip():
                            metadata['title'] = title_text.strip()
                            break
                except:
                    continue
            
            # ì‘ì„±ì ì¶”ì¶œ
            author_selectors = [
                '.btm_area .side .member_plate',
                '.member_plate',
                '.meta .member_plate'
            ]
            
            for selector in author_selectors:
                try:
                    author_element = await self.page.query_selector(selector)
                    if author_element:
                        author_text = await author_element.inner_text()
                        if author_text and author_text.strip():
                            metadata['author'] = author_text.strip()
                            break
                except:
                    continue
            
            # ì‘ì„± ì‹œê°„ ì¶”ì¶œ
            date_selectors = [
                '.top_area .date.m_no',
                '.rd_hd .date.m_no',
                '.meta .date'
            ]
            
            for selector in date_selectors:
                try:
                    date_element = await self.page.query_selector(selector)
                    if date_element:
                        date_text = await date_element.inner_text()
                        if date_text and date_text.strip():
                            metadata['date'] = date_text.strip()
                            break
                except:
                    continue
            
            # ê¸°ë³¸ê°’ ì„¤ì •
            metadata.setdefault('title', '')
            metadata.setdefault('author', '')
            metadata.setdefault('date', '')
            metadata.setdefault('view_count', 0)
            metadata.setdefault('up_count', 0)
            metadata.setdefault('down_count', 0)
            metadata.setdefault('comment_count', 0)
            metadata.setdefault('category', '')
            
            return metadata
            
        except Exception as e:
            logger.error(f"ğŸ’¥ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {}
    
    async def extract_comments_data(self) -> List[Dict]:
        """ëŒ“ê¸€ ë°ì´í„° ì¶”ì¶œ"""
        try:
            comments = []
            
            # FMì½”ë¦¬ì•„ ëŒ“ê¸€ êµ¬ì¡°ì— ë§ëŠ” ì…€ë ‰í„°ë“¤
            comment_selectors = [
                'ul.fdb_lst_ul li',
                'div.fdb_lst_ul li',
                'ul.comment_list li'
            ]
            
            comment_elements = []
            for selector in comment_selectors:
                try:
                    elements = await self.page.query_selector_all(selector)
                    if elements:
                        # ì‹¤ì œ ëŒ“ê¸€ì¸ì§€ í™•ì¸ (comment_ IDê°€ ìˆëŠ” ê²ƒë§Œ)
                        valid_elements = []
                        for element in elements:
                            element_id = await element.get_attribute('id')
                            if element_id and element_id.startswith('comment_'):
                                valid_elements.append(element)
                        
                        if valid_elements:
                            comment_elements = valid_elements
                            logger.info(f"âœ… {len(valid_elements)}ê°œ ëŒ“ê¸€ ìš”ì†Œ ë°œê²¬")
                            break
                except Exception as e:
                    continue
            
            # ê° ëŒ“ê¸€ ìš”ì†Œì—ì„œ ë°ì´í„° ì¶”ì¶œ
            for comment_element in comment_elements:
                try:
                    comment_data = await self.extract_single_comment_data(comment_element)
                    if comment_data and comment_data.get('content'):
                        comments.append(comment_data)
                except Exception as e:
                    logger.warning(f"âš ï¸ ëŒ“ê¸€ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                    continue
            
            logger.info(f"âœ… ì´ {len(comments)}ê°œ ëŒ“ê¸€ ìˆ˜ì§‘ ì™„ë£Œ")
            return comments
            
        except Exception as e:
            logger.error(f"ğŸ’¥ ëŒ“ê¸€ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    async def extract_single_comment_data(self, comment_element) -> Optional[Dict]:
        """ê°œë³„ ëŒ“ê¸€ ë°ì´í„° ì¶”ì¶œ"""
        try:
            comment_data = {}
            
            # ëŒ“ê¸€ ID
            comment_data['comment_id'] = await comment_element.get_attribute('id') or ''
            
            # ì‘ì„±ì
            author_selectors = [
                'div.side a.member_plate', 
                'div.meta a.member_plate', 
                '.member_plate'
            ]
            
            for selector in author_selectors:
                try:
                    author_element = await comment_element.query_selector(selector)
                    if author_element:
                        author_text = await author_element.inner_text()
                        if author_text and author_text.strip():
                            comment_data['author'] = author_text.strip()
                            break
                except:
                    continue
            
            comment_data.setdefault('author', 'ìµëª…')
            
            # ëŒ“ê¸€ ë‚´ìš©
            content_selectors = [
                'div.comment-content div',
                'div.comment-content',
                '.comment_content'
            ]
            
            for selector in content_selectors:
                try:
                    content_element = await comment_element.query_selector(selector)
                    if content_element:
                        content_text = await content_element.inner_text()
                        if content_text and content_text.strip():
                            comment_data['content'] = content_text.strip()
                            break
                except:
                    continue
            
            comment_data.setdefault('content', '')
            
            # ì‘ì„± ì‹œê°„
            date_selectors = [
                'div.meta span.date',
                'span.date.m_no',
                '.date'
            ]
            
            for selector in date_selectors:
                try:
                    date_element = await comment_element.query_selector(selector)
                    if date_element:
                        date_text = await date_element.inner_text()
                        if date_text and date_text.strip():
                            comment_data['date'] = date_text.strip()
                            break
                except:
                    continue
            
            comment_data.setdefault('date', '')
            
            # ëŒ“ê¸€ ë ˆë²¨ ì¶”ì¶œ (ë“¤ì—¬ì“°ê¸° ê¸°ì¤€)
            try:
                style_attr = await comment_element.get_attribute('style')
                class_attr = await comment_element.get_attribute('class')
                
                level = 0
                
                # margin-leftë¡œ ë ˆë²¨ ê³„ì‚°
                if style_attr and 'margin-left:' in style_attr:
                    margin_match = re.search(r'margin-left:(\d+)%', style_attr)
                    if margin_match:
                        margin_left = int(margin_match.group(1))
                        level = margin_left // 2  # 2%ë‹¹ 1ë ˆë²¨
                
                # í´ë˜ìŠ¤ëª…ìœ¼ë¡œë„ í™•ì¸
                if class_attr and ('re' in class_attr or 'reply' in class_attr):
                    level = max(level, 1)
                
                comment_data['level'] = level
                comment_data['is_reply'] = level > 0
                
            except:
                comment_data['level'] = 0
                comment_data['is_reply'] = False
            
            # ë¶€ëª¨ ëŒ“ê¸€ ì •ë³´
            comment_data['parent_comment_id'] = ''
            if comment_data['is_reply']:
                try:
                    parent_element = await comment_element.query_selector('a.findParent')
                    if parent_element:
                        onclick_attr = await parent_element.get_attribute('onclick')
                        if onclick_attr:
                            # onclickì—ì„œ ë¶€ëª¨ ëŒ“ê¸€ ID ì¶”ì¶œ
                            match = re.search(r'comment_(\d+)', onclick_attr)
                            if match:
                                comment_data['parent_comment_id'] = f"comment_{match.group(1)}"
                except:
                    pass
            
            # ì¶”ì²œ/ë¹„ì¶”ì²œìˆ˜
            comment_data['up_count'] = 0
            comment_data['down_count'] = 0
            
            return comment_data
            
        except Exception as e:
            logger.error(f"ğŸ’¥ ê°œë³„ ëŒ“ê¸€ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    def parse_post_id_from_url(self, url: str) -> str:
        """URLì—ì„œ ê²Œì‹œê¸€ ID ì¶”ì¶œ"""
        try:
            if '/index.php' in url:
                from urllib.parse import parse_qs, urlparse
                parsed = urlparse(url)
                params = parse_qs(parsed.query)
                if 'document_srl' in params:
                    return params['document_srl'][0]
            else:
                match = re.search(r'/(\d+)/?$', url)
                if match:
                    return match.group(1)
            
            # ìµœí›„ ìˆ˜ë‹¨: URLì—ì„œ ê°€ì¥ ê¸´ ìˆ«ì ì‹œí€€ìŠ¤ ì¶”ì¶œ
            numbers = re.findall(r'\d+', url)
            if numbers:
                return max(numbers, key=len)
            
            return 'unknown'
            
        except Exception as e:
            logger.error(f"ğŸ’¥ ê²Œì‹œê¸€ ID ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return 'unknown'

# í¸ì˜ í•¨ìˆ˜ë“¤
async def scrape_fmkorea_politics_improved(limit: int = 10) -> List[Dict]:
    """
    ê°œì„ ëœ FMì½”ë¦¬ì•„ ì •ì¹˜ ê²Œì‹œíŒ ìŠ¤í¬ë˜í•‘ í¸ì˜ í•¨ìˆ˜
    
    Args:
        limit: ìŠ¤í¬ë˜í•‘í•  ê²Œì‹œê¸€ ìˆ˜
    
    Returns:
        List[Dict]: ìŠ¤í¬ë˜í•‘ ê²°ê³¼
    """
    async with ImprovedFMKoreaScraper() as scraper:
        # ê²Œì‹œê¸€ ëª©ë¡ ìŠ¤í¬ë˜í•‘
        posts = await scraper.scrape_politics_page_list(limit)
        
        # ê° ê²Œì‹œê¸€ ìƒì„¸ ì •ë³´ ìŠ¤í¬ë˜í•‘
        detailed_posts = []
        for i, post in enumerate(posts[:3]):  # ì²˜ìŒ 3ê°œë§Œ ìƒì„¸ ìŠ¤í¬ë˜í•‘
            try:
                logger.info(f"ğŸ“– ìƒì„¸ ìŠ¤í¬ë˜í•‘ {i+1}/{min(3, len(posts))}: {post['title'][:30]}...")
                detail = await scraper.scrape_post_detail(post['url'])
                detailed_posts.append(detail)
                
                # ìš”ì²­ ê°„ ëŒ€ê¸°
                await asyncio.sleep(2)
                
            except Exception as e:
                logger.error(f"ğŸ’¥ ìƒì„¸ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
                continue
        
        return detailed_posts

async def test_improved_scraper():
    """ê°œì„ ëœ ìŠ¤í¬ë˜í¼ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    logger.info("ğŸš€ ê°œì„ ëœ FMì½”ë¦¬ì•„ ìŠ¤í¬ë˜í¼ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    logger.info(f"â° ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    try:
        results = await scrape_fmkorea_politics_improved(limit=5)
        
        logger.info("ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
        logger.info("=" * 50)
        logger.info(f"âœ… ì„±ê³µì ìœ¼ë¡œ ìŠ¤í¬ë˜í•‘ëœ ê²Œì‹œê¸€: {len(results)}ê°œ")
        
        for i, result in enumerate(results):
            if 'error' not in result:
                logger.info(f"ğŸ“ ê²Œì‹œê¸€ {i+1}: {result.get('metadata', {}).get('title', 'N/A')[:50]}...")
                logger.info(f"   - ì½˜í…ì¸ : {len(result.get('content', []))}ê°œ")
                logger.info(f"   - ëŒ“ê¸€: {len(result.get('comments', []))}ê°œ")
            else:
                logger.error(f"âŒ ê²Œì‹œê¸€ {i+1}: ì˜¤ë¥˜ ë°œìƒ - {result.get('error', 'Unknown')}")
        
        # ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"improved_fmkorea_test_{timestamp}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {filename}")
        
        return results
        
    except Exception as e:
        logger.error(f"ğŸ’¥ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return []
    
    finally:
        logger.info(f"â° ì™„ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

if __name__ == "__main__":
    asyncio.run(test_improved_scraper()) 