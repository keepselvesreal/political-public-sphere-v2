#!/usr/bin/env python3
"""
ê°œì„ ëœ FMì½”ë¦¬ì•„ ìŠ¤í¬ë˜í¼ v3 (ê³µê³µ ë„ì„œê´€ HTTPS ì°¨ë‹¨ ëŒ€ì‘)

ëª©ì°¨:
1. ëª¨ë“ˆ ì„í¬íŠ¸ ë° ì„¤ì • (1-40)
2. ë„¤íŠ¸ì›Œí¬ ìš°íšŒ ì„¤ì • (41-120)
3. FMKoreaScraper í´ë˜ìŠ¤ (121-200)
4. ë¸Œë¼ìš°ì € ì„¤ì • ë° ê´€ë¦¬ (201-350)
5. ê²Œì‹œê¸€ ëª©ë¡ ìŠ¤í¬ë˜í•‘ (351-450)
6. ê°œë³„ ê²Œì‹œê¸€ ìŠ¤í¬ë˜í•‘ (451-550)
7. ë°ì´í„° ì¶”ì¶œ í•¨ìˆ˜ë“¤ (551-650)
8. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (651-750)

ì‘ì„±ì: AI Assistant
ì‘ì„±ì¼: 2025ë…„ 6ì›” 5ì¼ 11:03 (KST)
ëª©ì : ê³µê³µ ë„ì„œê´€ HTTPS ì°¨ë‹¨ ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ë‹¤ì¤‘ ëŒ€ì•ˆ ì ìš© (API ìˆ˜ì •)
"""

import asyncio
import json
import re
import time
import random
from datetime import datetime
from typing import List, Dict, Optional, Any
from urllib.parse import urljoin, urlparse
import pytz
import requests

from playwright.async_api import async_playwright, Page, Browser
from playwright_stealth import stealth_async
from loguru import logger

class NetworkBypassConfig:
    """ë„¤íŠ¸ì›Œí¬ ìš°íšŒ ì„¤ì • í´ë˜ìŠ¤"""
    
    def __init__(self):
        # ëŒ€ì•ˆ 1: ë‹¤ì–‘í•œ User-Agent ë¡œí…Œì´ì…˜
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:109.0) Gecko/20100101 Firefox/121.0',
            # ëª¨ë°”ì¼ User-Agent ì¶”ê°€
            'Mozilla/5.0 (iPhone; CPU iPhone OS 14_7_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.2 Mobile/15E148 Safari/604.1',
            'Mozilla/5.0 (Linux; Android 10; SM-G973F) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36'
        ]
        
        # ëŒ€ì•ˆ 2: ë‹¤ì–‘í•œ ì ‘ê·¼ URL (HTTP ìš°ì„  ì‹œë„)
        self.alternative_urls = [
            'http://www.fmkorea.com',   # HTTP ë²„ì „ (ì°¨ë‹¨ ìš°íšŒ ê°€ëŠ¥ì„± ë†’ìŒ)
            'http://m.fmkorea.com',    # ëª¨ë°”ì¼ HTTP ë²„ì „
            'https://www.fmkorea.com', # HTTPS ë²„ì „
            'https://m.fmkorea.com',   # ëª¨ë°”ì¼ HTTPS ë²„ì „
        ]
        
        # ëŒ€ì•ˆ 3: ë‹¤ì–‘í•œ ë¸Œë¼ìš°ì € ì„¤ì •
        self.browser_configs = [
            {
                'name': 'http_first_chrome',
                'headless': False,
                'args': [
                    '--no-sandbox',
                    '--disable-dev-shm-usage',
                    '--disable-blink-features=AutomationControlled',
                    '--window-size=1920,1080',
                    '--disable-web-security',
                    '--disable-features=VizDisplayCompositor',
                    '--ignore-certificate-errors',
                    '--ignore-ssl-errors',
                    '--ignore-certificate-errors-spki-list',
                    '--allow-running-insecure-content',
                    '--disable-extensions'
                ]
            },
            {
                'name': 'stealth_minimal',
                'headless': True,
                'args': [
                    '--no-sandbox',
                    '--disable-dev-shm-usage',
                    '--disable-web-security',
                    '--ignore-certificate-errors',
                    '--disable-features=VizDisplayCompositor',
                    '--allow-running-insecure-content'
                ]
            },
            {
                'name': 'mobile_bypass',
                'headless': False,
                'args': [
                    '--no-sandbox',
                    '--disable-dev-shm-usage',
                    '--window-size=375,812',
                    '--disable-web-security',
                    '--ignore-certificate-errors',
                    '--allow-running-insecure-content'
                ]
            }
        ]
        
        # ëŒ€ì•ˆ 4: íƒ€ì„ì•„ì›ƒ ë° ì¬ì‹œë„ ì„¤ì •
        self.timeout_configs = [
            {'navigation': 20000, 'wait': 10000},  # ì§§ì€ íƒ€ì„ì•„ì›ƒ
            {'navigation': 45000, 'wait': 20000},  # ì¤‘ê°„ íƒ€ì„ì•„ì›ƒ
            {'navigation': 90000, 'wait': 45000}   # ê¸´ íƒ€ì„ì•„ì›ƒ
        ]

class ImprovedFMKoreaScraper:
    """ê°œì„ ëœ FMì½”ë¦¬ì•„ ìŠ¤í¬ë˜í¼ (ë„¤íŠ¸ì›Œí¬ ì°¨ë‹¨ ëŒ€ì‘)"""
    
    def __init__(self):
        self.bypass_config = NetworkBypassConfig()
        self.browser: Optional[Browser] = None
        self.page: Optional[Page] = None
        self.playwright = None
        self.kst = pytz.timezone('Asia/Seoul')
        
        # í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ì„¤ì •
        self.current_base_url = None
        self.current_config = None
        self.current_timeout = None
        self.current_user_agent = None
        
        # ì„±ê³µí•œ ì„¤ì • ê¸°ë¡
        self.successful_configs = []
    
    async def __aenter__(self):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§„ì…"""
        success = await self.setup_browser_with_alternatives()
        if not success:
            raise Exception("ëª¨ë“  ë¸Œë¼ìš°ì € ì„¤ì • ëŒ€ì•ˆì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ"""
        await self.close_browser()
    
    async def setup_browser_with_alternatives(self) -> bool:
        """ì—¬ëŸ¬ ëŒ€ì•ˆì„ ì‹œë„í•˜ì—¬ ë¸Œë¼ìš°ì € ì„¤ì •"""
        logger.info("ğŸ”§ ë„¤íŠ¸ì›Œí¬ ì°¨ë‹¨ ëŒ€ì‘ì„ ìœ„í•œ ë‹¤ì¤‘ ë¸Œë¼ìš°ì € ì„¤ì • ì‹œë„")
        
        for i, config in enumerate(self.bypass_config.browser_configs):
            try:
                logger.info(f"ğŸ§ª ë¸Œë¼ìš°ì € ì„¤ì • {i+1}/{len(self.bypass_config.browser_configs)} ì‹œë„: {config['name']}")
                
                # ì´ì „ ë¸Œë¼ìš°ì € ì •ë¦¬
                await self.close_browser()
                
                self.playwright = await async_playwright().start()
                
                # ë¸Œë¼ìš°ì € ì‹¤í–‰
                self.browser = await self.playwright.chromium.launch(
                    headless=config['headless'],
                    args=config['args']
                )
                
                # ìƒˆ í˜ì´ì§€ ìƒì„±
                self.page = await self.browser.new_page()
                
                # Stealth ëª¨ë“œ ì ìš©
                await stealth_async(self.page)
                
                # ë·°í¬íŠ¸ ì„¤ì •
                if 'mobile' in config['name']:
                    await self.page.set_viewport_size({"width": 375, "height": 812})
                else:
                    await self.page.set_viewport_size({"width": 1920, "height": 1080})
                
                # User-Agent ì„¤ì • (ì˜¬ë°”ë¥¸ ë°©ë²•)
                user_agent = random.choice(self.bypass_config.user_agents)
                self.current_user_agent = user_agent
                
                # HTTP í—¤ë” ì„¤ì • (User-Agent í¬í•¨)
                await self.page.set_extra_http_headers({
                    'User-Agent': user_agent,
                    'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                    'Accept-Encoding': 'gzip, deflate, br',
                    'DNT': '1',
                    'Connection': 'keep-alive',
                    'Upgrade-Insecure-Requests': '1',
                    'Sec-Fetch-Dest': 'document',
                    'Sec-Fetch-Mode': 'navigate',
                    'Sec-Fetch-Site': 'none',
                    'Cache-Control': 'no-cache'
                })
                
                # ì—°ê²° í…ŒìŠ¤íŠ¸
                success = await self.test_connection()
                if success:
                    self.current_config = config
                    logger.info(f"âœ… ë¸Œë¼ìš°ì € ì„¤ì • ì„±ê³µ: {config['name']}")
                    logger.info(f"ğŸŒ ì„±ê³µí•œ URL: {self.current_base_url}")
                    logger.info(f"ğŸ‘¤ ì‚¬ìš©ëœ User-Agent: {user_agent[:50]}...")
                    return True
                else:
                    logger.warning(f"âš ï¸ ë¸Œë¼ìš°ì € ì„¤ì • {config['name']} ì—°ê²° ì‹¤íŒ¨")
                    
            except Exception as e:
                logger.warning(f"âš ï¸ ë¸Œë¼ìš°ì € ì„¤ì • {config['name']} ì‹¤íŒ¨: {e}")
                await self.close_browser()
                continue
        
        logger.error("âŒ ëª¨ë“  ë¸Œë¼ìš°ì € ì„¤ì • ëŒ€ì•ˆì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        return False
    
    async def test_connection(self) -> bool:
        """ì—°ê²° í…ŒìŠ¤íŠ¸ (ì—¬ëŸ¬ URL ëŒ€ì•ˆ ì‹œë„)"""
        logger.info("ğŸ” ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        for i, base_url in enumerate(self.bypass_config.alternative_urls):
            for j, timeout_config in enumerate(self.bypass_config.timeout_configs):
                try:
                    test_url = f"{base_url}/politics"
                    logger.info(f"ğŸ§ª ì—°ê²° í…ŒìŠ¤íŠ¸ {i+1}-{j+1}: {test_url}")
                    
                    # í˜ì´ì§€ ì´ë™ ì‹œë„
                    response = await self.page.goto(
                        test_url, 
                        wait_until="domcontentloaded", 
                        timeout=timeout_config['navigation']
                    )
                    
                    # ì‘ë‹µ ìƒíƒœ í™•ì¸
                    if response and response.status == 200:
                        # í˜ì´ì§€ ë¡œë“œ í™•ì¸
                        title = await self.page.title()
                        logger.info(f"ğŸ“„ í˜ì´ì§€ ì œëª©: {title}")
                        
                        # FMì½”ë¦¬ì•„ í˜ì´ì§€ì¸ì§€ í™•ì¸
                        if title and ('fmkorea' in title.lower() or 'ì •ì¹˜' in title or 'fmì½”ë¦¬ì•„' in title):
                            self.current_base_url = base_url
                            self.current_timeout = timeout_config
                            logger.info(f"âœ… ì—°ê²° ì„±ê³µ: {test_url}")
                            return True
                        else:
                            logger.warning(f"âš ï¸ ì˜ˆìƒê³¼ ë‹¤ë¥¸ í˜ì´ì§€: {title}")
                    else:
                        status = response.status if response else 'No Response'
                        logger.warning(f"âš ï¸ HTTP ìƒíƒœ ì˜¤ë¥˜: {status}")
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ ì—°ê²° ì‹¤íŒ¨ {test_url}: {str(e)[:100]}...")
                    continue
                
                # ê° ì‹œë„ ê°„ ì§§ì€ ëŒ€ê¸°
                await asyncio.sleep(1)
        
        logger.error("âŒ ëª¨ë“  URL ëŒ€ì•ˆ ì—°ê²° ì‹¤íŒ¨")
        return False
    
    async def close_browser(self):
        """ë¸Œë¼ìš°ì € ì¢…ë£Œ"""
        try:
            if self.page:
                await self.page.close()
                self.page = None
                
            if self.browser:
                await self.browser.close()
                self.browser = None
                
            if self.playwright:
                await self.playwright.stop()
                self.playwright = None
                
        except Exception as e:
            logger.warning(f"âš ï¸ ë¸Œë¼ìš°ì € ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: {e}")
    
    async def scrape_politics_page_list(self, limit: int = 20) -> List[Dict]:
        """ì •ì¹˜ ê²Œì‹œíŒ ëª©ë¡ ìŠ¤í¬ë˜í•‘ (ì°¨ë‹¨ ëŒ€ì‘)"""
        try:
            if not self.current_base_url:
                raise Exception("ì—°ê²°ëœ ê¸°ë³¸ URLì´ ì—†ìŠµë‹ˆë‹¤.")
            
            url = f"{self.current_base_url}/politics"
            logger.info(f"ğŸ” ì •ì¹˜ ê²Œì‹œíŒ ëª©ë¡ ìŠ¤í¬ë˜í•‘: {url}")
            
            # í˜ì´ì§€ ì´ë™ (í˜„ì¬ ì„±ê³µí•œ íƒ€ì„ì•„ì›ƒ ì„¤ì • ì‚¬ìš©)
            response = await self.page.goto(
                url, 
                wait_until="networkidle", 
                timeout=self.current_timeout['navigation']
            )
            
            if not response or response.status != 200:
                logger.error(f"âŒ í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨: HTTP {response.status if response else 'No Response'}")
                return []
            
            # í˜ì´ì§€ ë¡œë“œ í™•ì¸
            page_title = await self.page.title()
            logger.info(f"ğŸ“„ í˜ì´ì§€ ì œëª©: {page_title}")
            
            # ê²Œì‹œê¸€ ëª©ë¡ í…Œì´ë¸” í™•ì¸ (ì—¬ëŸ¬ ì…€ë ‰í„° ì‹œë„)
            table_selectors = [
                'table.bd_lst.bd_tb_lst.bd_tb',
                'table.bd_lst.bd_tb_lst',
                'table.bd_lst',
                '.bd_lst'
            ]
            
            table_element = None
            for selector in table_selectors:
                try:
                    element = await self.page.query_selector(selector)
                    if element:
                        table_element = element
                        logger.info(f"âœ… ê²Œì‹œê¸€ ëª©ë¡ í…Œì´ë¸” ë°œê²¬: {selector}")
                        break
                except:
                    continue
            
            if not table_element:
                logger.error("âŒ ê²Œì‹œê¸€ ëª©ë¡ í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                # í˜ì´ì§€ ì†ŒìŠ¤ ì¼ë¶€ í™•ì¸
                content = await self.page.content()
                logger.info(f"ğŸ“„ í˜ì´ì§€ ì†ŒìŠ¤ ê¸¸ì´: {len(content)} ë¬¸ì")
                return []
            
            # ê²Œì‹œê¸€ í–‰ë“¤ ì¶”ì¶œ (ì—¬ëŸ¬ ì…€ë ‰í„° ì‹œë„)
            row_selectors = [
                'table.bd_lst tbody tr:not(.notice)',
                'table.bd_lst tr:not(.notice)',
                '.bd_lst tbody tr',
                '.bd_lst tr'
            ]
            
            post_rows = []
            for selector in row_selectors:
                try:
                    rows = await self.page.query_selector_all(selector)
                    if rows:
                        post_rows = rows
                        logger.info(f"âœ… ê²Œì‹œê¸€ í–‰ ë°œê²¬: {len(rows)}ê°œ ({selector})")
                        break
                except:
                    continue
            
            if not post_rows:
                logger.error("âŒ ê²Œì‹œê¸€ í–‰ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return []
            
            # ê²Œì‹œê¸€ ì •ë³´ ì¶”ì¶œ
            posts = []
            for i, row in enumerate(post_rows[:limit]):
                try:
                    post_info = await self.extract_post_list_info(row)
                    if post_info and post_info.get('title'):
                        posts.append(post_info)
                        logger.info(f"ğŸ“ ê²Œì‹œê¸€ {i+1}: {post_info.get('title', 'N/A')[:30]}...")
                except Exception as e:
                    logger.warning(f"âš ï¸ ê²Œì‹œê¸€ {i+1} ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                    continue
            
            logger.info(f"âœ… ì´ {len(posts)}ê°œ ê²Œì‹œê¸€ ëª©ë¡ ìˆ˜ì§‘ ì™„ë£Œ")
            return posts
            
        except Exception as e:
            logger.error(f"ğŸ’¥ ê²Œì‹œê¸€ ëª©ë¡ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
            return []
    
    async def extract_post_list_info(self, row_element) -> Optional[Dict]:
        """ê²Œì‹œê¸€ ëª©ë¡ ì •ë³´ ì¶”ì¶œ (ê°•í™”ëœ ì…€ë ‰í„°)"""
        try:
            post_info = {}
            
            # ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ (ì—¬ëŸ¬ ì…€ë ‰í„° ì‹œë„)
            category_selectors = ['td.cate a', 'td.cate span', '.cate a', '.cate span', '.category']
            for selector in category_selectors:
                try:
                    cate_element = await row_element.query_selector(selector)
                    if cate_element:
                        category = await cate_element.inner_text()
                        if category and category.strip():
                            post_info['category'] = category.strip()
                            break
                except:
                    continue
            post_info.setdefault('category', '')
            
            # ì œëª© ë° URL ì¶”ì¶œ (ì—¬ëŸ¬ ì…€ë ‰í„° ì‹œë„)
            title_selectors = ['td.title a', '.title a', 'td a[href*="/"]', 'a[href*="/"]']
            for selector in title_selectors:
                try:
                    title_element = await row_element.query_selector(selector)
                    if title_element:
                        title = await title_element.inner_text()
                        href = await title_element.get_attribute('href')
                        
                        if title and title.strip() and href:
                            post_info['title'] = title.strip()
                            
                            # ì ˆëŒ€ URLë¡œ ë³€í™˜
                            if href.startswith('/'):
                                post_info['url'] = urljoin(self.current_base_url, href)
                            else:
                                post_info['url'] = href
                            
                            # ê²Œì‹œê¸€ ID ì¶”ì¶œ
                            post_info['post_id'] = self.parse_post_id_from_url(post_info['url'])
                            break
                except:
                    continue
            
            if not post_info.get('title'):
                return None
            
            # ëŒ“ê¸€ ìˆ˜ ì¶”ì¶œ
            reply_selectors = ['td.title a.replyNum', '.replyNum', 'a[class*="reply"]']
            for selector in reply_selectors:
                try:
                    reply_element = await row_element.query_selector(selector)
                    if reply_element:
                        reply_text = await reply_element.inner_text()
                        if reply_text and reply_text.strip().isdigit():
                            post_info['comment_count'] = int(reply_text.strip())
                            break
                except:
                    continue
            post_info.setdefault('comment_count', 0)
            
            # ì‘ì„±ì ì¶”ì¶œ
            author_selectors = ['td.author a.member_plate', 'td.author span', '.author a', '.author span', '.member_plate']
            for selector in author_selectors:
                try:
                    author_element = await row_element.query_selector(selector)
                    if author_element:
                        author = await author_element.inner_text()
                        if author and author.strip():
                            post_info['author'] = author.strip()
                            break
                except:
                    continue
            post_info.setdefault('author', '')
            
            # ì‘ì„±ì‹œê°„ ì¶”ì¶œ
            time_selectors = ['td.time', '.time', 'td[class*="date"]', '.date']
            for selector in time_selectors:
                try:
                    time_element = await row_element.query_selector(selector)
                    if time_element:
                        time_text = await time_element.inner_text()
                        if time_text and time_text.strip():
                            post_info['date'] = time_text.strip()
                            break
                except:
                    continue
            post_info.setdefault('date', '')
            
            # ì¡°íšŒìˆ˜ ë° ì¶”ì²œìˆ˜ ì¶”ì¶œ
            number_elements = await row_element.query_selector_all('td.m_no, .m_no, td[class*="num"]')
            
            if len(number_elements) >= 1:
                try:
                    view_text = await number_elements[0].inner_text()
                    post_info['view_count'] = int(view_text.strip()) if view_text.strip().isdigit() else 0
                except:
                    post_info['view_count'] = 0
            else:
                post_info['view_count'] = 0
            
            if len(number_elements) >= 2:
                try:
                    vote_text = await number_elements[-1].inner_text()
                    vote_text = vote_text.strip()
                    
                    if vote_text.startswith('-'):
                        post_info['up_count'] = 0
                        post_info['down_count'] = abs(int(vote_text)) if vote_text[1:].isdigit() else 0
                    elif vote_text.isdigit():
                        post_info['up_count'] = int(vote_text)
                        post_info['down_count'] = 0
                    else:
                        post_info['up_count'] = 0
                        post_info['down_count'] = 0
                except:
                    post_info['up_count'] = 0
                    post_info['down_count'] = 0
            else:
                post_info['up_count'] = 0
                post_info['down_count'] = 0
            
            return post_info
            
        except Exception as e:
            logger.warning(f"âš ï¸ ê²Œì‹œê¸€ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    def parse_post_id_from_url(self, url: str) -> str:
        """URLì—ì„œ ê²Œì‹œê¸€ ID ì¶”ì¶œ"""
        try:
            if '/index.php' in url:
                from urllib.parse import parse_qs, urlparse
                parsed = urlparse(url)
                params = parse_qs(parsed.query)
                if 'document_srl' in params:
                    return params['document_srl'][0]
            else:
                match = re.search(r'/(\d+)/?$', url)
                if match:
                    return match.group(1)
            
            numbers = re.findall(r'\d+', url)
            if numbers:
                return max(numbers, key=len)
            
            return 'unknown'
            
        except Exception as e:
            logger.error(f"ğŸ’¥ ê²Œì‹œê¸€ ID ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return 'unknown'

# í¸ì˜ í•¨ìˆ˜ë“¤
async def test_network_bypass():
    """ë„¤íŠ¸ì›Œí¬ ì°¨ë‹¨ ìš°íšŒ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    logger.info("ğŸš€ ë„¤íŠ¸ì›Œí¬ ì°¨ë‹¨ ìš°íšŒ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    logger.info(f"â° ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    try:
        async with ImprovedFMKoreaScraper() as scraper:
            # ê²Œì‹œê¸€ ëª©ë¡ ìŠ¤í¬ë˜í•‘ í…ŒìŠ¤íŠ¸
            posts = await scraper.scrape_politics_page_list(limit=10)
            
            logger.info("ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
            logger.info("=" * 50)
            logger.info(f"âœ… ì„±ê³µì ìœ¼ë¡œ ìŠ¤í¬ë˜í•‘ëœ ê²Œì‹œê¸€: {len(posts)}ê°œ")
            logger.info(f"ğŸŒ ì‚¬ìš©ëœ URL: {scraper.current_base_url}")
            logger.info(f"âš™ï¸ ì‚¬ìš©ëœ ì„¤ì •: {scraper.current_config['name'] if scraper.current_config else 'N/A'}")
            
            for i, post in enumerate(posts):
                logger.info(f"ğŸ“ ê²Œì‹œê¸€ {i+1}: {post.get('title', 'N/A')[:50]}...")
                logger.info(f"   - ì‘ì„±ì: {post.get('author', 'N/A')}")
                logger.info(f"   - ì¡°íšŒìˆ˜: {post.get('view_count', 0)}")
                logger.info(f"   - ì¶”ì²œìˆ˜: {post.get('up_count', 0)}")
                logger.info(f"   - URL: {post.get('url', 'N/A')}")
            
            # ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"network_bypass_test_{timestamp}.json"
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(posts, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {filename}")
            
            return posts
            
    except Exception as e:
        logger.error(f"ğŸ’¥ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return []
    
    finally:
        logger.info(f"â° ì™„ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

if __name__ == "__main__":
    asyncio.run(test_network_bypass()) 