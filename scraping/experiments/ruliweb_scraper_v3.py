#!/usr/bin/env python3
"""
ë£¨ë¦¬ì›¹ ì •ì¹˜ìœ ë¨¸ ê²Œì‹œíŒ ìŠ¤í¬ë˜í¼ v3 (TDD ê¸°ë°˜)

ì£¼ìš” ê¸°ëŠ¥:
- ë£¨ë¦¬ì›¹ ì •ì¹˜ìœ ë¨¸ ê²Œì‹œíŒ (https://bbs.ruliweb.com/community/board/300148) ì „ìš©
- ê²Œì‹œê¸€ ëª©ë¡ ìŠ¤í¬ë˜í•‘ ë° ê°œë³„ ê²Œì‹œê¸€ ìƒì„¸ ì •ë³´ ì¶”ì¶œ
- ë©”íƒ€ë°ì´í„°, ë³¸ë¬¸ ì½˜í…ì¸ , ëŒ“ê¸€ ì™„ì „ ì¶”ì¶œ
- ì‹¤ì‹œê°„ API ì—°ë™ ì§€ì›
- ê°œì„ ëœ í…ìŠ¤íŠ¸ ì²˜ë¦¬ (ì¤„ë°”ê¿ˆ, ìƒ‰ìƒ ë³´ì¡´)

ì‘ì„±ì: AI Assistant
ì‘ì„±ì¼: 2025ë…„ 6ì›” 4ì¼ 17:45 (KST)
ê¸°ë°˜: ë””ë²„ê·¸ ìŠ¤í¬ë¦½íŠ¸ ë¶„ì„ ê²°ê³¼
"""

import asyncio
import json
import re
import requests
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
from playwright.async_api import async_playwright, Browser, Page, ElementHandle
import pytz
import os

# ë£¨ë¦¬ì›¹ ì…€ë ‰í„° ì •ì˜ (ë””ë²„ê·¸ ê²°ê³¼ ê¸°ë°˜)
RULIWEB_SELECTORS = {
    "list": {
        "container": "table.board_list_table tbody",
        "post_rows": "tr.table_body.blocktarget",
        "title": "td.subject a.subject_link",
        "author": "td.writer a",
        "recommend": "td.recomd",
        "hit": "td.hit",
        "time": "td.time",
        "category": "td.divsn a",
        "comment_count_in_title": "span.num_reply"
    },
    "metadata": {
        "title": ".board_main_view .subject_inner",
        "author": ".board_main_view .writer",
        "date": ".board_main_view .regdate",
        "category": ".board_main_view .category",
        "info": ".board_main_view .board_info span",
        "view_count": ".board_main_view .board_info .hit",
        "recommend_count": ".board_main_view .board_info .recomd"
    },
    "content": {
        "container": ".board_main_view .view_content"
    },
    "comments": {
        "container": ".comment_view .comment_element",
        "author": ".comment_writer",
        "content": ".comment_content",
        "date": ".comment_regdate",
        "media": "img, video",
        "up_count": ".comment_up",
        "down_count": ".comment_down"
    }
}

# ë°ì´í„° ìŠ¤í‚¤ë§ˆ
POST_SCHEMA = {
    "post_id": str,
    "post_url": str,
    "scraped_at": str,
    "metadata": {
        "title": str,
        "category": str,
        "author": str,
        "date": str,
        "view_count": int,
        "up_count": int,
        "down_count": int,
        "comment_count": int
    },
    "content": list,
    "comments": list
}

# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
def clean_text(text: str) -> str:
    """í…ìŠ¤íŠ¸ ì •ë¦¬ (ê¸°ë³¸ì ì¸ ì •ë¦¬ë§Œ)"""
    if not text:
        return ""
    return text.strip()

def parse_number(text: str) -> int:
    """í…ìŠ¤íŠ¸ì—ì„œ ìˆ«ì ì¶”ì¶œ"""
    if not text:
        return 0
    numbers = re.findall(r'\d+', str(text).replace(',', ''))
    return int(numbers[0]) if numbers else 0

def parse_post_id(url: str) -> str:
    """URLì—ì„œ ê²Œì‹œê¸€ ID ì¶”ì¶œ"""
    if not url:
        return ""
    match = re.search(r'/read/(\d+)', url)
    return match.group(1) if match else ""

def parse_date(date_str: str) -> str:
    """ë‚ ì§œ ë¬¸ìì—´ì„ ISO í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    if not date_str:
        return datetime.now(pytz.timezone('Asia/Seoul')).isoformat()
    
    try:
        # ë£¨ë¦¬ì›¹ ë‚ ì§œ í˜•ì‹ ì²˜ë¦¬
        date_str = date_str.strip()
        
        # "17:43" í˜•ì‹ (ì˜¤ëŠ˜)
        if re.match(r'^\d{1,2}:\d{2}$', date_str):
            now = datetime.now(pytz.timezone('Asia/Seoul'))
            time_parts = date_str.split(':')
            hour, minute = int(time_parts[0]), int(time_parts[1])
            today = now.replace(hour=hour, minute=minute, second=0, microsecond=0)
            return today.isoformat()
        
        # "2025.06.04" í˜•ì‹
        elif re.match(r'^\d{4}\.\d{2}\.\d{2}$', date_str):
            date_obj = datetime.strptime(date_str, '%Y.%m.%d')
            date_obj = pytz.timezone('Asia/Seoul').localize(date_obj)
            return date_obj.isoformat()
        
        # ê¸°íƒ€ í˜•ì‹ì€ ê·¸ëŒ€ë¡œ ë°˜í™˜
        else:
            return date_str
            
    except Exception as e:
        print(f"ë‚ ì§œ íŒŒì‹± ì˜¤ë¥˜: {e}")
        return datetime.now(pytz.timezone('Asia/Seoul')).isoformat()

def select_top_posts(posts: List[Dict], limit: int = 10) -> List[Dict]:
    """ìƒìœ„ ê²Œì‹œê¸€ ì„ ë³„ (ì¶”ì²œìˆ˜, ì¡°íšŒìˆ˜, ëŒ“ê¸€ìˆ˜ ê¸°ì¤€)"""
    if not posts:
        return []
    
    # ê³µì§€ì‚¬í•­ ì œì™¸
    filtered_posts = [post for post in posts if post.get("category", "").lower() != "ê³µì§€"]
    
    # ì ìˆ˜ ê³„ì‚° (ì¶”ì²œìˆ˜ * 3 + ì¡°íšŒìˆ˜ * 0.1 + ëŒ“ê¸€ìˆ˜ * 2)
    for post in filtered_posts:
        recommend = post.get("recommend", 0)
        hit = post.get("hit", 0)
        comment_count = post.get("comment_count", 0)
        
        score = recommend * 3 + hit * 0.1 + comment_count * 2
        post["_score"] = score
    
    # ì ìˆ˜ìˆœ ì •ë ¬ í›„ ìƒìœ„ ì„ íƒ
    sorted_posts = sorted(filtered_posts, key=lambda x: x.get("_score", 0), reverse=True)
    return sorted_posts[:limit]

# ë¸Œë¼ìš°ì € ê´€ë ¨ í•¨ìˆ˜ë“¤
async def setup_browser(config: Optional[Dict] = None) -> Tuple[Browser, Page]:
    """ë¸Œë¼ìš°ì € ì„¤ì • ë° ì‹œì‘"""
    if config is None:
        config = {}
    
    playwright = await async_playwright().start()
    browser = await playwright.chromium.launch(
        headless=config.get('headless', True),
        slow_mo=config.get('slow_mo', 0)
    )
    
    page = await browser.new_page()
    await page.set_viewport_size({"width": 1920, "height": 1080})
    
    return browser, page

async def close_browser(browser: Browser):
    """ë¸Œë¼ìš°ì € ì¢…ë£Œ"""
    await browser.close()

async def navigate_to_page(page: Page, url: str, config: Optional[Dict] = None) -> bool:
    """í˜ì´ì§€ ì´ë™"""
    if config is None:
        config = {}
    
    try:
        await page.goto(url, wait_until="networkidle", timeout=config.get('timeout', 30000))
        await page.wait_for_timeout(config.get('wait_time', 2) * 1000)
        return True
    except Exception as e:
        print(f"í˜ì´ì§€ ì´ë™ ì‹¤íŒ¨: {e}")
        return False

# í…ìŠ¤íŠ¸ ì¶”ì¶œ í•¨ìˆ˜ (ê°œì„ ëœ ë²„ì „)
async def extract_text_content(element: ElementHandle) -> str:
    """ìš”ì†Œì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì¤„ë°”ê¿ˆ ë° ìŠ¤íƒ€ì¼ ë³´ì¡´)"""
    try:
        html_content = await element.evaluate("""
            element => {
                const clone = element.cloneNode(true);
                
                // br íƒœê·¸ë¥¼ ì¤„ë°”ê¿ˆìœ¼ë¡œ ë³€í™˜
                const brElements = clone.querySelectorAll('br');
                brElements.forEach(br => {
                    br.replaceWith('\\n');
                });
                
                // p, div íƒœê·¸ ì‚¬ì´ì— ì¤„ë°”ê¿ˆ ì¶”ê°€
                const blockElements = clone.querySelectorAll('p, div');
                blockElements.forEach(block => {
                    if (block.nextSibling) {
                        block.insertAdjacentText('afterend', '\\n');
                    }
                });
                
                // ìƒ‰ìƒ ì •ë³´ê°€ ìˆëŠ” ìš”ì†Œë“¤ ì²˜ë¦¬
                const colorElements = clone.querySelectorAll('[style*="color"], [color]');
                colorElements.forEach(el => {
                    const style = el.getAttribute('style') || '';
                    const color = el.getAttribute('color') || '';
                    let colorValue = '';
                    
                    if (style.includes('color:')) {
                        const colorMatch = style.match(/color:\\s*([^;]+)/);
                        if (colorMatch) colorValue = colorMatch[1].trim();
                    } else if (color) {
                        colorValue = color;
                    }
                    
                    if (colorValue && colorValue !== 'black' && colorValue !== '#000' && colorValue !== '#000000') {
                        const text = el.textContent || '';
                        if (text.trim()) {
                            el.textContent = `[ìƒ‰ìƒ:${colorValue}]${text}[/ìƒ‰ìƒ]`;
                        }
                    }
                });
                
                return clone.textContent || clone.innerText || '';
            }
        """)
        
        if html_content:
            text = re.sub(r'\n{3,}', '\n\n', html_content)
            text = text.strip()
            return text
        else:
            return ""
    except Exception as e:
        print(f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        try:
            return await element.evaluate("element => element.textContent || ''")
        except:
            return ""

async def extract_attribute(element: ElementHandle, attr: str) -> str:
    """ìš”ì†Œì—ì„œ ì†ì„± ì¶”ì¶œ"""
    try:
        value = await element.get_attribute(attr)
        return value or ""
    except Exception:
        return ""

# ê²Œì‹œê¸€ ëª©ë¡ ì¶”ì¶œ
async def extract_post_list(page: Page, selectors: Dict) -> List[Dict]:
    """ê²Œì‹œê¸€ ëª©ë¡ ì¶”ì¶œ"""
    posts = []
    post_selectors = selectors["list"]
    
    try:
        post_elements = await page.query_selector_all(post_selectors["post_rows"])
        print(f"ğŸ“‹ ë°œê²¬ëœ ê²Œì‹œê¸€ ìˆ˜: {len(post_elements)}")
        
        for element in post_elements:
            post_data = {}
            
            # ì œëª© ë° URL ì¶”ì¶œ
            title_element = await element.query_selector(post_selectors["title"])
            if title_element:
                title = await extract_text_content(title_element)
                url = await extract_attribute(title_element, "href")
                
                # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
                if url and url.startswith('/'):
                    url = "https://bbs.ruliweb.com" + url
                    
                post_data["title"] = clean_text(title)
                post_data["url"] = url
                post_data["post_id"] = parse_post_id(url) if url else ""
            else:
                continue  # ì œëª©ì´ ì—†ìœ¼ë©´ ìŠ¤í‚µ
                
            # ì‘ì„±ì ì¶”ì¶œ
            author_element = await element.query_selector(post_selectors["author"])
            if author_element:
                post_data["author"] = await extract_text_content(author_element)
            else:
                post_data["author"] = ""
                
            # ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
            category_element = await element.query_selector(post_selectors["category"])
            if category_element:
                post_data["category"] = await extract_text_content(category_element)
            else:
                post_data["category"] = ""
                
            # ì¶”ì²œìˆ˜ ì¶”ì¶œ
            recommend_element = await element.query_selector(post_selectors["recommend"])
            if recommend_element:
                recommend_text = await extract_text_content(recommend_element)
                post_data["recommend"] = parse_number(recommend_text)
            else:
                post_data["recommend"] = 0
                
            # ì¡°íšŒìˆ˜ ì¶”ì¶œ
            hit_element = await element.query_selector(post_selectors["hit"])
            if hit_element:
                hit_text = await extract_text_content(hit_element)
                post_data["hit"] = parse_number(hit_text)
            else:
                post_data["hit"] = 0
                
            # ì‘ì„±ì¼ ì¶”ì¶œ
            time_element = await element.query_selector(post_selectors["time"])
            if time_element:
                time_text = await extract_text_content(time_element)
                post_data["date"] = parse_date(time_text)
            else:
                post_data["date"] = datetime.now(pytz.timezone('Asia/Seoul')).isoformat()
                
            # ëŒ“ê¸€ìˆ˜ ì¶”ì¶œ (ì œëª©ì—ì„œ)
            comment_element = await element.query_selector(post_selectors["comment_count_in_title"])
            if comment_element:
                comment_text = await extract_text_content(comment_element)
                post_data["comment_count"] = parse_number(comment_text)
            else:
                # ì œëª©ì—ì„œ ëŒ“ê¸€ìˆ˜ ì¶”ì¶œ ì‹œë„
                title_text = post_data.get("title", "")
                comment_match = re.search(r'\((\d+)\)', title_text)
                post_data["comment_count"] = int(comment_match.group(1)) if comment_match else 0
                
            posts.append(post_data)
            
    except Exception as e:
        print(f"ê²Œì‹œê¸€ ëª©ë¡ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        
    return posts

# ë£¨ë¦¬ì›¹ ìŠ¤í¬ë˜í¼ í´ë˜ìŠ¤
class RuliwebScraper:
    """ë£¨ë¦¬ì›¹ ìŠ¤í¬ë˜í¼ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.selectors = {
            "post_list": {
                "container": ".board_list_table tbody",
                "items": "tr.table_body:not(.notice):not(.list_inner)",
                "title": ".subject .subject_link",
                "url": ".subject .subject_link",
                "author": ".writer a",
                "date": ".time",
                "view_count": ".hit",
                "recommend_count": ".recomd",
                "comment_count": ".num_reply"
            },
            "metadata": {
                "title": ".subject_wrapper .subject .subject_text .subject_inner_text",
                "author": ".user_info .nick a",
                "date": ".regdate",
                "view_count": ".mini_profile .hit strong",
                "recommend_count": ".mini_profile .recomd strong",
                "comment_count": ".reply_count",
                "category": ".category_text",
                "info": ".user_info p"
            },
            "content": {
                "container": ".view_content article"
            },
            "comments": {
                "container": ".comment_table tbody tr.comment_element",
                "author": ".nick_link strong span",
                "content": ".text_wrapper .text",
                "date": ".time",
                "recommend": ".btn_like .num",
                "unrecommend": ".btn_dislike .num",
                "profile_image": ".profile_image_m"
            }
        }

    async def extract_metadata(self, page: Page) -> Dict:
        """ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (ì‹¤ì œ HTML êµ¬ì¡° ê¸°ë°˜)"""
        metadata = {}
        selectors = self.selectors["metadata"]
        
        try:
            # ì œëª© ì¶”ì¶œ (ì‹¤ì œ HTML êµ¬ì¡° ê¸°ë°˜)
            title_selectors = [
                ".subject_wrapper .subject .subject_text .subject_inner_text",  # ì‹¤ì œ êµ¬ì¡°
                ".subject_text .subject_inner_text",
                ".subject .subject_inner_text",
                ".subject_inner_text",  # ë” ê°„ë‹¨í•œ ì…€ë ‰í„°
                "h4.subject .subject_inner_text",
                ".subject_wrapper .subject_inner_text",
                ".subject_text",
                ".subject",
                "h4.subject",
                ".board_main_view .subject",
                ".view_subject",
                ".title",
                "h1",
                "h2",
                "h3",
                "h4"
            ]
            
            title_found = False
            for title_selector in title_selectors:
                title_element = await page.query_selector(title_selector)
                if title_element:
                    title_text = await extract_text_content(title_element)
                    if title_text and title_text.strip() and len(title_text.strip()) > 3:
                        # ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ëŠ” ì œëª©ì´ ì•„ë‹ ê°€ëŠ¥ì„±ì´ ë†’ìŒ
                        metadata["title"] = title_text.strip()
                        title_found = True
                        print(f"âœ… ì œëª© ì¶”ì¶œ ì„±ê³µ ({title_selector}): {title_text[:50]}...")
                        break
                    else:
                        print(f"âŒ ì œëª© í›„ë³´ ë°œê²¬í–ˆì§€ë§Œ ë¶€ì ì ˆ ({title_selector}): '{title_text}'")
                else:
                    print(f"âŒ ì…€ë ‰í„° ë§¤ì¹˜ ì—†ìŒ: {title_selector}")
            
            if not title_found:
                # í˜ì´ì§€ ì œëª©ì—ì„œ ì¶”ì¶œ ì‹œë„
                page_title = await page.title()
                print(f"ğŸ” í˜ì´ì§€ ì œëª© í™•ì¸: {page_title}")
                if page_title and "ë£¨ë¦¬ì›¹" in page_title:
                    # "ì œëª© - ë£¨ë¦¬ì›¹" í˜•ì‹ì—ì„œ ì œëª© ë¶€ë¶„ë§Œ ì¶”ì¶œ
                    title_part = page_title.split(" - ")[0].strip()
                    if title_part and title_part != "ë£¨ë¦¬ì›¹" and len(title_part) > 3:
                        metadata["title"] = title_part
                        print(f"âœ… í˜ì´ì§€ ì œëª©ì—ì„œ ì¶”ì¶œ: {title_part}")
                        title_found = True
                
            if not title_found:
                # ëª¨ë“  í…ìŠ¤íŠ¸ ìš”ì†Œì—ì„œ ì œëª© í›„ë³´ ì°¾ê¸°
                print("ğŸ” ëª¨ë“  í…ìŠ¤íŠ¸ ìš”ì†Œì—ì„œ ì œëª© í›„ë³´ ì°¾ëŠ” ì¤‘...")
                all_text_elements = await page.query_selector_all("h1, h2, h3, h4, .title, .subject, [class*='title'], [class*='subject']")
                for i, element in enumerate(all_text_elements[:10]):  # ìƒìœ„ 10ê°œë§Œ í™•ì¸
                    text = await extract_text_content(element)
                    if text and len(text.strip()) > 5 and len(text.strip()) < 200:
                        print(f"ğŸ” ì œëª© í›„ë³´ {i+1}: '{text[:50]}...'")
                        if not title_found:  # ì²« ë²ˆì§¸ ì ì ˆí•œ í›„ë³´ë¥¼ ì œëª©ìœ¼ë¡œ ì‚¬ìš©
                            metadata["title"] = text.strip()
                            title_found = True
                            print(f"âœ… ì²« ë²ˆì§¸ í›„ë³´ë¥¼ ì œëª©ìœ¼ë¡œ ì„ íƒ: {text[:50]}...")
                            break
                
            if not title_found:
                metadata["title"] = f"ê²Œì‹œê¸€ #{parse_post_id(page.url)}"
                print(f"âŒ ì œëª© ì¶”ì¶œ ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©: {metadata['title']}")
                
            # ì‘ì„±ì ì¶”ì¶œ (ì‹¤ì œ HTML êµ¬ì¡° ê¸°ë°˜)
            author_selectors = [
                ".user_info_wrapper .nick .nick_link strong",  # ì‹¤ì œ êµ¬ì¡°
                ".user_info .nick a",  # ê¸°ì¡´ êµ¬ì¡°
                ".user_info_wrapper .nick a",
                ".nick a"
            ]
            
            author_found = False
            for author_selector in author_selectors:
                author_element = await page.query_selector(author_selector)
                if author_element:
                    author_text = await extract_text_content(author_element)
                    if author_text and author_text.strip():
                        metadata["author"] = author_text.strip()
                        author_found = True
                        print(f"âœ… ì‘ì„±ì ì¶”ì¶œ ì„±ê³µ: {author_text}")
                        break
                        
            if not author_found:
                metadata["author"] = ""
                print("âŒ ì‘ì„±ì ì¶”ì¶œ ì‹¤íŒ¨")
                
            # ì‘ì„±ì¼ ì¶”ì¶œ
            date_selectors = [
                ".control_box .time",  # ì‹¤ì œ êµ¬ì¡°
                ".regdate",
                ".user_info .regdate",
                ".mini_profile .regdate"
            ]
            
            date_found = False
            for date_selector in date_selectors:
                date_element = await page.query_selector(date_selector)
                if date_element:
                    date_text = await extract_text_content(date_element)
                    if date_text and date_text.strip():
                        metadata["date"] = parse_date(date_text)
                        date_found = True
                        print(f"âœ… ì‘ì„±ì¼ ì¶”ì¶œ ì„±ê³µ: {date_text}")
                        break
                        
            if not date_found:
                metadata["date"] = datetime.now(pytz.timezone('Asia/Seoul')).isoformat()
                
            # ì¡°íšŒìˆ˜ ì¶”ì¶œ (ì‹¤ì œ HTML êµ¬ì¡° ê¸°ë°˜)
            view_selectors = [
                ".mini_profile .hit strong",  # ì‹¤ì œ êµ¬ì¡°
                ".user_info .hit",
                ".hit strong"
            ]
            
            metadata["view_count"] = 0
            for view_selector in view_selectors:
                view_element = await page.query_selector(view_selector)
                if view_element:
                    view_text = await extract_text_content(view_element)
                    if view_text and view_text.strip():
                        metadata["view_count"] = parse_number(view_text)
                        if metadata["view_count"] > 0:
                            print(f"âœ… ì¡°íšŒìˆ˜ ì¶”ì¶œ ì„±ê³µ: {metadata['view_count']:,}")
                            break
            
            # ì¶”ì²œìˆ˜ ì¶”ì¶œ (ì‹¤ì œ HTML êµ¬ì¡° ê¸°ë°˜)
            recommend_selectors = [
                ".mini_profile .recomd strong",  # ì‹¤ì œ êµ¬ì¡°
                ".like_value",
                ".recomd strong"
            ]
            
            metadata["up_count"] = 0
            for recommend_selector in recommend_selectors:
                recommend_element = await page.query_selector(recommend_selector)
                if recommend_element:
                    recommend_text = await extract_text_content(recommend_element)
                    if recommend_text and recommend_text.strip():
                        metadata["up_count"] = parse_number(recommend_text)
                        if metadata["up_count"] > 0:
                            print(f"âœ… ì¶”ì²œìˆ˜ ì¶”ì¶œ ì„±ê³µ: {metadata['up_count']:,}")
                            break
            
            # ëŒ“ê¸€ìˆ˜ ì¶”ì¶œ (ì‹¤ì œ HTML êµ¬ì¡° ê¸°ë°˜)
            comment_count_selectors = [
                ".reply_count",
                ".mini_profile .replycount .num strong",
                ".num_reply"
            ]
            
            metadata["comment_count"] = 0
            for comment_selector in comment_count_selectors:
                comment_element = await page.query_selector(comment_selector)
                if comment_element:
                    comment_text = await extract_text_content(comment_element)
                    if comment_text and comment_text.strip():
                        metadata["comment_count"] = parse_number(comment_text)
                        if metadata["comment_count"] > 0:
                            print(f"âœ… ëŒ“ê¸€ìˆ˜ ì¶”ì¶œ ì„±ê³µ: {metadata['comment_count']:,}")
                            break
            
            # ëŒ“ê¸€ìˆ˜ë¥¼ ëŒ“ê¸€ ì»¨í…Œì´ë„ˆì—ì„œ ì§ì ‘ ê³„ì‚° (ë” ì •í™•í•¨)
            if metadata["comment_count"] == 0:
                comment_elements = await page.query_selector_all(".comment_table tbody tr.comment_element")
                if comment_elements:
                    # ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•´ ê³ ìœ í•œ comment_idë§Œ ê³„ì‚°
                    unique_comment_ids = set()
                    for element in comment_elements:
                        comment_id = await element.get_attribute("id")
                        if comment_id:
                            unique_comment_ids.add(comment_id)
                    metadata["comment_count"] = len(unique_comment_ids)
                    print(f"âœ… ëŒ“ê¸€ìˆ˜ ì§ì ‘ ê³„ì‚° (ì¤‘ë³µ ì œê±°): {metadata['comment_count']:,}")
                    
            # ë¹„ì¶”ì²œìˆ˜ (ë£¨ë¦¬ì›¹ì€ ì—†ìŒ)
            metadata["down_count"] = 0
                
            # ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ (ì‹¤ì œ HTML êµ¬ì¡° ê¸°ë°˜)
            category_selectors = [
                ".subject_text .category_text",  # ì‹¤ì œ êµ¬ì¡°
                ".category_text",
                ".subject_text .category_text"
            ]
            
            category_found = False
            for category_selector in category_selectors:
                category_element = await page.query_selector(category_selector)
                if category_element:
                    category_text = await extract_text_content(category_element)
                    if category_text and category_text.strip():
                        # [ìœ ë¨¸], [ì •ì¹˜] ë“±ì—ì„œ ëŒ€ê´„í˜¸ ì œê±°
                        category_clean = category_text.strip().replace('[', '').replace(']', '')
                        metadata["category"] = category_clean
                        category_found = True
                        print(f"âœ… ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ ì„±ê³µ: {category_clean}")
                        break
                        
            if not category_found:
                # URLì—ì„œ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ ì‹œë„
                current_url = page.url
                if "board/300148" in current_url:
                    metadata["category"] = "ì •ì¹˜ìœ ë¨¸"
                else:
                    metadata["category"] = ""
                
            print(f"ğŸ“Š ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ:")
            print(f"  - ì œëª©: {metadata['title'][:50]}...")
            print(f"  - ì‘ì„±ì: {metadata['author']}")
            print(f"  - ì¡°íšŒìˆ˜: {metadata['view_count']:,}")
            print(f"  - ì¶”ì²œìˆ˜: {metadata['up_count']:,}")
            print(f"  - ëŒ“ê¸€ìˆ˜: {metadata['comment_count']:,}")
            print(f"  - ì¹´í…Œê³ ë¦¬: {metadata['category']}")
                
        except Exception as e:
            print(f"ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            metadata = {
                "title": "",
                "author": "",
                "date": datetime.now(pytz.timezone('Asia/Seoul')).isoformat(),
                "view_count": 0,
                "up_count": 0,
                "down_count": 0,
                "comment_count": 0,
                "category": ""
            }
            
        return metadata
        
    async def extract_content(self, page: Page) -> List[Dict]:
        """ë³¸ë¬¸ ì½˜í…ì¸  ì¶”ì¶œ"""
        content = []
        selectors = self.selectors["content"]
        order = 0
        
        try:
            container = await page.query_selector(selectors["container"])
            if not container:
                print("âŒ ë³¸ë¬¸ ì»¨í…Œì´ë„ˆë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return content
                
            print("ğŸ” ë³¸ë¬¸ ì½˜í…ì¸  ì¶”ì¶œ ì¤‘...")
            
            # ëª¨ë“  í•˜ìœ„ ìš”ì†Œë¥¼ ìˆœì„œëŒ€ë¡œ ì²˜ë¦¬
            all_elements = await container.query_selector_all("*")
            
            for element in all_elements:
                tag_name = await element.evaluate("el => el.tagName.toLowerCase()")
                
                # í…ìŠ¤íŠ¸ ìš”ì†Œ ì²˜ë¦¬
                if tag_name in ['p', 'div']:
                    text = await extract_text_content(element)
                    if text and text.strip():
                        # í•˜ìœ„ì— ì´ë¯¸ì§€ê°€ ìˆëŠ”ì§€ í™•ì¸
                        img_elements = await element.query_selector_all("img")
                        if len(img_elements) == 0:  # ì´ë¯¸ì§€ê°€ ì—†ëŠ” í…ìŠ¤íŠ¸ë§Œ
                            content.append({
                                "type": "text",
                                "order": order,
                                "data": {"text": text.strip()}
                            })
                            order += 1
                            print(f"  ğŸ“ í…ìŠ¤íŠ¸ ì¶”ê°€: {text[:50]}...")
                
                # ì´ë¯¸ì§€ ìš”ì†Œ ì²˜ë¦¬
                elif tag_name == 'img':
                    src = await extract_attribute(element, "src")
                    alt = await extract_attribute(element, "alt")
                    width = await extract_attribute(element, "width")
                    height = await extract_attribute(element, "height")
                    
                    if src:
                        # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
                        if src.startswith("//"):
                            src = "https:" + src
                        elif src.startswith("/"):
                            src = "https://bbs.ruliweb.com" + src
                            
                        content.append({
                            "type": "image",
                            "order": order,
                            "data": {
                                "src": src,
                                "alt": alt or f"image_{order}",
                                "width": width or "",
                                "height": height or ""
                            }
                        })
                        order += 1
                        print(f"  ğŸ–¼ï¸ ì´ë¯¸ì§€ ì¶”ê°€: {src}")
                
                # ë¹„ë””ì˜¤ ìš”ì†Œ ì²˜ë¦¬
                elif tag_name == 'video':
                    src = await extract_attribute(element, "src")
                    autoplay = await extract_attribute(element, "autoplay")
                    muted = await extract_attribute(element, "muted")
                    
                    if src:
                        content.append({
                            "type": "video",
                            "order": order,
                            "data": {
                                "src": src,
                                "autoplay": autoplay == "true" or autoplay == "",
                                "muted": muted == "true" or muted == ""
                            }
                        })
                        order += 1
                        print(f"  ğŸ¥ ë¹„ë””ì˜¤ ì¶”ê°€: {src}")
            
            # ëŒ€ì²´ ë°©ë²•ìœ¼ë¡œ ì½˜í…ì¸  ì¶”ì¶œ
            if len(content) == 0:
                print("ğŸ”„ ëŒ€ì²´ ë°©ë²•ìœ¼ë¡œ ì½˜í…ì¸  ì¶”ì¶œ ì‹œë„...")
                
                # í…ìŠ¤íŠ¸ ìš”ì†Œë“¤
                text_elements = await container.query_selector_all("p, div")
                for element in text_elements:
                    text = await extract_text_content(element)
                    if text and text.strip():
                        content.append({
                            "type": "text",
                            "order": order,
                            "data": {"text": text.strip()}
                        })
                        order += 1
                        print(f"  ğŸ“ í…ìŠ¤íŠ¸ ì¶”ê°€ (ëŒ€ì²´): {text[:50]}...")
                
                # ì´ë¯¸ì§€ë“¤
                img_elements = await container.query_selector_all("img")
                for element in img_elements:
                    src = await extract_attribute(element, "src")
                    alt = await extract_attribute(element, "alt")
                    
                    if src:
                        if src.startswith("//"):
                            src = "https:" + src
                        elif src.startswith("/"):
                            src = "https://bbs.ruliweb.com" + src
                            
                        content.append({
                            "type": "image",
                            "order": order,
                            "data": {
                                "src": src,
                                "alt": alt or f"image_{order}",
                                "width": "",
                                "height": ""
                            }
                        })
                        order += 1
                        print(f"  ğŸ–¼ï¸ ì´ë¯¸ì§€ ì¶”ê°€ (ëŒ€ì²´): {src}")
                    
        except Exception as e:
            print(f"ë³¸ë¬¸ ì½˜í…ì¸  ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            
        print(f"ğŸ“„ ì´ {len(content)}ê°œ ì½˜í…ì¸  ìš”ì†Œ ì¶”ì¶œ ì™„ë£Œ")
        return content
        
    async def extract_comments(self, page: Page) -> List[Dict]:
        """ëŒ“ê¸€ ì¶”ì¶œ (ì‹¤ì œ HTML êµ¬ì¡° ê¸°ë°˜, ì¤‘ë³µ ì œê±°)"""
        comments = []
        seen_comment_ids = set()  # ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ ì§‘í•©
        
        try:
            # ì‹¤ì œ HTML êµ¬ì¡°ì— ë§ëŠ” ëŒ“ê¸€ ì»¨í…Œì´ë„ˆ ì„ íƒ
            comment_elements = await page.query_selector_all(".comment_table tbody tr.comment_element")
            print(f"ğŸ” ëŒ“ê¸€ ìš”ì†Œ {len(comment_elements)}ê°œ ë°œê²¬")
            
            for i, element in enumerate(comment_elements):
                comment_data = {}
                
                # ëŒ“ê¸€ ID ì¶”ì¶œ (ì‹¤ì œ HTMLì—ì„œ id ì†ì„± ì‚¬ìš©)
                comment_id = await element.get_attribute("id")
                if comment_id:
                    # ì¤‘ë³µ ì²´í¬
                    if comment_id in seen_comment_ids:
                        print(f"  âš ï¸ ì¤‘ë³µ ëŒ“ê¸€ ID ë°œê²¬, ê±´ë„ˆëœ€: {comment_id}")
                        continue
                    seen_comment_ids.add(comment_id)
                    comment_data["comment_id"] = comment_id
                else:
                    comment_data["comment_id"] = f"comment_{i}"
                    
                # ëŒ“ê¸€ ì‘ì„±ì ì¶”ì¶œ (ì‹¤ì œ HTML êµ¬ì¡° ê¸°ë°˜)
                author_selectors = [
                    ".user_info_wrapper .nick .nick_link strong",  # ì‹¤ì œ êµ¬ì¡°
                    ".nick_link strong",
                    ".nick a",
                    ".user_info_wrapper .nick a"
                ]
                
                author_found = False
                for author_selector in author_selectors:
                    author_element = await element.query_selector(author_selector)
                    if author_element:
                        author_text = await extract_text_content(author_element)
                        if author_text and author_text.strip():
                            comment_data["author"] = author_text.strip()
                            author_found = True
                            break
                            
                if not author_found:
                    comment_data["author"] = "ìµëª…"
                    
                # ëŒ“ê¸€ ë‚´ìš© ì¶”ì¶œ (ì‹¤ì œ HTML êµ¬ì¡° ê¸°ë°˜)
                content_selectors = [
                    ".text_wrapper .text",  # ì‹¤ì œ êµ¬ì¡°
                    ".comment .text",
                    ".text"
                ]
                
                content_found = False
                for content_selector in content_selectors:
                    content_element = await element.query_selector(content_selector)
                    if content_element:
                        content_text = await extract_text_content(content_element)
                        if content_text and content_text.strip():
                            comment_data["content"] = content_text.strip()
                            content_found = True
                            break
                            
                if not content_found:
                    comment_data["content"] = ""
                    
                # ëŒ“ê¸€ ì‘ì„±ì¼ ì¶”ì¶œ (ì‹¤ì œ HTML êµ¬ì¡° ê¸°ë°˜)
                date_selectors = [
                    ".control_box .time",  # ì‹¤ì œ êµ¬ì¡°
                    ".parent_control_box_wrapper .control_box .time",
                    ".time",
                    ".regdate"
                ]
                
                date_found = False
                for date_selector in date_selectors:
                    date_element = await element.query_selector(date_selector)
                    if date_element:
                        date_text = await extract_text_content(date_element)
                        if date_text and date_text.strip():
                            comment_data["date"] = parse_date(date_text)
                            date_found = True
                            break
                            
                if not date_found:
                    comment_data["date"] = datetime.now(pytz.timezone('Asia/Seoul')).isoformat()
                    
                # ëŒ“ê¸€ ì¶”ì²œìˆ˜ ì¶”ì¶œ (ì‹¤ì œ HTML êµ¬ì¡° ê¸°ë°˜)
                recommend_selectors = [
                    ".btn_like .num",  # ì‹¤ì œ êµ¬ì¡°
                    ".control_box .btn_like .num",
                    ".parent_control_box_wrapper .control_box .btn_like .num"
                ]
                
                comment_data["up_count"] = 0
                for recommend_selector in recommend_selectors:
                    recommend_element = await element.query_selector(recommend_selector)
                    if recommend_element:
                        recommend_text = await extract_text_content(recommend_element)
                        if recommend_text and recommend_text.strip():
                            comment_data["up_count"] = parse_number(recommend_text)
                            break
                
                # ëŒ“ê¸€ ë¹„ì¶”ì²œìˆ˜ ì¶”ì¶œ (ì‹¤ì œ HTML êµ¬ì¡° ê¸°ë°˜)
                unrecommend_selectors = [
                    ".btn_dislike .num",  # ì‹¤ì œ êµ¬ì¡°
                    ".control_box .btn_dislike .num",
                    ".parent_control_box_wrapper .control_box .btn_dislike .num"
                ]
                
                comment_data["down_count"] = 0
                for unrecommend_selector in unrecommend_selectors:
                    unrecommend_element = await element.query_selector(unrecommend_selector)
                    if unrecommend_element:
                        unrecommend_text = await extract_text_content(unrecommend_element)
                        if unrecommend_text and unrecommend_text.strip():
                            comment_data["down_count"] = parse_number(unrecommend_text)
                            break
                    
                # ëŒ“ê¸€ ë¯¸ë””ì–´ ì¶”ì¶œ (í”„ë¡œí•„ ì´ë¯¸ì§€ í¬í•¨)
                media_list = []
                
                # í”„ë¡œí•„ ì´ë¯¸ì§€ ì¶”ì¶œ
                profile_img_element = await element.query_selector(".profile_image_m")
                if profile_img_element:
                    profile_src = await extract_attribute(profile_img_element, "src")
                    if profile_src:
                        # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
                        if profile_src.startswith("//"):
                            profile_src = "https:" + profile_src
                        elif profile_src.startswith("/"):
                            profile_src = "https://bbs.ruliweb.com" + profile_src
                            
                        media_list.append({
                            "type": "image",
                            "order": 0,
                            "data": {
                                "src": profile_src,
                                "alt": "í”„ë¡œí•„ ì´ë¯¸ì§€",
                                "width": "40",
                                "height": "40"
                            }
                        })
                
                # ëŒ“ê¸€ ë‚´ ë‹¤ë¥¸ ì´ë¯¸ì§€ë“¤ ì¶”ì¶œ
                img_elements = await element.query_selector_all(".text_wrapper img, .comment img")
                for j, img_element in enumerate(img_elements):
                    img_src = await extract_attribute(img_element, "src")
                    if img_src:
                        if img_src.startswith("//"):
                            img_src = "https:" + img_src
                        elif img_src.startswith("/"):
                            img_src = "https://bbs.ruliweb.com" + img_src
                            
                        media_list.append({
                            "type": "image",
                            "order": j + 1,
                            "data": {
                                "src": img_src,
                                "alt": f"ëŒ“ê¸€ ì´ë¯¸ì§€ {j+1}",
                                "width": "",
                                "height": ""
                            }
                        })
                        
                comment_data["media"] = media_list
                
                # ë£¨ë¦¬ì›¹ ëŒ“ê¸€ êµ¬ì¡° (ëŒ€ë¶€ë¶„ 1ë ˆë²¨, ë‹µê¸€ êµ¬ì¡°ëŠ” ë³µì¡í•¨)
                comment_data["level"] = 1
                comment_data["is_reply"] = False
                comment_data["parent_comment_id"] = ""
                
                # ëŒ“ê¸€ì´ ìœ íš¨í•œ ê²½ìš°ì—ë§Œ ì¶”ê°€
                if comment_data["content"] or comment_data["media"]:
                    comments.append(comment_data)
                    print(f"  âœ… ëŒ“ê¸€ {len(comments)}: {comment_data['author']} - {comment_data['content'][:30]}... (ğŸ‘{comment_data['up_count']} ğŸ‘{comment_data['down_count']})")
                    
        except Exception as e:
            print(f"ëŒ“ê¸€ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            
        print(f"ğŸ“ ì´ {len(comments)}ê°œ ëŒ“ê¸€ ì¶”ì¶œ ì™„ë£Œ (ì¤‘ë³µ ì œê±°ë¨)")
        return comments
        
    async def scrape_post(self, url: str, config: Optional[Dict] = None) -> Dict:
        """ê°œë³„ ê²Œì‹œê¸€ ìŠ¤í¬ë˜í•‘"""
        if config is None:
            config = {}
            
        browser, page = await setup_browser(config)
        
        try:
            success = await navigate_to_page(page, url, config)
            if not success:
                return {}
                
            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            metadata = await self.extract_metadata(page)
            
            # ë³¸ë¬¸ ì½˜í…ì¸  ì¶”ì¶œ
            content = await self.extract_content(page)
            
            # ëŒ“ê¸€ ì¶”ì¶œ
            comments = await self.extract_comments(page)
            
            # ê²°ê³¼ êµ¬ì„±
            result = {
                "post_id": parse_post_id(url),
                "post_url": url,
                "scraped_at": datetime.now(pytz.timezone('Asia/Seoul')).isoformat(),
                "metadata": metadata,
                "content": content,
                "comments": comments
            }
            
            return result
            
        except Exception as e:
            print(f"ê²Œì‹œê¸€ ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜: {e}")
            return {}
        finally:
            await close_browser(browser)

# API ì „ì†¡ í•¨ìˆ˜
async def send_to_api(data: List[Dict], api_url: str) -> bool:
    """APIë¡œ ë°ì´í„° ì „ì†¡"""
    try:
        response = requests.post(api_url, json=data, timeout=30)
        if response.status_code == 200:
            print(f"âœ… API ì „ì†¡ ì„±ê³µ: {len(data)}ê°œ ê²Œì‹œê¸€")
            return True
        else:
            print(f"âŒ API ì „ì†¡ ì‹¤íŒ¨: {response.status_code}")
            return False
    except Exception as e:
        print(f"âŒ API ì „ì†¡ ì˜¤ë¥˜: {e}")
        return False

# ë©”ì¸ ìŠ¤í¬ë˜í•‘ í•¨ìˆ˜
async def scrape_ruliweb_politics_page(config: Optional[Dict] = None) -> List[Dict]:
    """ë£¨ë¦¬ì›¹ ì •ì¹˜ìœ ë¨¸ ê²Œì‹œíŒ ìŠ¤í¬ë˜í•‘"""
    if config is None:
        config = {}
    
    page_url = "https://bbs.ruliweb.com/community/board/300148"
    print(f"ğŸ” ë£¨ë¦¬ì›¹ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘ ì‹œì‘: {page_url}")
    
    browser, page = await setup_browser(config)
    
    try:
        # ê²Œì‹œê¸€ ëª©ë¡ í˜ì´ì§€ ì´ë™
        success = await navigate_to_page(page, page_url, config)
        if not success:
            return []
        
        # ê²Œì‹œê¸€ ëª©ë¡ ì¶”ì¶œ
        posts = await extract_post_list(page, RULIWEB_SELECTORS)
        
        # ìƒìœ„ ê²Œì‹œê¸€ ì„ ë³„
        selected_posts = select_top_posts(posts, limit=config.get('post_limit', 10))
        print(f"ğŸ“‹ ì„ ë³„ëœ ê²Œì‹œê¸€ ìˆ˜: {len(selected_posts)}")
        
        await close_browser(browser)
        
        # ê°œë³„ ê²Œì‹œê¸€ ìŠ¤í¬ë˜í•‘
        scraper = RuliwebScraper()
        results = []
        
        for i, post in enumerate(selected_posts, 1):
            post_url = post.get("url")
            if not post_url:
                continue
                
            print(f"ğŸ“„ ê²Œì‹œê¸€ {i}/{len(selected_posts)} ìŠ¤í¬ë˜í•‘ ì¤‘: {post.get('title', 'N/A')}")
            
            # ìš”ì²­ ê°„ ì§€ì—°
            if i > 1:
                delay = config.get('delay_between_requests', 3)
                await asyncio.sleep(delay)
            
            # ê°œë³„ ê²Œì‹œê¸€ ìŠ¤í¬ë˜í•‘
            post_data = await scraper.scrape_post(post_url, config)
            
            if post_data:
                results.append(post_data)
                print(f"âœ… ì„±ê³µ: {post_data['metadata']['title']}")
            else:
                print(f"âŒ ì‹¤íŒ¨: {post.get('title', 'N/A')}")
        
        # ê²°ê³¼ ì €ì¥
        if results:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"ruliweb_politics_experiment_{timestamp}.json"
            
            # scraping/data í´ë”ì— ì €ì¥
            data_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), "data")
            os.makedirs(data_dir, exist_ok=True)
            filepath = os.path.join(data_dir, filename)
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ ê²°ê³¼ê°€ {filepath}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            # APIë¡œ ì „ì†¡ (ìˆ˜ì •ëœ ì—”ë“œí¬ì¸íŠ¸)
            api_url = config.get('api_url', 'http://localhost:3000/api/scraping-data')
            if api_url and results:
                print(f"ğŸŒ APIë¡œ ë°ì´í„° ì „ì†¡ ì¤‘: {api_url}")
                try:
                    response = requests.post(api_url, json=results, timeout=30)
                    if response.status_code == 200:
                        print(f"âœ… API ì „ì†¡ ì„±ê³µ: {len(results)}ê°œ ê²Œì‹œê¸€")
                    else:
                        print(f"âŒ API ì „ì†¡ ì‹¤íŒ¨: {response.status_code} - {response.text}")
                except Exception as e:
                    print(f"âŒ API ì „ì†¡ ì˜¤ë¥˜: {e}")
        
        print(f"ğŸ“Š ìµœì¢… ê²°ê³¼: {len(results)}/{len(selected_posts)} ì„±ê³µ")
        return results
        
    except Exception as e:
        print(f"ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜: {e}")
        return []
    finally:
        try:
            await close_browser(browser)
        except:
            pass

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    async def main():
        config = {
            'headless': True,
            'slow_mo': 1000,
            'wait_time': 2,
            'delay_between_requests': 3,
            'timeout': 45000,
            'post_limit': 5,
            'api_url': 'http://localhost:3000/api/scraping-data'
        }
        
        results = await scrape_ruliweb_politics_page(config)
        print(f"âœ… ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: {len(results)}ê°œ ê²Œì‹œê¸€")
    
    asyncio.run(main()) 